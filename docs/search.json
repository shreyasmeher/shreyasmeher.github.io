[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shreyas Meher",
    "section": "",
    "text": "Postdoctoral Researcher Erasmus School of Social and Behavioural Sciences\nErasmus University Rotterdam\n Twitter  GitHub  LinkedIn\n\n\n\n\n\nI am a Postdoctoral Researcher in the Policy, Politics, and Society group at the Erasmus School of Social and Behavioural Sciences, Erasmus University Rotterdam. My research leverages computational methods to address core questions in comparative politics, digital governance, and international relations.\nCurrently, as part of the Horizon Europe TWIN4DEM project, I develop machine learning frameworks to model and detect democratic backsliding. My substantive focus is on executive aggrandizement—the process by which elected leaders systematically weaken democratic institutions to consolidate power. My work involves creating the algorithmic core for ‘digital twins’ of political systems, which allows for the simulation and analysis of threats to democratic resilience.\nMy broader research agenda explores:\n\nCensorship & Digital Sovereignty: Analyzing how different regime types implement internet control, from subtle content moderation in democracies to overt filtering and blackouts in autocracies.\nComputational Methods & Political Conflict: Building specialized language models for structured political event classification. Previously, as a researcher on a major NSF-funded project, I developed models like ConfliBERT and ConflLlama and applied reinforcement learning to enhance event data annotation, and I remain an active contributor to this line of research.\nInternational Political Economy: The politics of investment treaties and the network effects within international organizations.\n\nMethodologically, I specialize in computational text analysis, reinforcement learning, and the application of LLMs to political science research in HPC environments.\n\n\n\n\n\nErasmus University Rotterdam (June 2024-Present) As a researcher on the TWIN4DEM (Strengthening Democratic Resilience Through Digital Twins) project, my work focuses on developing the machine learning frameworks that power the project’s ‘digital twins’ of European democracies. I am responsible for pre-training and fine-tuning large language models to identify signals of executive aggrandizement from political text and designing the simulation algorithms that form the core of the project’s models.\n\n\n\n\n“Digital Sovereignty: The Political Economy of Internet Governance”\nAn analysis of internet content filtering across regime types.\nAwarded by: The University of Texas at Dallas\nCommittee: Dr. Clint Peinhardt, Co-Chair\nDr. Pengfei Zhang, Co-Chair\nDr. Patrick T. Brandt\nDr. Karl Ho"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Professionally\nI am a PhD Candidate in Public Policy & Political Economy at the University of Texas at Dallas. My research bridges the gap between political science and computational linguistics, focusing on areas such as global finance, economic development, and the impact of internet censorship in different political regimes. I have been actively involved in the ConfliBERT Research Lab with Dr. Patrick Brandt, contributing to the development of a language model for analyzing political conflict and violence. My upcoming work includes papers related to internet censorship and electoral accountability (with Dr. Pengfei Zhang) along with my dissertation on internet censorship across various regimes.\n\n\nPersonally\nI’m an avid football fan, rooting for Inter Milan, and a culinary enthusiast who loves exploring and creating new Dim Sum recipes.\n\n\nEducation\n\nNarsee Monjee Institute of Management Studies\nBsc Economics, 2016\n\n\nUniversity of Mumbai\nMa Public Policy, 2018\n\n\nThe University of Texas at Dallas\nPhd Public Policy & Political Economy, 2025\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Shreyas Meher",
    "section": "Education",
    "text": "Education\nUniversity of Texas at Dallas | Richardson, TX  PhD in Public Policy & Political Economy | Aug 2020 - May 2024\nUniversity of Texas at Dallas | Richardson, TX  Masters of Science in Management | Aug 2018 - May 2020"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Shreyas Meher",
    "section": "Experience",
    "text": "Experience\nThe University of Texas at Dallas | Research & Teaching Assistant | August 2020 - present"
  },
  {
    "objectID": "index.html#fa-hand-peace-hello-there-this-is-a-website-i-dont-always-have-a-chance-to-update-but-it-has-a-few-posts-and-some-ways-to-get-in-touch-with-me-if-you-need-to-cheers-fa-champagne-glasses",
    "href": "index.html#fa-hand-peace-hello-there-this-is-a-website-i-dont-always-have-a-chance-to-update-but-it-has-a-few-posts-and-some-ways-to-get-in-touch-with-me-if-you-need-to-cheers-fa-champagne-glasses",
    "title": "Shreyas Meher",
    "section": " hello there! this is a website I don’t always have a chance to update but it has a few posts and some ways to get in touch with me if you need to; cheers ",
    "text": "hello there! this is a website I don’t always have a chance to update but it has a few posts and some ways to get in touch with me if you need to; cheers"
  },
  {
    "objectID": "assignments/labs/Lab01.html",
    "href": "assignments/labs/Lab01.html",
    "title": "EPPS 6302: Lab Assignments",
    "section": "",
    "text": "x <- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=F) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9946764\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(5) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.03163502\n\nvar(y)\n\n[1] 0.8935626\n\nsqrt(var(y))\n\n[1] 0.9452844\n\nsd(y)\n\n[1] 0.9452844\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y, pch=20, col = \"purple\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"purple\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"skyblue\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "assignments/labs/Lab01.html#indexing-data-using",
    "href": "assignments/labs/Lab01.html#indexing-data-using",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Indexing Data using []",
    "text": "Indexing Data using []\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A)\n\n[1] 4 4"
  },
  {
    "objectID": "assignments/labs/Lab01.html#loading-data-from-github",
    "href": "assignments/labs/Lab01.html#loading-data-from-github",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Loading Data from GitHub",
    "text": "Loading Data from GitHub\n\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\")\n# fix(Auto) # Starting the X11 R data editor\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\",header=T,na.strings=\"?\")\n# fix(Auto)\nAuto=read.csv(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.csv\",header=T,na.strings=\"?\")\n# fix(Auto)\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,]\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "assignments/labs/Lab01.html#load-data-from-islr-website",
    "href": "assignments/labs/Lab01.html#load-data-from-islr-website",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Load data from ISLR website",
    "text": "Load data from ISLR website\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "assignments/labs/Lab01.html#additional-graphical-and-numerical-summaries",
    "href": "assignments/labs/Lab01.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Additional Graphical and Numerical Summaries",
    "text": "Additional Graphical and Numerical Summaries\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "assignments/labs/Lab01.html#linear-regression",
    "href": "assignments/labs/Lab01.html#linear-regression",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/Asus/Documents/R/win-library/4.1'\n(as 'lib' is unspecified)\n\n\npackage 'MASS' successfully unpacked and MD5 sums checked\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Asus\\AppData\\Local\\Temp\\Rtmp2rbXAY\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "assignments/labs/Lab01.html#multiple-linear-regression",
    "href": "assignments/labs/Lab01.html#multiple-linear-regression",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  < 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "assignments/labs/Lab01.html#non-linear-transformations-of-the-predictors",
    "href": "assignments/labs/Lab01.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Non-linear Transformations of the Predictors",
    "text": "Non-linear Transformations of the Predictors\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "assignments/labs/Lab01.html#qualitative-predictors",
    "href": "assignments/labs/Lab01.html#qualitative-predictors",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "assignments/labs/Lab01.html#interaction-terms-including-interaction-and-single-effects",
    "href": "assignments/labs/Lab01.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Interaction Terms (including interaction and single effects)",
    "text": "Interaction Terms (including interaction and single effects)\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "labs/lab1/Lab01.html",
    "href": "labs/lab1/Lab01.html",
    "title": "EPPS 6302: Lab01",
    "section": "",
    "text": "x <- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=F) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9956564\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(5) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.03163502\n\nvar(y)\n\n[1] 0.8935626\n\nsqrt(var(y))\n\n[1] 0.9452844\n\nsd(y)\n\n[1] 0.9452844\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y, pch=20, col = \"purple\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"purple\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"skyblue\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "labs/lab2/Lab02.html",
    "href": "labs/lab2/Lab02.html",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A)\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\")\n# fix(Auto) # Starting the X11 R data editor\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\",header=T,na.strings=\"?\")\n# fix(Auto)\nAuto=read.csv(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.csv\",header=T,na.strings=\"?\")\n# fix(Auto)\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,]\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/Asus/Documents/R/win-library/4.1'\n(as 'lib' is unspecified)\n\n\npackage 'MASS' successfully unpacked and MD5 sums checked\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Asus\\AppData\\Local\\Temp\\RtmpknA7wj\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  < 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "labs/lab3/Lab03.html",
    "href": "labs/lab3/Lab03.html",
    "title": "EPPS 6323: Lab03",
    "section": "",
    "text": "R Programming (EDA)\n(Adapted from Stackoverflow examples) (Objectives: Use plotly, reshape packages, interactive visualization)\n\nlibrary(tidyverse)\nlibrary(plotly)\ndata(iris)\nattach(iris)\n# Generate plot on three quantitative variables\niris_plot <- plot_ly(iris,\n                     x = Sepal.Length,\n                     y = Sepal.Width,\n                     z = Petal.Length,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\niris_plot\n\n\n\n\n# Regression object\n\npetal_lm <- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,\n               data = iris)\nlibrary(reshape2)\n\n#load data\n\npetal_lm <- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,data = iris)\n\n# Setting resolution parameter\ngraph_reso <- 0.05\n\n#Setup Axis\naxis_x <- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by = graph_reso)\naxis_y <- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), by = graph_reso)\n\n# Regression surface\n# Rearranging data for plotting\npetal_lm_surface <- expand.grid(Sepal.Length = axis_x,Sepal.Width = axis_y,KEEP.OUT.ATTRS = F)\npetal_lm_surface$Petal.Length <- predict.lm(petal_lm, newdata = petal_lm_surface)\npetal_lm_surface <- acast(petal_lm_surface, Sepal.Width ~ Sepal.Length, value.var = \"Petal.Length\")\nhcolors=c(\"orange\",\"blue\",\"green\")[iris$Species]\niris_plot <- plot_ly(iris,\n                     x = ~Sepal.Length,\n                     y = ~Sepal.Width,\n                     z = ~Petal.Length,\n                     text = Species,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     marker = list(color = hcolors),\n                     size=0.02)\n# Add surface\niris_plot <- add_trace(p = iris_plot,\n                       z = petal_lm_surface,\n                       x = axis_x,\n                       y = axis_y,\n                       type = \"surface\",mode = \"markers\",\n                       marker = list(color = hcolors))\niris_plot\n\n\n\n\n\n\n\nRegression object\n\npetal_lm <- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)"
  },
  {
    "objectID": "assignments/labs/lab.html",
    "href": "assignments/labs/lab.html",
    "title": "EPPS 6302: Lab Assignments",
    "section": "",
    "text": "x <- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=F) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9964111\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(5) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.03163502\n\nvar(y)\n\n[1] 0.8935626\n\nsqrt(var(y))\n\n[1] 0.9452844\n\nsd(y)\n\n[1] 0.9452844\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y, pch=20, col = \"purple\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"purple\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"skyblue\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "assignments/labs/lab.html#indexing-data-using",
    "href": "assignments/labs/lab.html#indexing-data-using",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Indexing Data using []",
    "text": "Indexing Data using []\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A)\n\n[1] 4 4"
  },
  {
    "objectID": "assignments/labs/lab.html#loading-data-from-github",
    "href": "assignments/labs/lab.html#loading-data-from-github",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Loading Data from GitHub",
    "text": "Loading Data from GitHub\n\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\")\n# fix(Auto) # Starting the X11 R data editor\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\",header=T,na.strings=\"?\")\n# fix(Auto)\nAuto=read.csv(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.csv\",header=T,na.strings=\"?\")\n# fix(Auto)\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,]\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "assignments/labs/lab.html#load-data-from-islr-website",
    "href": "assignments/labs/lab.html#load-data-from-islr-website",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Load data from ISLR website",
    "text": "Load data from ISLR website\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "assignments/labs/lab.html#additional-graphical-and-numerical-summaries",
    "href": "assignments/labs/lab.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Additional Graphical and Numerical Summaries",
    "text": "Additional Graphical and Numerical Summaries\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "assignments/labs/lab.html#linear-regression",
    "href": "assignments/labs/lab.html#linear-regression",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/Asus/Documents/R/win-library/4.1'\n(as 'lib' is unspecified)\n\n\npackage 'MASS' successfully unpacked and MD5 sums checked\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Asus\\AppData\\Local\\Temp\\Rtmp0yK4P7\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "assignments/labs/lab.html#multiple-linear-regression",
    "href": "assignments/labs/lab.html#multiple-linear-regression",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  < 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "assignments/labs/lab.html#non-linear-transformations-of-the-predictors",
    "href": "assignments/labs/lab.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Non-linear Transformations of the Predictors",
    "text": "Non-linear Transformations of the Predictors\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "assignments/labs/lab.html#qualitative-predictors",
    "href": "assignments/labs/lab.html#qualitative-predictors",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "assignments/labs/lab.html#interaction-terms-including-interaction-and-single-effects",
    "href": "assignments/labs/lab.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6302: Lab Assignments",
    "section": "Interaction Terms (including interaction and single effects)",
    "text": "Interaction Terms (including interaction and single effects)\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "assignments/a2/a2.html",
    "href": "assignments/a2/a2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "library(haven)\nlibrary(ggmice)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.2.1      v stringr 1.4.1 \nv readr   2.1.3      v forcats 0.5.2 \nv purrr   0.3.5      \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(Ecdat)\n\nLoading required package: Ecfun\n\nAttaching package: 'Ecfun'\n\nThe following object is masked from 'package:base':\n\n    sign\n\n\nAttaching package: 'Ecdat'\n\nThe following object is masked from 'package:datasets':\n\n    Orange\n\nlibrary(dplyr)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(ggcorrplot)\nlibrary(naniar)\nlibrary(cli)\n\nTEDS_2016<-read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")"
  },
  {
    "objectID": "assignments/a2/a2.html#missingness-plots",
    "href": "assignments/a2/a2.html#missingness-plots",
    "title": "Assignment 2",
    "section": "Missingness plots",
    "text": "Missingness plots\n\ncolSums(is.na(TEDS_2016)) / nrow(TEDS_2016)\n\n       District             Sex             Age             Edu           Arear \n     0.00000000      0.00000000      0.00000000      0.00000000      0.00000000 \n         Career         Career8          Ethnic           Party         PartyID \n     0.00000000      0.00000000      0.00000000      0.00000000      0.00000000 \n          Tondu          Tondu3             nI2        votetsai           green \n     0.00000000      0.00000000      0.00000000      0.25384615      0.00000000 \n    votetsai_nm    votetsai_all    Independence     Unification              sq \n     0.25384615      0.14674556      0.00000000      0.00000000      0.00000000 \n      Taiwanese             edu          female     whitecollar       lowincome \n     0.00000000      0.00591716      0.00000000      0.00000000      0.00000000 \n         income       income_nm             age             KMT             DPP \n     0.00000000      0.19526627      0.00000000      0.00000000      0.00000000 \n            npp         noparty             pfp           South           north \n     0.00000000      0.00000000      0.00000000      0.00000000      0.00000000 \n  Minnan_father Mainland_father      Econ_worse      Inequality     inequality5 \n     0.00000000      0.00000000      0.00000000      0.00000000      0.00000000 \n     econworse5 Govt_for_public        pubwelf5  Govt_dont_care      highincome \n     0.00000000      0.00000000      0.00000000      0.00000000      0.19526627 \n        votekmt      votekmt_nm            Blue           Green        No_Party \n     0.00000000      0.25384615      0.00000000      0.00000000      0.00000000 \n       voteblue     voteblue_nm       votedpp_1       votekmt_1 \n     0.00000000      0.25384615      0.11065089      0.11065089 \n\nplot_pattern(TEDS_2016)\n\n\n\nTEDS_2016 %>%\n  # Create an UpSet plot\n  gg_miss_upset(., nsets = 10)\n\n\n\ntable(TEDS_2016$Tondu)\n\n\n  1   2   3   4   5   6   9 \n 27 180 546 328 380 108 121"
  },
  {
    "objectID": "assignments/a2/a2.html#some-plots",
    "href": "assignments/a2/a2.html#some-plots",
    "title": "Assignment 2",
    "section": "Some plots",
    "text": "Some plots\n\nbarplot(table(TEDS_2016$Tondu))\n\n\n\ncounts <- table(TEDS_2016$Tondu, TEDS_2016$Sex)\nmosaicplot(counts, xlab='Tondu', ylab='Sex',main='Tondu by Sex', col='orange')"
  },
  {
    "objectID": "assignments/a2/a2.html#correlation-plots",
    "href": "assignments/a2/a2.html#correlation-plots",
    "title": "Assignment 2",
    "section": "Correlation plots",
    "text": "Correlation plots\n\nsel_dat<-TEDS_2016%>%select(Tondu,female, DPP, age, income, edu, Taiwanese, Econ_worse,votetsai)\n\ncormat <- sel_dat %>%\n  cor(., use = \"pairwise.complete.obs\")\n\ncorrplot(cormat, # correlation matrix\n         order = \"hclust\", # hierarchical clustering of correlations\n         addrect = 2) # number of rectangles to draw around clusters\n\n\n\nggcorrplot(cormat, # correlation matrix\n           type = \"lower\", # print the lower part of the correlation matrix\n           hc.order = TRUE, # hierarchical clustering of correlations\n           lab = TRUE) # add correlation values as labels"
  },
  {
    "objectID": "projects/BLScraper/bls.html",
    "href": "projects/BLScraper/bls.html",
    "title": "BLS Scraper Project",
    "section": "",
    "text": "Project Summary\nThe script is a .ipynb file named as Final.ipynb. Load the jupyter notebook with Jupyter Lab or Jupyter notebook.\nThe first few cells in the script ask the user to input the working url of the BLS content/table that they want to scrape using the script. Here, they are expected to input a url from https://www.bls.gov/bls/newsrels.htm#OPLC, after which the url is stored by the script.\nThe url is then scraped using BeautifulSoup, looking for the relevant tags of the various tables in the page. This script is unique in the sense that it allows for selective scraping of multiple tables in the webpage, which is not a base function of the packages used (Pandas or BeautifulSoup). Tables are first unwrapped from within the various tags and then merged using a loop function.\nThe user is then asked which of the tables in the page (in the case that there are multiple of them - eg. https://www.bls.gov/regions/southwest/news-release/2022/occupationalemploymentandwages_houston_20220624.htm) they want to scrape.\nTo clean the data up even more, the footnote markers are then removed from the dataframe using another function. As the data scraped is going to be quantitative in nature, $ signs and markers are removed from the dataframe created as well. This is to ensure that the data is readable by statistical software and requires very little cleaning after it is run through the script. You might have to change a few tags in the script for this to work, and so I have #’d out the commands to do the same.\nFinally, the user is then prompted once again to define a .csv filename for the data to be stored to in their local computing environment. The .csv file is then stored onto the working directory.\n\n#workspace init\nimport urllib\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n#defining variables with user-input\n\ndef new_func():\n    url = input('Please enter the BLS publication that you want to scrape table from:')\n    return url\n\nurl = new_func()\ndata = urllib.request.urlopen(url).read()\n\nsp = BeautifulSoup(data,'html.parser')\n\n#Let us first get the table attributes on the BLS website\nprint('Table attributes')\nfor table in sp.find_all('table'):\n    print(table.get('class'))\n    break\n\nlsttb = sp.find('table',class_='regular')\n\n#check for multiple tables and unwrap them\n\nfor table in sp.findChildren(attrs={'id': 'regular'}): \n    for c in table.children:\n        if c.name in ['tbody', 'thead']:\n            c.unwrap()\n\n#dataset creation \n\ndata_pandas = pd.read_html(str(sp), flavor=\"bs4\",thousands=',',decimal='.')\n\n#clean dataset\ndf = pd.concat(data_pandas, axis=0)#to convert lists of pd.read_html to dataframe\n\n#export to csv\ndf.to_csv(input('Specify .csv filename:'))\n\nTable attributes\n['regular']"
  },
  {
    "objectID": "projects/AidData/Flows.html",
    "href": "projects/AidData/Flows.html",
    "title": "Shreyas Meher",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd \nimport seaborn as sn\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport missingno\n#from floweaver import *\nimport plotly as py\n#from ipysankeywidget import SankeyWidget\nimport missingno\ninit_notebook_mode(connected=True)\n\nmaster = pd.read_csv('C:/Users/Asus/Box/PhD/SummerProj/Python/all_flow_classes.csv')\nms = master.copy() #working dataframe\n\n\nprint(ms.shape)\n\n(6190, 78)\n\n\n\nms.rename(columns = {'recipient_condensed':'country'}, inplace = True) \nms.rename(columns = {'transactions_start_year':'start_year'}, inplace = True) \nms['flow_class'] = ms['flow_class'].replace({'ODA-like': 'Aid', 'OOF-like': 'Non-Aid', 'Vague (Official Finance)': 'Not Sure'})\n\n\nmapbox_access_token  =  'pk.eyJ1IjoidW5rbGV0YW0iLCJhIjoiY2tkNnljemFxMG1mYTJ6cmE4bW1yYjczeiJ9.rBvwQf6Zw4BTA_f_O9dKbg'\nms  = ms.sort_values(by=['start_year'], ascending=True)\n\nms['Project'] = \" \"+ ms['project_title'] + \" [\" + ms['location_type_name']+ \"]\"\nfig = px.scatter_mapbox(ms, \n                     lon = ms['longitude'],\n                     lat = ms['latitude'],\n                     \n                     opacity = 0.5,\n                     color=\"flow_class\", \n                     hover_name=\"place_name\",\n                     animation_frame=\"start_year\",\n                     )\n\nfig.update_layout(\n    title = \"Investment from China [2000 - 2014]\",\n    legend_title_text='Investment Category', \n    hovermode='closest',\n    mapbox=dict(\n        \n        accesstoken=mapbox_access_token,\n        bearing=0,\n        pitch=0,\n        zoom=2,\n        style = \"light\"\n    )\n)\n#fig.show()\niplot(fig)\n\n\n                                                \n\n\n\nvalues = ms['flow_class'].value_counts()\nclass_ = pd.unique(ms['flow_class'])\n\nfig = px.pie(ms, values=values, labels=class_, names = class_)\nfig.update_layout(\n    \n    title = \"Total Investment - Category\",\n    legend_title_text='Investment Category',\n    showlegend=True\n    \n)\n\nfig.show()\n\n\n                                                \n\n\n\nvalues = ms['flow'].value_counts()\nclass_ = pd.unique(ms['flow'])\n\nfig = px.pie(ms, values=values, labels=class_, names = class_, hole=.7)\nfig.update_layout(\n    \n    title = \"Total Investment - Type\",\n    legend_title_text='Type of Investment',\n    showlegend=True\n    \n)\n\nfig.show()"
  },
  {
    "objectID": "projects/AidData/Flows.html#references",
    "href": "projects/AidData/Flows.html#references",
    "title": "Shreyas Meher",
    "section": "References",
    "text": "References\n\nTierney, Michael J., Daniel L. Nielson, Darren G. Hawkins, J. Timmons Roberts, Michael G. Findley, Ryan M. Powers, Bradley Parks, Sven E. Wilson, and Robert L. Hicks. 2011. More Dollars than Sense: Refining Our Knowledge of Development Finance Using AidData. World Development 39 (11): 1891-1906.\n\n\nStrandow, Daniel, Michael Findley, Daniel Nielson, and Joshua Powell. 2011. The UCDP-AidData codebook on Geo-referencing Foreign Aid. Version 1.1. Uppsala Conflict Data Program. Uppsala, Sweden: Uppsala University."
  },
  {
    "objectID": "assignments/a3/a3.html",
    "href": "assignments/a3/a3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "pacman::p_load(ggplot2, tidyr, dplyr, haven, gridExtra, ggExtra, RColorBrewer)\nTEDS_2016 <- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nTEDS_2016$Tondu<-as.numeric(TEDS_2016$Tondu,labels=c(\"Unificationnow”,“Statusquo,unif.infuture”,“Statusquo,decidelater\",\"Statusquoforever\",\"Statusquo,indep.infuture\",\"Independencenow”,“Noresponse\"))\nhead(TEDS_2016$Tondu)\n\n[1] 3 5 3 5 9 4\n\n\n\n\n\nsel_dat<-TEDS_2016%>%select(Tondu,female, DPP, age, income, edu, Taiwanese, Econ_worse,votetsai)\n\n\n\n\n\nfit1<-lm(Tondu~age+edu+income, data=sel_dat)\nsummary(fit1)\n\n\nCall:\nlm(formula = Tondu ~ age + edu + income, data = sel_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7780 -1.1841 -0.4322  1.1079  5.4157 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.302529   0.257369  20.603  < 2e-16 ***\nage         -0.004205   0.003194  -1.316   0.1882    \nedu         -0.244608   0.037579  -6.509 9.96e-11 ***\nincome      -0.031855   0.016357  -1.948   0.0516 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.725 on 1676 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.04287,   Adjusted R-squared:  0.04115 \nF-statistic: 25.02 on 3 and 1676 DF,  p-value: 7.771e-16\n\nta<-ggplot(sel_dat, aes(x=age,y=Tondu))+\n  geom_smooth(method = \"lm\", se = F, show.legend = F)+\n  geom_point(show.legend = F, position = \"jitter\",alpha=.5, pch=16) + ggthemes::theme_few() +\n  labs(x=\"Age\", y=\"TONDU preferences\")\n\nte<-ggplot(sel_dat, aes(x=edu,y=Tondu))+\n  geom_smooth(method = \"lm\", se = F, show.legend = F)+\n  geom_point(show.legend = F, position = \"jitter\",alpha=.5, pch=16) + ggthemes::theme_few() +\n  labs(x=\"Education\", y=\"TONDU preferences\")\n\nti<-ggplot(sel_dat, aes(x=income,y=Tondu))+\n  geom_smooth(method = \"lm\", se = F, show.legend = F)+\n  geom_point(show.legend = F, position = \"jitter\",alpha=.5, pch=16) + ggthemes::theme_few() +\n  labs(x=\"Income\", y=\"TONDU preferences\")\n\ngrid.arrange(ta,te,ti,ncol=3,nrow=1)\n\n\n\n\n\n\n\n\ntaei<-ggplot(sel_dat, aes(age, Tondu, colour=edu))+\n  geom_point()\n\nggMarginal(taei, type=\"histogram\")\n\n\n\n\n\n\n\nThe dependent variable has too many values, which we can find out using the unique() function. A multinomial logit would be better to use here, or other statistical methods to fit the model as linear regression is pretty weak here.\n\nunique(sel_dat$Tondu)\n\n[1] 3 5 9 4 6 2 1"
  },
  {
    "objectID": "assignments/proposal/prop-2-22-23.html",
    "href": "assignments/proposal/prop-2-22-23.html",
    "title": "Knowledge Mining Proposal",
    "section": "",
    "text": "Authors - Arslan Khalid, Kiwan Park, Shreyas Meher\n\n\nMedia Coverage of U.S. Foreign Policy\n\nArea/Topic\nThe news media often regards itself as the fourth branch of government. Some have referred to it as a watchdog that checks government access and holds elected officials accountable. However, research shows that media is influenced by a number of factors. Particularly, when reporting on foreign affairs the media is susceptible to “contesting government propaganda campaigns where the government can employ ideological weapons like anti-communism, a demonized enemy or alleged national security threats” (Herman, 1993). Research also shows that the media does not report on issues in an unbiased way. Some of the factors affecting media coverage that have been noted in research are:\n\nDominant media are themselves corporate elite establishment\nEconomic incentives of media companies\nStructural aspects of media, i.e. who owns the media companies\nIdeological bend of news organizations\n\nThis research looks to compare media outlets’ coverage and reporting of foreign policy issues in the United States. Our hypothesis is that media companies shift their coverage of foreign policy issues depending on which political party is in power.\n\n\n\nResearch Statement\nHow does the media’s coverage of U.S. foreign policy change with respect to which political party is in power? Specifically, this research will focus on the change in the President’s office after the 2016 election.\n\n\nMethods\nWe look to conduct a sentiment and network analysis of a large dataset. The dataset that we use is the All the News 2.0 dataset compiled by Andrew Thompson link to data. This dataset is useful for us as it contains text and publication data on a number of US news media websites such as Fox News, CNN, NYT, etc. along with essay websites such as Politico, New Yorker, etc. It will be interesting to see the number of clusters that we can form where we categorize these 27 American websites and conduct sentiment and network analysis for each one of this cluster to highlight differences and even similarities. Additionally, as we are required to add to the database, we look to use web scraping tools such as BeautifulSoup to further expand the dataset (possibly bring in news media from outside the U.S. such as Al Jazeera)."
  },
  {
    "objectID": "assignments/a4/a4.html",
    "href": "assignments/a4/a4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Foreign aid and migration are two closely intertwined topics that have gained significant attention in recent years. Foreign aid refers to the financial or material assistance provided by one country to another, typically with the aim of promoting economic, social, or political development in the recipient country. Migration, on the other hand, refers to the movement of people from one country to another, usually in search of better economic opportunities, political stability, or refuge from conflict or persecution.\nForeign aid can have both positive and negative effects on migration. On the one hand, aid can help to reduce poverty and create economic opportunities in recipient countries, which may discourage people from emigrating in search of better prospects. On the other hand, aid can also contribute to the creation of a dependent relationship between donor and recipient countries, which may perpetuate economic and political instability and ultimately drive people to migrate.\nMoreover, the impact of migration on recipient countries can also vary depending on the context. While migration can bring new skills, ideas, and resources to host countries, it can also pose challenges in terms of social integration, labor market competition, and cultural differences.\nOverall, the relationship between foreign aid and migration is complex and multifaceted, and requires careful consideration and analysis to understand its implications and potential solutions."
  },
  {
    "objectID": "assignments/a4/a4.html#data",
    "href": "assignments/a4/a4.html#data",
    "title": "Assignment 4",
    "section": "Data",
    "text": "Data\nThe World Development Indicators (WDI) package in R is a tool used to access and manipulate data from the World Bank’s World Development Indicators database. The package provides functions for downloading and extracting data on a wide range of indicators related to economic and social development, such as population, GDP, education, health, and poverty.\nThe WDI package in R allows users to easily retrieve data from the World Bank database in a format that is suitable for analysis and visualization. The package includes functions for filtering and aggregating data, as well as for merging data from multiple indicators and countries.\n\n\n[1] \"country\" \"year\"    \"migr\"    \"fa\"      \"inf\""
  },
  {
    "objectID": "assignments/a4/a4.html#scaling-all-of-the-data",
    "href": "assignments/a4/a4.html#scaling-all-of-the-data",
    "title": "Assignment 4",
    "section": "Scaling all of the data",
    "text": "Scaling all of the data\nScaling data before applying the k-means clustering algorithm is important because it helps to ensure that variables with larger scales or variances do not dominate the analysis.\nK-means clustering is a distance-based clustering algorithm, which means that it uses the Euclidean distance between variables to form clusters. If variables have different scales, those with larger scales or variances will have a greater impact on the distance calculation, and thus, the clustering outcome.\nBy scaling the data, we can ensure that each variable has a similar range of values and variance, which allows the k-means algorithm to equally weigh each variable in the clustering process. This can improve the accuracy and robustness of the clustering results.\nAdditionally, scaling the data can also help to improve the interpretability of the results. Since the variables are on the same scale, it is easier to compare the contributions of each variable to the cluster formation and to understand the relative importance of each variable in differentiating between the clusters."
  },
  {
    "objectID": "assignments/a4/a4.html#various-plots",
    "href": "assignments/a4/a4.html#various-plots",
    "title": "Assignment 4",
    "section": "Various plots",
    "text": "Various plots"
  },
  {
    "objectID": "assignments/a4/a4.html#hierarchical-clustering",
    "href": "assignments/a4/a4.html#hierarchical-clustering",
    "title": "Assignment 4",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering"
  },
  {
    "objectID": "assignments/a4/a4.html#clustering-plots-figuring-out-optimum-cluster-number",
    "href": "assignments/a4/a4.html#clustering-plots-figuring-out-optimum-cluster-number",
    "title": "Assignment 4",
    "section": "Clustering plots & figuring out optimum cluster number",
    "text": "Clustering plots & figuring out optimum cluster number\n\n\nK-means clustering with 3 clusters of sizes 3, 33, 15\n\nCluster means:\n   mig_scale   fa_scale  inf_scale\n1 -2.9308285 -0.8960418  0.2749959\n2  0.1423792 -0.5373472 -0.3690791\n3  0.2729314  1.3613721  0.7569748\n\nClustering vector:\n [1] 2 2 2 2 2 2 2 1 2 1 1 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3\n[39] 3 3 3 3 3 3 3 3 3 3 3 3 3\n\nWithin cluster sum of squares by cluster:\n[1]  1.920832 34.173419 33.295700\n (between_SS / total_SS =  53.7 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n\nFurther plots\n\n\n\n\n\n\n 1  2  3 \n 3 33 15 \n\n\n\n\n\n  cluster size ave.sil.width\n1       1   17          0.20\n2       2    5          0.12\n3       3   29          0.51"
  },
  {
    "objectID": "assignments/a4/a4.html#animated-clusterplot",
    "href": "assignments/a4/a4.html#animated-clusterplot",
    "title": "Assignment 4",
    "section": "Animated clusterplot",
    "text": "Animated clusterplot"
  },
  {
    "objectID": "projects/CPU/CPU.html",
    "href": "projects/CPU/CPU.html",
    "title": "CPU Usage",
    "section": "",
    "text": "Embedded Shiny application\nThis is a CPU usage widget that is created using shiny & python. Code edited from - https://quarto-ext.github.io/shinylive/.\n#| standalone: true\n#| components: [editor, viewer]\n## file: app.py\nimport sys\n\nif \"pyodide\" in sys.modules:\n    # psutil doesn't work on pyodide--use fake data instead\n    from fakepsutil import cpu_count, cpu_percent\n\n    shinylive_message = \"Note: the CPU data is simulated when running in Shinylive.\"\nelse:\n    from psutil import cpu_count, cpu_percent\n\n    shinylive_message = \"\"\n\nfrom math import ceil\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom shiny import App, Inputs, Outputs, Session, reactive, render, ui\n\n# The agg matplotlib backend seems to be a little more efficient than the default when\n# running on macOS, and also gives more consistent results across operating systems\nmatplotlib.use(\"agg\")\n\n# max number of samples to retain\nMAX_SAMPLES = 1000\n# secs between samples\nSAMPLE_PERIOD = 1\n\n\nncpu = cpu_count(logical=True)\n\napp_ui = ui.page_fluid(\n    ui.tags.style(\n        \"\"\"\n        /* Don't apply fade effect, it's constantly recalculating */\n        .recalculating {\n            opacity: 1;\n        }\n        tbody > tr:last-child {\n            /*border: 3px solid var(--bs-dark);*/\n            box-shadow:\n                0 0 2px 1px #fff, /* inner white */\n                0 0 4px 2px #0ff, /* middle cyan */\n                0 0 5px 3px #00f; /* outer blue */\n        }\n        #table table {\n            table-layout: fixed;\n            width: %s;\n            font-size: 0.8em;\n        }\n        th, td {\n            text-align: center;\n        }\n        \"\"\"\n        % f\"{ncpu*4}em\"\n    ),\n    ui.h3(\"CPU Usage %\", class_=\"mt-2\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"cmap\",\n                \"Colormap\",\n                {\n                    \"inferno\": \"inferno\",\n                    \"viridis\": \"viridis\",\n                    \"copper\": \"copper\",\n                    \"prism\": \"prism (not recommended)\",\n                },\n            ),\n            ui.p(ui.input_action_button(\"reset\", \"Clear history\", class_=\"btn-sm\")),\n            ui.input_switch(\"hold\", \"Freeze output\", value=False),\n            shinylive_message,\n            class_=\"mb-3\",\n        ),\n        ui.panel_main(\n            ui.div(\n                {\"class\": \"card mb-3\"},\n                ui.div(\n                    {\"class\": \"card-body\"},\n                    ui.h5({\"class\": \"card-title mt-0\"}, \"Graphs\"),\n                    ui.output_plot(\"plot\", height=f\"{ncpu * 40}px\"),\n                ),\n                ui.div(\n                    {\"class\": \"card-footer\"},\n                    ui.input_numeric(\"sample_count\", \"Number of samples per graph\", 50),\n                ),\n            ),\n            ui.div(\n                {\"class\": \"card\"},\n                ui.div(\n                    {\"class\": \"card-body\"},\n                    ui.h5({\"class\": \"card-title m-0\"}, \"Heatmap\"),\n                ),\n                ui.div(\n                    {\"class\": \"card-body overflow-auto pt-0\"},\n                    ui.output_table(\"table\"),\n                ),\n                ui.div(\n                    {\"class\": \"card-footer\"},\n                    ui.input_numeric(\"table_rows\", \"Rows to display\", 5),\n                ),\n            ),\n        ),\n    ),\n)\n\n\n@reactive.Calc\ndef cpu_current():\n    reactive.invalidate_later(SAMPLE_PERIOD)\n    return cpu_percent(percpu=True)\n\n\ndef server(input: Inputs, output: Outputs, session: Session):\n    cpu_history = reactive.Value(None)\n\n    @reactive.Calc\n    def cpu_history_with_hold():\n        # If \"hold\" is on, grab an isolated snapshot of cpu_history; if not, then do a\n        # regular read\n        if not input.hold():\n            return cpu_history()\n        else:\n            # Even if frozen, we still want to respond to input.reset()\n            input.reset()\n            with reactive.isolate():\n                return cpu_history()\n\n    @reactive.Effect\n    def collect_cpu_samples():\n        \"\"\"cpu_percent() reports just the current CPU usage sample; this Effect gathers\n        them up and stores them in the cpu_history reactive value, in a numpy 2D array\n        (rows are CPUs, columns are time).\"\"\"\n\n        new_data = np.vstack(cpu_current())\n        with reactive.isolate():\n            if cpu_history() is None:\n                cpu_history.set(new_data)\n            else:\n                combined_data = np.hstack([cpu_history(), new_data])\n                # Throw away extra data so we don't consume unbounded amounts of memory\n                if combined_data.shape[1] > MAX_SAMPLES:\n                    combined_data = combined_data[:, -MAX_SAMPLES:]\n                cpu_history.set(combined_data)\n\n    @reactive.Effect(priority=100)\n    @reactive.event(input.reset)\n    def reset_history():\n        cpu_history.set(None)\n\n    @output\n    @render.plot\n    def plot():\n        history = cpu_history_with_hold()\n\n        if history is None:\n            history = np.array([])\n            history.shape = (ncpu, 0)\n\n        nsamples = input.sample_count()\n\n        # Throw away samples too old to fit on the plot\n        if history.shape[1] > nsamples:\n            history = history[:, -nsamples:]\n\n        ncols = 2\n        nrows = int(ceil(ncpu / ncols))\n        fig, axeses = plt.subplots(\n            nrows=nrows,\n            ncols=ncols,\n            squeeze=False,\n        )\n        for i in range(0, ncols * nrows):\n            row = i // ncols\n            col = i % ncols\n            axes = axeses[row, col]\n            if i >= len(history):\n                axes.set_visible(False)\n                continue\n            data = history[i]\n            axes.yaxis.set_label_position(\"right\")\n            axes.yaxis.tick_right()\n            axes.set_xlim(-(nsamples - 1), 0)\n            axes.set_ylim(0, 100)\n\n            assert len(data) <= nsamples\n\n            # Set up an array of x-values that will right-align the data relative to the\n            # plotting area\n            x = np.arange(0, len(data))\n            x = np.flip(-x)\n\n            # Color bars by cmap\n            color = plt.get_cmap(input.cmap())(data / 100)\n            axes.bar(x, data, color=color, linewidth=0, width=1.0)\n\n            axes.set_yticks([25, 50, 75])\n            for ytl in axes.get_yticklabels():\n                if col == ncols - 1 or i == ncpu - 1 or True:\n                    ytl.set_fontsize(7)\n                else:\n                    ytl.set_visible(False)\n                    hide_ticks(axes.yaxis)\n            for xtl in axes.get_xticklabels():\n                xtl.set_visible(False)\n            hide_ticks(axes.xaxis)\n            axes.grid(True, linewidth=0.25)\n\n        return fig\n\n    @output\n    @render.table\n    def table():\n        history = cpu_history_with_hold()\n        latest = pd.DataFrame(history).transpose().tail(input.table_rows())\n        if latest.shape[0] == 0:\n            return latest\n        return (\n            latest.style.format(precision=0)\n            .hide(axis=\"index\")\n            .set_table_attributes(\n                'class=\"dataframe shiny-table table table-borderless font-monospace\"'\n            )\n            .background_gradient(cmap=input.cmap(), vmin=0, vmax=100)\n        )\n\n\ndef hide_ticks(axis):\n    for ticks in [axis.get_major_ticks(), axis.get_minor_ticks()]:\n        for tick in ticks:\n            tick.tick1line.set_visible(False)\n            tick.tick2line.set_visible(False)\n            tick.label1.set_visible(False)\n            tick.label2.set_visible(False)\n\n\napp = App(app_ui, server)\n\n\n\n## file: fakepsutil.py\n\n\"\"\"Generates synthetic data\"\"\"\n\nimport numpy as np\n\n\ndef cpu_count(logical: bool = True):\n    return 8 if logical else 4\n\n\nlast_sample = np.random.uniform(0, 100, size=cpu_count(True))\n\n\ndef cpu_percent(interval=None, percpu=False):\n    global last_sample\n    delta = np.random.normal(scale=10, size=len(last_sample))\n    last_sample = (last_sample + delta).clip(0, 100)\n    if percpu:\n        return last_sample.tolist()\n    else:\n        return last_sample.mean()\n    \n## file: requirements.txt\n    \n# Pandas needs Jinja2 for table styling, but it doesn't (yet) load automatically\n# in Pyodide, so we need to explicitly list it here.\nJinja2"
  },
  {
    "objectID": "assignments/a5/a5.html",
    "href": "assignments/a5/a5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "The website at http://www.analytictech.com/mb021/mlk.htm provides access to the full text of the famous “I Have a Dream” speech delivered by Martin Luther King Jr. on August 28, 1963, during the March on Washington for Jobs and Freedom. The speech is widely considered to be one of the most important speeches of the 20th century, and has become a landmark moment in the American Civil Rights Movement.\nThe speech was delivered at the Lincoln Memorial in Washington D.C., and was attended by over 200,000 people. It called for an end to racial segregation and discrimination in the United States, and expressed a vision of a future where all people, regardless of their race or color, could live in harmony and equality.\nThe speech is notable for its powerful use of language and imagery, and its iconic closing lines in which Dr. King declared, “Free at last! Free at last! Thank God Almighty, we are free at last!”\nThe website provides the full text of the speech, as well as some background information on its historical context and significance. It is a valuable resource for anyone interested in the history of the American Civil Rights Movement or the art of public speaking."
  },
  {
    "objectID": "assignments/a5/a5.html#next-steps",
    "href": "assignments/a5/a5.html#next-steps",
    "title": "Assignment 5",
    "section": "Next steps",
    "text": "Next steps"
  },
  {
    "objectID": "assignments/a5/a5.html#coding",
    "href": "assignments/a5/a5.html#coding",
    "title": "Assignment 5",
    "section": "Coding",
    "text": "Coding\n\n\n[1] \"I am happy to join with you today in what will go down in\\r\\nhistory as the greatest demonstration for freedom in the history\\r\\nof our nation. \"                                                                                                                                                                                                              \n[2] \"Five score years ago a great American in whose symbolic shadow\\r\\nwe stand today signed the Emancipation Proclamation. This\\r\\nmomentous decree came as a great beckoning light of hope to\\r\\nmillions of Negro slaves who had been seared in the flames of\\r\\nwithering injustice. It came as a joyous daybreak to end the long\\r\\nnight of their captivity. \"\n[3] \"But one hundred years later the Negro is still not free. One\\r\\nhundred years later the life of the Negro is still sadly crippled\\r\\nby the manacles of segregation and the chains of discrimination. \"                                                                                                                                                        \n\n\n[1] \"VectorSource\" \"SimpleSource\" \"Source\"      \n\n\n<<SimpleCorpus>>\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 26\n\n [1] I am happy to join with you today in what will go down in\\r\\nhistory as the greatest demonstration for freedom in the history\\r\\nof our nation.                                                                                                                                                                                                                                                                                                                                                   \n [2] Five score years ago a great American in whose symbolic shadow\\r\\nwe stand today signed the Emancipation Proclamation. This\\r\\nmomentous decree came as a great beckoning light of hope to\\r\\nmillions of Negro slaves who had been seared in the flames of\\r\\nwithering injustice. It came as a joyous daybreak to end the long\\r\\nnight of their captivity.                                                                                                                                     \n [3] But one hundred years later the Negro is still not free. One\\r\\nhundred years later the life of the Negro is still sadly crippled\\r\\nby the manacles of segregation and the chains of discrimination.                                                                                                                                                                                                                                                                                             \n [4] One hundred years later the Negro lives on a lonely island of\\r\\npoverty in the midst of a vast ocean of material prosperity.                                                                                                                                                                                                                                                                                                                                                                     \n [5] One hundred years later the Negro is still languishing in the\\r\\ncomers of American society and finds himself in exile in his own\\r\\nland.                                                                                                                                                                                                                                                                                                                                                        \n [6] We all have come to this hallowed spot to remind America of\\r\\nthe fierce urgency of now. Now is the time to rise from the dark\\r\\nand desolate valley of segregation to the sunlit path of racial\\r\\njustice. Now is the time to change racial injustice to the solid\\r\\nrock of brotherhood. Now is the time to make justice ring out for\\r\\nall of God's children.                                                                                                                             \n [7] There will be neither rest nor tranquility in America until\\r\\nthe Negro is granted citizenship rights.                                                                                                                                                                                                                                                                                                                                                                                           \n [8] We must forever conduct our struggle on the high plane of\\r\\ndignity and discipline. We must not allow our creative protest to\\r\\ndegenerate into physical violence. Again and again we must rise\\r\\nto the majestic heights of meeting physical force with soul\\r\\nforce.                                                                                                                                                                                                                        \n [9] And the marvelous new militarism which has engulfed the Negro\\r\\ncommunity must not lead us to a distrust of all white people, for\\r\\nmany of our white brothers have evidenced by their presence here\\r\\ntoday that they have come to realize that their destiny is part\\r\\nof our destiny.                                                                                                                                                                                                      \n[10] So even though we face the difficulties of today and tomorrow\\r\\nI still have a dream. It is a dream deeply rooted in the American\\r\\ndream.                                                                                                                                                                                                                                                                                                                                                      \n[11] I have a dream that one day this nation will rise up and live\\r\\nout the true meaning of its creed: 'We hold these truths to be\\r\\nself-evident; that all men are created equal.\"                                                                                                                                                                                                                                                                                                                 \n[12] I have a dream that one day on the red hills of Georgia the\\r\\nsons of former slaves and the sons of former slave owners will be\\r\\nable to sit together at the table of brotherhood.                                                                                                                                                                                                                                                                                                             \n[13] I have a dream that one day even the state of Mississippi, a\\r\\nstate sweltering with the heat of injustice, sweltering with the\\r\\nheat of oppression, will be transformed into an oasis of freedom\\r\\nand justice.                                                                                                                                                                                                                                                                              \n[14] I have a dream that little children will one day live in a\\r\\nnation where they will not be judged by the color of their skin\\r\\nbut by the content of their character.                                                                                                                                                                                                                                                                                                                           \n[15] I have a dream today.                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n[16] I have a dream that one day down in Alabama, with its vicious\\r\\nracists, with its Governor having his lips dripping with the\\r\\nwords of interposition and nullification, one day right there in\\r\\nAlabama little black boys and black girls will be able to join\\r\\nhands with little white boys and white girls as sisters and\\r\\nbrothers.                                                                                                                                                   \n[17] I have a dream today.                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n[18] I have a dream that one day every valley shall be exalted,\\r\\nevery hill and mountain shall be made low, the rough places\\r\\nplains, and the crooked places will be made straight, and before\\r\\nthe Lord will be revealed, and all flesh shall see it together.                                                                                                                                                                                                                                  \n[19] This is our hope. This is the faith that I go back to the\\r\\nmount with. With this faith we will be able to hew out of the\\r\\nmountain of despair a stone of hope. With this faith we will be\\r\\nable to transform the genuine discords of our nation into a\\r\\nbeautiful symphony of brotherhood. With this faith we will be\\r\\nable to work together, pray together; to struggle together, to go\\r\\nto jail together, to stand up for freedom forever, )mowing that\\r\\nwe will be free one day. \n[20] And I say to you today my friends, let freedom ring. From the\\r\\nprodigious hilltops of New Hampshire, let freedom ring. From the\\r\\nmighty mountains of New York, let freedom ring. From the mighty\\r\\nAlleghenies of Pennsylvania!                                                                                                                                                                                                                                                              \n[21] Let freedom ring from the snow capped Rockies of Colorado!                                                                                                                                                                                                                                                                                                                                                                                                                                        \n[22] Let freedom ring from the curvaceous slopes of California!                                                                                                                                                                                                                                                                                                                                                                                                                                        \n[23] But not only there; let freedom ring from the Stone Mountain\\r\\nof Georgia!                                                                                                                                                                                                                                                                                                                                                                                                                       \n[24] Let freedom ring from Lookout Mountain in Tennessee!                                                                                                                                                                                                                                                                                                                                                                                                                                              \n[25] Let freedom ring from every hill and molehill in Mississippi.\\r\\nFrom every mountainside, let freedom ring.                                                                                                                                                                                                                                                                                                                                                                                       \n[26] And when this happens, when we allow freedom to ring, when we\\r\\nlet it ring from every village and hamlet, from every state and\\r\\nevery city, we will be able to speed up that day when all of\\r\\nGod's children, black men and white men, Jews and Gentiles,\\r\\nProtestants and Catholics, will be able to join hands and sing in\\r\\nthe words of the old Negro spiritual, \"Free at last! Free at\\r\\nlast! Thank God almighty, we're free at last!\"                                            \n\n\n<<TermDocumentMatrix (terms: 260, documents: 26)>>\nNon-/sparse entries: 383/6377\nSparsity           : 94%\nMaximal term length: 14\nWeighting          : term frequency (tf)\nSample             :\n         Docs\nTerms     16 18 19 2 20 26 3 6 8 9\n  able     1  0  3 0  0  2 0 0 0 0\n  day      2  1  1 0  0  1 0 0 0 0\n  dream    1  1  0 0  0  0 0 0 0 0\n  every    0  2  0 0  0  3 0 0 0 0\n  freedom  0  0  1 0  3  1 0 0 0 0\n  let      0  0  0 0  3  1 0 0 0 0\n  negro    0  0  0 1  0  1 2 0 0 1\n  one      2  1  1 0  0  0 2 0 0 0\n  ring     0  0  0 0  3  2 0 1 0 0\n  today    0  0  0 1  1  0 0 0 0 1\n\n\nfreedom     one    ring   dream     let     day \n     13      12      12      11      10       9"
  },
  {
    "objectID": "assignments/a6/a6.html",
    "href": "assignments/a6/a6.html",
    "title": "Assignment 6",
    "section": "",
    "text": "[1] \"District\"        \"Sex\"             \"Age\"             \"Edu\"            \n [5] \"Arear\"           \"Career\"          \"Career8\"         \"Ethnic\"         \n [9] \"Party\"           \"PartyID\"         \"Tondu\"           \"Tondu3\"         \n[13] \"nI2\"             \"votetsai\"        \"green\"           \"votetsai_nm\"    \n[17] \"votetsai_all\"    \"Independence\"    \"Unification\"     \"sq\"             \n[21] \"Taiwanese\"       \"edu\"             \"female\"          \"whitecollar\"    \n[25] \"lowincome\"       \"income\"          \"income_nm\"       \"age\"            \n[29] \"KMT\"             \"DPP\"             \"npp\"             \"noparty\"        \n[33] \"pfp\"             \"South\"           \"north\"           \"Minnan_father\"  \n[37] \"Mainland_father\" \"Econ_worse\"      \"Inequality\"      \"inequality5\"    \n[41] \"econworse5\"      \"Govt_for_public\" \"pubwelf5\"        \"Govt_dont_care\" \n[45] \"highincome\"      \"votekmt\"         \"votekmt_nm\"      \"Blue\"           \n[49] \"Green\"           \"No_Party\"        \"voteblue\"        \"voteblue_nm\"    \n[53] \"votedpp_1\"       \"votekmt_1\"      \n\n\n\nCall:\nglm(formula = votetsai ~ female, family = binomial, data = TEDS_2016)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4180  -1.3889   0.9546   0.9797   0.9797  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.54971    0.08245   6.667 2.61e-11 ***\nfemale      -0.06517    0.11644  -0.560    0.576    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1666.5  on 1260  degrees of freedom\nResidual deviance: 1666.2  on 1259  degrees of freedom\n  (429 observations deleted due to missingness)\nAIC: 1670.2\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "assignments/a6/a6.html#trying-out-for-taiwan-data",
    "href": "assignments/a6/a6.html#trying-out-for-taiwan-data",
    "title": "Assignment 6",
    "section": "Trying out for Taiwan data",
    "text": "Trying out for Taiwan data\n\n\n [1] \"District\"        \"Sex\"             \"Age\"             \"Edu\"            \n [5] \"Arear\"           \"Career\"          \"Career8\"         \"Ethnic\"         \n [9] \"Party\"           \"PartyID\"         \"Tondu\"           \"Tondu3\"         \n[13] \"nI2\"             \"votetsai\"        \"green\"           \"votetsai_nm\"    \n[17] \"votetsai_all\"    \"Independence\"    \"Unification\"     \"sq\"             \n[21] \"Taiwanese\"       \"edu\"             \"female\"          \"whitecollar\"    \n[25] \"lowincome\"       \"income\"          \"income_nm\"       \"age\"            \n[29] \"KMT\"             \"DPP\"             \"npp\"             \"noparty\"        \n[33] \"pfp\"             \"South\"           \"north\"           \"Minnan_father\"  \n[37] \"Mainland_father\" \"Econ_worse\"      \"Inequality\"      \"inequality5\"    \n[41] \"econworse5\"      \"Govt_for_public\" \"pubwelf5\"        \"Govt_dont_care\" \n[45] \"highincome\"      \"votekmt\"         \"votekmt_nm\"      \"Blue\"           \n[49] \"Green\"           \"No_Party\"        \"voteblue\"        \"voteblue_nm\"    \n[53] \"votedpp_1\"       \"votekmt_1\"      \n\n\n\nCall:\nglm(formula = votetsai ~ female, family = binomial, data = TEDS_2016)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4180  -1.3889   0.9546   0.9797   0.9797  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.54971    0.08245   6.667 2.61e-11 ***\nfemale      -0.06517    0.11644  -0.560    0.576    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1666.5  on 1260  degrees of freedom\nResidual deviance: 1666.2  on 1259  degrees of freedom\n  (429 observations deleted due to missingness)\nAIC: 1670.2\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "assignments/a6/a6.html#interpreting-the-first-logistic-regression-model",
    "href": "assignments/a6/a6.html#interpreting-the-first-logistic-regression-model",
    "title": "Assignment 6",
    "section": "Interpreting the first logistic regression model",
    "text": "Interpreting the first logistic regression model\nBased on the output of the logistic regression model, the coefficient for the female variable is -0.06517, and the p-value is 0.576. Since the p-value is greater than the standard significance level of 0.05, we fail to reject the null hypothesis, and there is no evidence to suggest that female voters are more likely to vote for President Tsai than male voters in this model.\nThe intercept of the model is 0.54971, which represents the log-odds of votetsai (voting for Tsai Ing-wen) for the reference group (male voters) in this case. The negative coefficient for the female variable (-0.06517) indicates that the log-odds of votetsai for female voters are slightly lower than for male voters, but this difference is not statistically significant.\nIt is essential to note that this model only includes the female predictor variable. Adding more variables (e.g., party ID, demographics, or issue-specific variables) may improve the model and provide more insights into factors affecting voting for President Tsai, which is what the next section will attempt to do.\n\n\n\nCall:\nglm(formula = votetsai ~ female + KMT + DPP + age + edu + income, \n    family = binomial, data = TEDS_2016)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7360  -0.3673   0.2408   0.2946   2.5408  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.618640   0.592084   2.734  0.00626 ** \nfemale       0.047406   0.177403   0.267  0.78930    \nKMT         -3.156273   0.250360 -12.607  < 2e-16 ***\nDPP          2.888943   0.267968  10.781  < 2e-16 ***\nage         -0.011808   0.007164  -1.648  0.09931 .  \nedu         -0.184604   0.083102  -2.221  0.02632 *  \nincome       0.013727   0.034382   0.399  0.68971    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1661.76  on 1256  degrees of freedom\nResidual deviance:  836.15  on 1250  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 850.15\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "assignments/a6/a6.html#interpretation-for-the-updated-model",
    "href": "assignments/a6/a6.html#interpretation-for-the-updated-model",
    "title": "Assignment 6",
    "section": "Interpretation for the updated model",
    "text": "Interpretation for the updated model\nBased on the output of the logistic regression model with additional predictors, here is the interpretation of the results:\nFemale: The coefficient for the female variable is 0.047406 with a p-value of 0.78930. The p-value is greater than 0.05, so the effect of the female variable is not statistically significant. This means that there is no evidence to suggest that female voters are more likely to vote for President Tsai compared to male voters, after controlling for other variables.\nKMT: The coefficient for the KMT variable is -3.156273 with a p-value close to 0 (p < 2e-16). This indicates that respondents with a stronger KMT party affiliation are significantly less likely to vote for President Tsai.\nDPP: The coefficient for the DPP variable is 2.888943 with a p-value close to 0 (p < 2e-16). This suggests that respondents with a stronger DPP party affiliation are significantly more likely to vote for President Tsai.\nAge: The coefficient for the age variable is -0.011808 with a p-value of 0.09931. The p-value is slightly greater than 0.05, so the effect of age is not statistically significant at the 0.05 level. However, the negative coefficient suggests that older respondents are somewhat less likely to vote for President Tsai, but this relationship is weak.\nEdu: The coefficient for the edu variable is -0.184604 with a p-value of 0.02632. The negative coefficient indicates that respondents with higher education levels are more likely to vote for President Tsai, and this effect is statistically significant (p < 0.05).\nIncome: The coefficient for the income variable is 0.013727 with a p-value of 0.68971. The p-value is greater than 0.05, so the effect of income is not statistically significant. This means that there is no evidence to suggest that income levels significantly influence the likelihood of voting for President Tsai.\nIn summary, the most significant predictors in this model are KMT and DPP party affiliations, which have strong and statistically significant effects on the likelihood of voting for President Tsai. Education also has a significant effect, while the female, age, and income variables are not statistically significant in this model."
  },
  {
    "objectID": "assignments/a6/a6.html#coefficient-plots-for-the-two-models",
    "href": "assignments/a6/a6.html#coefficient-plots-for-the-two-models",
    "title": "Assignment 6",
    "section": "Coefficient plots for the two models",
    "text": "Coefficient plots for the two models\n\n\n\n\n\n\n\n\n\n\nStart:  AIC=793.13\nvotetsai ~ female + KMT + DPP + age + edu + income + Independence + \n    Econ_worse + Govt_dont_care + Minnan_father + Mainland_father + \n    Taiwanese\n\n                  Df Deviance    AIC\n- Govt_dont_care   1   767.14 791.14\n- age              1   767.31 791.31\n- female           1   767.40 791.40\n- income           1   767.49 791.49\n- Minnan_father    1   768.09 792.09\n- edu              1   768.18 792.18\n<none>                 767.13 793.13\n- Econ_worse       1   769.82 793.82\n- Mainland_father  1   774.99 798.99\n- Independence     1   784.68 808.68\n- Taiwanese        1   787.92 811.92\n- DPP              1   884.02 908.02\n- KMT              1   954.40 978.40\n\nStep:  AIC=791.14\nvotetsai ~ female + KMT + DPP + age + edu + income + Independence + \n    Econ_worse + Minnan_father + Mainland_father + Taiwanese\n\n                  Df Deviance    AIC\n- age              1   767.32 789.32\n- female           1   767.40 789.40\n- income           1   767.49 789.49\n- Minnan_father    1   768.11 790.11\n- edu              1   768.18 790.18\n<none>                 767.14 791.14\n- Econ_worse       1   769.84 791.84\n+ Govt_dont_care   1   767.13 793.13\n- Mainland_father  1   775.08 797.08\n- Independence     1   784.68 806.68\n- Taiwanese        1   787.92 809.92\n- DPP              1   884.68 906.68\n- KMT              1   954.41 976.41\n\nStep:  AIC=789.32\nvotetsai ~ female + KMT + DPP + edu + income + Independence + \n    Econ_worse + Minnan_father + Mainland_father + Taiwanese\n\n                  Df Deviance    AIC\n- female           1   767.59 787.59\n- income           1   767.70 787.70\n- Minnan_father    1   768.21 788.21\n<none>                 767.32 789.32\n- Econ_worse       1   770.11 790.11\n- edu              1   770.33 790.33\n+ age              1   767.14 791.14\n+ Govt_dont_care   1   767.31 791.31\n- Mainland_father  1   775.09 795.09\n- Independence     1   784.72 804.72\n- Taiwanese        1   787.93 807.93\n- DPP              1   885.39 905.39\n- KMT              1   954.61 974.61\n\nStep:  AIC=787.59\nvotetsai ~ KMT + DPP + edu + income + Independence + Econ_worse + \n    Minnan_father + Mainland_father + Taiwanese\n\n                  Df Deviance    AIC\n- income           1   767.95 785.95\n- Minnan_father    1   768.49 786.49\n<none>                 767.59 787.59\n- edu              1   770.43 788.43\n- Econ_worse       1   770.44 788.44\n+ female           1   767.32 789.32\n+ age              1   767.40 789.40\n+ Govt_dont_care   1   767.58 789.58\n- Mainland_father  1   775.21 793.21\n- Independence     1   785.27 803.27\n- Taiwanese        1   787.94 805.94\n- DPP              1   886.58 904.58\n- KMT              1   955.77 973.77\n\nStep:  AIC=785.95\nvotetsai ~ KMT + DPP + edu + Independence + Econ_worse + Minnan_father + \n    Mainland_father + Taiwanese\n\n                  Df Deviance    AIC\n- Minnan_father    1   768.87 784.87\n<none>                 767.95 785.95\n- edu              1   770.43 786.43\n- Econ_worse       1   770.69 786.69\n+ income           1   767.59 787.59\n+ female           1   767.70 787.70\n+ age              1   767.74 787.74\n+ Govt_dont_care   1   767.95 787.95\n- Mainland_father  1   775.59 791.59\n- Independence     1   785.62 801.62\n- Taiwanese        1   788.14 804.14\n- DPP              1   888.19 904.19\n- KMT              1   956.43 972.43\n\nStep:  AIC=784.87\nvotetsai ~ KMT + DPP + edu + Independence + Econ_worse + Mainland_father + \n    Taiwanese\n\n                  Df Deviance    AIC\n<none>                 768.87 784.87\n- Econ_worse       1   771.48 785.48\n- edu              1   771.59 785.59\n+ Minnan_father    1   767.95 785.95\n+ income           1   768.49 786.49\n+ female           1   768.61 786.61\n+ age              1   768.74 786.74\n+ Govt_dont_care   1   768.86 786.86\n- Mainland_father  1   775.96 789.96\n- Independence     1   786.35 800.35\n- Taiwanese        1   788.59 802.59\n- DPP              1   888.66 902.66\n- KMT              1   956.56 970.56\n\n\n\nCall:\nglm(formula = votetsai ~ KMT + DPP + edu + Independence + Econ_worse + \n    Mainland_father + Taiwanese, family = binomial, data = TEDS_2016)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.0043  -0.3074   0.1731   0.4096   2.7622  \n\nCoefficients:\n                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)      0.05688    0.27971   0.203  0.83886    \nKMT             -2.88317    0.25561 -11.280  < 2e-16 ***\nDPP              2.47837    0.27407   9.043  < 2e-16 ***\nedu             -0.10296    0.06257  -1.645  0.09989 .  \nIndependence     1.00339    0.24761   4.052 5.07e-05 ***\nEcon_worse       0.30187    0.18640   1.619  0.10535    \nMainland_father -0.85644    0.33052  -2.591  0.00956 ** \nTaiwanese        0.86729    0.19455   4.458 8.28e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1661.76  on 1256  degrees of freedom\nResidual deviance:  768.87  on 1249  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 784.87\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "assignments/a6/a6.html#interpreting-the-best-model",
    "href": "assignments/a6/a6.html#interpreting-the-best-model",
    "title": "Assignment 6",
    "section": "Interpreting the best model",
    "text": "Interpreting the best model\nThis is the best model selected by stepAIC based on AIC criteria. The model predicts the likelihood of voting for Tsai Ing-wen (votetsai) using the following predictors: KMT, DPP, edu, Independence, Econ_worse, Mainland_father, and Taiwanese.\nHere’s the interpretation of the model:\nKMT (Kuomintang) Party ID: The coefficient is -2.88317, and it is highly significant (p < 2e-16). A one-unit increase in KMT affiliation is associated with a decrease in the log-odds of voting for Tsai Ing-wen by 2.88317 units, holding other variables constant. In other words, KMT supporters are less likely to vote for Tsai Ing-wen.\nDPP (Democratic Progressive Party) Party ID: The coefficient is 2.47837, and it is highly significant (p < 2e-16). A one-unit increase in DPP affiliation is associated with an increase in the log-odds of voting for Tsai Ing-wen by 2.47837 units, holding other variables constant. DPP supporters are more likely to vote for Tsai Ing-wen.\nEducation (edu): The coefficient is -0.10296, and it is marginally significant (p = 0.09989). A one-unit increase in education level is associated with a decrease in the log-odds of voting for Tsai Ing-wen by 0.10296 units, holding other variables constant. More educated individuals are slightly less likely to vote for Tsai Ing-wen.\nIndependence: The coefficient is 1.00339, and it is highly significant (p = 5.07e-05). A one-unit increase in support for Taiwan’s independence is associated with an increase in the log-odds of voting for Tsai Ing-wen by 1.00339 units, holding other variables constant. Those who support Taiwan’s independence are more likely to vote for Tsai Ing-wen.\nEconomic evaluation (Econ_worse): The coefficient is 0.30187, and it is not significant (p = 0.10535). A one-unit increase in negative economic evaluation is associated with an increase in the log-odds of voting for Tsai Ing-wen by 0.30187 units, holding other variables constant. However, this effect is not statistically significant.\nMainland father (Mainland_father): The coefficient is -0.85644, and it is significant (p = 0.00956). A one-unit increase in being a descendent of mainland China is associated with a decrease in the log-odds of voting for Tsai Ing-wen by 0.85644 units, holding other variables constant. Individuals with mainland Chinese ancestry are less likely to vote for Tsai Ing-wen.\nSelf-identified Taiwanese (Taiwanese): The coefficient is 0.86729, and it is highly significant (p = 8.28e-06). A one-unit increase in self-identification as Taiwanese is associated with an increase in the log-odds of voting for Tsai Ing-wen by 0.86729 units, holding other variables constant. Self-identified Taiwanese are more likely to vote for Tsai Ing-wen.\nThe model has an AIC of 784.87, and the residual deviance is 768.87 on 1249 degrees of freedom. This model provides a better fit compared"
  },
  {
    "objectID": "assignments/a6/a6.html#lab-assignment",
    "href": "assignments/a6/a6.html#lab-assignment",
    "title": "Assignment 6",
    "section": "Lab Assignment",
    "text": "Lab Assignment\n\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\n\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\n       fit      lwr      upr\n1 34.55384 33.44846 35.65922\n2 29.80359 29.00741 30.59978\n3 25.05335 24.47413 25.63256\n4 20.30310 19.73159 20.87461\n\n\n       fit       lwr      upr\n1 34.55384 22.291923 46.81576\n2 29.80359 17.565675 42.04151\n3 25.05335 12.827626 37.27907\n4 20.30310  8.077742 32.52846\n\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCall:\nlm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + \n    tax + ptratio + black + lstat, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5984  -2.7386  -0.5046   1.7273  26.2373 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***\ncrim         -0.108413   0.032779  -3.307 0.001010 ** \nzn            0.045845   0.013523   3.390 0.000754 ***\nchas          2.718716   0.854240   3.183 0.001551 ** \nnox         -17.376023   3.535243  -4.915 1.21e-06 ***\nrm            3.801579   0.406316   9.356  < 2e-16 ***\ndis          -1.492711   0.185731  -8.037 6.84e-15 ***\nrad           0.299608   0.063402   4.726 3.00e-06 ***\ntax          -0.011778   0.003372  -3.493 0.000521 ***\nptratio      -0.946525   0.129066  -7.334 9.24e-13 ***\nblack         0.009291   0.002674   3.475 0.000557 ***\nlstat        -0.522553   0.047424 -11.019  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.736 on 494 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 \nF-statistic: 128.2 on 11 and 494 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16\n\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\n\n\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US     \n No :118   No :142  \n Yes:282   Yes:258  \n                    \n                    \n                    \n                    \n\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Age:Price, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "assignments/a8/a8.html#generate-response-vector-𝑦-and-plot-x-and-y",
    "href": "assignments/a8/a8.html#generate-response-vector-𝑦-and-plot-x-and-y",
    "title": "Assignment 8",
    "section": "Generate response vector 𝑦 and plot x and y:",
    "text": "Generate response vector 𝑦 and plot x and y:"
  },
  {
    "objectID": "assignments/a8/a8.html#load-the-leaps-package-and-use-the-regsubsets-function",
    "href": "assignments/a8/a8.html#load-the-leaps-package-and-use-the-regsubsets-function",
    "title": "Assignment 8",
    "section": "Load the leaps package and use the regsubsets() function:",
    "text": "Load the leaps package and use the regsubsets() function:\n\n\nSubset selection object\nCall: regsubsets.formula(y ~ poly(X, 10, raw = T), data = data.frame(y, \n    X), nvmax = 10)\n10 Variables  (and intercept)\n                       Forced in Forced out\npoly(X, 10, raw = T)1      FALSE      FALSE\npoly(X, 10, raw = T)2      FALSE      FALSE\npoly(X, 10, raw = T)3      FALSE      FALSE\npoly(X, 10, raw = T)4      FALSE      FALSE\npoly(X, 10, raw = T)5      FALSE      FALSE\npoly(X, 10, raw = T)6      FALSE      FALSE\npoly(X, 10, raw = T)7      FALSE      FALSE\npoly(X, 10, raw = T)8      FALSE      FALSE\npoly(X, 10, raw = T)9      FALSE      FALSE\npoly(X, 10, raw = T)10     FALSE      FALSE\n1 subsets of each size up to 10\nSelection Algorithm: exhaustive\n          poly(X, 10, raw = T)1 poly(X, 10, raw = T)2 poly(X, 10, raw = T)3\n1  ( 1 )  \"*\"                   \" \"                   \" \"                  \n2  ( 1 )  \"*\"                   \" \"                   \" \"                  \n3  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n4  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n5  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n6  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n7  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n8  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n9  ( 1 )  \"*\"                   \"*\"                   \" \"                  \n10  ( 1 ) \"*\"                   \"*\"                   \"*\"                  \n          poly(X, 10, raw = T)4 poly(X, 10, raw = T)5 poly(X, 10, raw = T)6\n1  ( 1 )  \" \"                   \" \"                   \" \"                  \n2  ( 1 )  \"*\"                   \" \"                   \" \"                  \n3  ( 1 )  \" \"                   \" \"                   \" \"                  \n4  ( 1 )  \" \"                   \"*\"                   \" \"                  \n5  ( 1 )  \" \"                   \"*\"                   \"*\"                  \n6  ( 1 )  \" \"                   \" \"                   \" \"                  \n7  ( 1 )  \" \"                   \"*\"                   \"*\"                  \n8  ( 1 )  \"*\"                   \" \"                   \"*\"                  \n9  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n10  ( 1 ) \"*\"                   \"*\"                   \"*\"                  \n          poly(X, 10, raw = T)7 poly(X, 10, raw = T)8 poly(X, 10, raw = T)9\n1  ( 1 )  \" \"                   \" \"                   \" \"                  \n2  ( 1 )  \" \"                   \" \"                   \" \"                  \n3  ( 1 )  \" \"                   \" \"                   \" \"                  \n4  ( 1 )  \" \"                   \" \"                   \" \"                  \n5  ( 1 )  \" \"                   \" \"                   \" \"                  \n6  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n7  ( 1 )  \" \"                   \"*\"                   \" \"                  \n8  ( 1 )  \" \"                   \"*\"                   \"*\"                  \n9  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n10  ( 1 ) \"*\"                   \"*\"                   \"*\"                  \n          poly(X, 10, raw = T)10\n1  ( 1 )  \" \"                   \n2  ( 1 )  \" \"                   \n3  ( 1 )  \" \"                   \n4  ( 1 )  \" \"                   \n5  ( 1 )  \" \"                   \n6  ( 1 )  \" \"                   \n7  ( 1 )  \"*\"                   \n8  ( 1 )  \"*\"                   \n9  ( 1 )  \"*\"                   \n10  ( 1 ) \"*\""
  },
  {
    "objectID": "assignments/a8/a8.html#find-the-best-model-according-to-cp-bic-and-adjusted-r2-and-report-the-coefficients-of-the-best-model",
    "href": "assignments/a8/a8.html#find-the-best-model-according-to-cp-bic-and-adjusted-r2-and-report-the-coefficients-of-the-best-model",
    "title": "Assignment 8",
    "section": "Find the best model according to Cp, BIC, and adjusted R2, and report the coefficients of the best model:",
    "text": "Find the best model according to Cp, BIC, and adjusted R2, and report the coefficients of the best model:\n\n\nBest Model (Cp): 4 \n\n\nBest Model (BIC): 3 \n\n\nBest Model (Adjusted R2): 4 \n\n\n          (Intercept) poly(X, 10, raw = T)1 poly(X, 10, raw = T)2 \n             4.061507              8.975280              1.876209 \npoly(X, 10, raw = T)3 \n             1.017639"
  },
  {
    "objectID": "assignments/a8/a8.html#repeat-step-3-using-forward-stepwise-selection-and-backward-stepwise-selection-and-compare-the-results",
    "href": "assignments/a8/a8.html#repeat-step-3-using-forward-stepwise-selection-and-backward-stepwise-selection-and-compare-the-results",
    "title": "Assignment 8",
    "section": "Repeat step 3 using forward stepwise selection and backward stepwise selection and compare the results:",
    "text": "Repeat step 3 using forward stepwise selection and backward stepwise selection and compare the results:\n\n\nSubset selection object\nCall: regsubsets.formula(y ~ poly(X, 10, raw = T), data = data.frame(y, \n    X), nvmax = 10, method = \"forward\")\n10 Variables  (and intercept)\n                       Forced in Forced out\npoly(X, 10, raw = T)1      FALSE      FALSE\npoly(X, 10, raw = T)2      FALSE      FALSE\npoly(X, 10, raw = T)3      FALSE      FALSE\npoly(X, 10, raw = T)4      FALSE      FALSE\npoly(X, 10, raw = T)5      FALSE      FALSE\npoly(X, 10, raw = T)6      FALSE      FALSE\npoly(X, 10, raw = T)7      FALSE      FALSE\npoly(X, 10, raw = T)8      FALSE      FALSE\npoly(X, 10, raw = T)9      FALSE      FALSE\npoly(X, 10, raw = T)10     FALSE      FALSE\n1 subsets of each size up to 10\nSelection Algorithm: forward\n          poly(X, 10, raw = T)1 poly(X, 10, raw = T)2 poly(X, 10, raw = T)3\n1  ( 1 )  \"*\"                   \" \"                   \" \"                  \n2  ( 1 )  \"*\"                   \" \"                   \" \"                  \n3  ( 1 )  \"*\"                   \" \"                   \"*\"                  \n4  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n5  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n6  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n7  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n8  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n9  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n10  ( 1 ) \"*\"                   \"*\"                   \"*\"                  \n          poly(X, 10, raw = T)4 poly(X, 10, raw = T)5 poly(X, 10, raw = T)6\n1  ( 1 )  \" \"                   \" \"                   \" \"                  \n2  ( 1 )  \"*\"                   \" \"                   \" \"                  \n3  ( 1 )  \"*\"                   \" \"                   \" \"                  \n4  ( 1 )  \"*\"                   \" \"                   \" \"                  \n5  ( 1 )  \"*\"                   \"*\"                   \" \"                  \n6  ( 1 )  \"*\"                   \"*\"                   \" \"                  \n7  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n8  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n9  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n10  ( 1 ) \"*\"                   \"*\"                   \"*\"                  \n          poly(X, 10, raw = T)7 poly(X, 10, raw = T)8 poly(X, 10, raw = T)9\n1  ( 1 )  \" \"                   \" \"                   \" \"                  \n2  ( 1 )  \" \"                   \" \"                   \" \"                  \n3  ( 1 )  \" \"                   \" \"                   \" \"                  \n4  ( 1 )  \" \"                   \" \"                   \" \"                  \n5  ( 1 )  \" \"                   \" \"                   \" \"                  \n6  ( 1 )  \" \"                   \" \"                   \"*\"                  \n7  ( 1 )  \" \"                   \" \"                   \"*\"                  \n8  ( 1 )  \"*\"                   \" \"                   \"*\"                  \n9  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n10  ( 1 ) \"*\"                   \"*\"                   \"*\"                  \n          poly(X, 10, raw = T)10\n1  ( 1 )  \" \"                   \n2  ( 1 )  \" \"                   \n3  ( 1 )  \" \"                   \n4  ( 1 )  \" \"                   \n5  ( 1 )  \" \"                   \n6  ( 1 )  \" \"                   \n7  ( 1 )  \" \"                   \n8  ( 1 )  \" \"                   \n9  ( 1 )  \" \"                   \n10  ( 1 ) \"*\"                   \n\n\nSubset selection object\nCall: regsubsets.formula(y ~ poly(X, 10, raw = T), data = data.frame(y, \n    X), nvmax = 10, method = \"backward\")\n10 Variables  (and intercept)\n                       Forced in Forced out\npoly(X, 10, raw = T)1      FALSE      FALSE\npoly(X, 10, raw = T)2      FALSE      FALSE\npoly(X, 10, raw = T)3      FALSE      FALSE\npoly(X, 10, raw = T)4      FALSE      FALSE\npoly(X, 10, raw = T)5      FALSE      FALSE\npoly(X, 10, raw = T)6      FALSE      FALSE\npoly(X, 10, raw = T)7      FALSE      FALSE\npoly(X, 10, raw = T)8      FALSE      FALSE\npoly(X, 10, raw = T)9      FALSE      FALSE\npoly(X, 10, raw = T)10     FALSE      FALSE\n1 subsets of each size up to 10\nSelection Algorithm: backward\n          poly(X, 10, raw = T)1 poly(X, 10, raw = T)2 poly(X, 10, raw = T)3\n1  ( 1 )  \"*\"                   \" \"                   \" \"                  \n2  ( 1 )  \"*\"                   \" \"                   \" \"                  \n3  ( 1 )  \"*\"                   \" \"                   \" \"                  \n4  ( 1 )  \"*\"                   \" \"                   \" \"                  \n5  ( 1 )  \"*\"                   \" \"                   \" \"                  \n6  ( 1 )  \"*\"                   \" \"                   \" \"                  \n7  ( 1 )  \"*\"                   \" \"                   \" \"                  \n8  ( 1 )  \"*\"                   \" \"                   \" \"                  \n9  ( 1 )  \"*\"                   \"*\"                   \" \"                  \n10  ( 1 ) \"*\"                   \"*\"                   \"*\"                  \n          poly(X, 10, raw = T)4 poly(X, 10, raw = T)5 poly(X, 10, raw = T)6\n1  ( 1 )  \" \"                   \" \"                   \" \"                  \n2  ( 1 )  \"*\"                   \" \"                   \" \"                  \n3  ( 1 )  \"*\"                   \"*\"                   \" \"                  \n4  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n5  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n6  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n7  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n8  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n9  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n10  ( 1 ) \"*\"                   \"*\"                   \"*\"                  \n          poly(X, 10, raw = T)7 poly(X, 10, raw = T)8 poly(X, 10, raw = T)9\n1  ( 1 )  \" \"                   \" \"                   \" \"                  \n2  ( 1 )  \" \"                   \" \"                   \" \"                  \n3  ( 1 )  \" \"                   \" \"                   \" \"                  \n4  ( 1 )  \" \"                   \" \"                   \" \"                  \n5  ( 1 )  \" \"                   \"*\"                   \" \"                  \n6  ( 1 )  \" \"                   \"*\"                   \" \"                  \n7  ( 1 )  \"*\"                   \"*\"                   \" \"                  \n8  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n9  ( 1 )  \"*\"                   \"*\"                   \"*\"                  \n10  ( 1 ) \"*\"                   \"*\"                   \"*\"                  \n          poly(X, 10, raw = T)10\n1  ( 1 )  \" \"                   \n2  ( 1 )  \" \"                   \n3  ( 1 )  \" \"                   \n4  ( 1 )  \" \"                   \n5  ( 1 )  \" \"                   \n6  ( 1 )  \"*\"                   \n7  ( 1 )  \"*\"                   \n8  ( 1 )  \"*\"                   \n9  ( 1 )  \"*\"                   \n10  ( 1 ) \"*\"                   \n\n\nBest Model (Forward, BIC): 4 \n\n\nBest Model (Backward, BIC): 6"
  },
  {
    "objectID": "assignments/a8/a8.html#generate-predictor-x-and-noise-vector-𝜀along-with-response-vector-𝑦-and-plot-x-and-y",
    "href": "assignments/a8/a8.html#generate-predictor-x-and-noise-vector-𝜀along-with-response-vector-𝑦-and-plot-x-and-y",
    "title": "Assignment 8",
    "section": "Generate predictor X and noise vector 𝜀along with response vector 𝑦 and plot x and y:",
    "text": "Generate predictor X and noise vector 𝜀along with response vector 𝑦 and plot x and y:"
  },
  {
    "objectID": "assignments/taiwan-talk/taiwan.html",
    "href": "assignments/taiwan-talk/taiwan.html",
    "title": "Taiwan Talk - Cunningham",
    "section": "",
    "text": "# Load required libraries\nlibrary(tm)\n\nLoading required package: NLP\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(wordcloud2)\n\n# Read in the text from the provided content\ntext <- \"Taiwan, China, Nationalize, Policy-makers, Taiwan Strait, War, Soldiers, American, Geopolitics, Economics, Trade, Electronics, Supply-Chain, Semiconductors, OPEC, Oil, Saudi Arabia, Geopolitics, Military-bases, Island, Okinawa, War, Taiwan, China, Japan, Oil, Nationalistic, United States, China, Territory, Geopolitics, Aircraft, Trillion, Cost, Policy, Taiwan, Domestic Politics in China, CCP, Power, China, Power, China, Threat, Nationalistic, Xi, Power, Interest, CCP, Pressure, Taiwan, Taiwan, Claim, Imperialist, Claim, Claim, Independence, Diplomatic, Beijing, Fight, War, CCP, War, Military, Modernization, War, United States, Japan, Rising, Time, Military, Navy, Air Force, Opportunity, China, United States, Military, Industrial base, War, Rival, Power, Military, CCP, Tax, Companies, Goods, Public, Military, Taiwan Strait, China, Belligerent, China, Taiwan, Taiwan, United States, Status-Quo, Status-Quo, Fuzzy, De jure, Beijing, Domestic, Legitimacy, Deterring, United States, Taiwan, Security, Respond, Beijing, War, Win, Provoke, United States, Taiwan, War, Win, Taiwan, Nancy Pelosi, Taiwan, Military, Island, Provocative, White Paper, Media, White Paper, Journalist, Beijing, White Paper, 1993, 2000, 2022, Taiwan, White Paper, Xi Jinping, White Paper, 2000, Taiwan, One country two systems, Autonomy, Military, White Paper, Promise, Beijing, Compete, 2000, Force, 2000, 2000, Force, 2022, Force, Force, Provocation, Separatist, Red-line, Red-line, Anti-Secession Law, Reunification, Reunification, Reunification, War, Operation Causeway, Casualty, Amphibious, Military, Peacekeeping, Africa, Island, Offshore, Mao, Seizure, Taiwan, Debunk, Myths, Policy making, China, Opportunity, Population, Military, Military, Lean, Power, Xi, Taiwan, CCP, Xi, Elites, China, Xi, CCP, CCP, Timeline, Timeline, Succeed, 2049, Unification, Prerequisite, Rejuvenation, Vague, 2049, Taiwan, One country two systems, One country two systems, One country two systems, Timeline, 2025, 2027, centenary, Xi, CCP, Xi, Order, Military, centenary, 2049, Peace, Conflict, War, Conflict, Escalation, Miscalculation, Reckless, Taiwan, Asserting, Tensions, United States, Washington, Catastrophe, United States, Red-line, Taiwan, China, Intentional, Resolution, Politics, Taiwan, Xi Jinping, Taiwan, Power, DPP, DPP, KMT, KMT, KMT, KMT, Consensus, 92 consensus, One country two systems, KMT, DPP, William Lai, Taipei, Taiwan, Resolve, Objective, War, Tension, Objective, Status-Quo, Tension, Taiwan, KMT, Unification, Washington, DPP, Pro-Independence, KMT,KMT, Washington, CCP, DPP, Arms, Taiwan, United States, Arms, Washington, Taiwan, TECRO, TECRO, Nancy Pelosi, TECRO, China, United States, Nancy Pelosi, China, TECRO, Status-quo, Military, Escalation, Deterrent\""
  },
  {
    "objectID": "assignments/taiwan-talk/taiwan.html#creation-of-the-corpus-for-data-analysis",
    "href": "assignments/taiwan-talk/taiwan.html#creation-of-the-corpus-for-data-analysis",
    "title": "Taiwan Talk - Cunningham",
    "section": "Creation of the Corpus for data analysis",
    "text": "Creation of the Corpus for data analysis\n\n# Create a corpus from the text\ncorpus <- Corpus(VectorSource(text))\n\n# Convert to lowercase, remove punctuation, and remove numbers\ncorpus <- tm_map(corpus, content_transformer(tolower))\ncorpus <- tm_map(corpus, removePunctuation)\ncorpus <- tm_map(corpus, removeNumbers)\n\n# Convert the corpus to a term-document matrix\ntdm <- TermDocumentMatrix(corpus)\n\n# Convert the tdm to a matrix and calculate the frequency of each term\nm <- as.matrix(tdm)\nv <- sort(rowSums(m), decreasing=TRUE)\n\n# Create a dataframe of the most frequent terms and their frequencies\ndf <- data.frame(word = names(v), freq = v)"
  },
  {
    "objectID": "assignments/taiwan-talk/taiwan.html#creation-of-plots",
    "href": "assignments/taiwan-talk/taiwan.html#creation-of-plots",
    "title": "Taiwan Talk - Cunningham",
    "section": "Creation of plots",
    "text": "Creation of plots\n\n# Generate a word cloud\nwordcloud(words = df$word, freq = df$freq, scale=c(5,0.5), min.freq = 1, max.words=Inf, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, \"Dark2\"))\n\n# Load required libraries\nlibrary(tm)\nlibrary(ggplot2)\n\n\n\n# Create a corpus from the text\ncorpus <- Corpus(VectorSource(text))\n\n# Convert to lowercase, remove punctuation, and remove numbers\ncorpus <- tm_map(corpus, content_transformer(tolower))\ncorpus <- tm_map(corpus, removePunctuation)\ncorpus <- tm_map(corpus, removeNumbers)\n\n# Convert the corpus to a term-document matrix\ntdm <- TermDocumentMatrix(corpus)\n\n# Convert the tdm to a matrix and calculate the frequency of each term\nm <- as.matrix(tdm)\nv <- sort(rowSums(m), decreasing=TRUE)\n\n# Create a dataframe of the most frequent terms and their frequencies\ndf <- data.frame(word = names(v), freq = v)\n\n# Create a bar chart of the top 20 most frequent terms\nggplot(head(df, 20), aes(x=word, y=freq)) + geom_bar(stat=\"identity\", fill=\"dodgerblue\") + xlab(\"Words\") + ylab(\"Frequency\") + ggtitle(\"Most Frequent Words in Cunningham Talk\") + theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My research uses computational methods to address core questions in three areas: digital governance and censorship, the dynamics of political conflict, and the causes and consequences of democratic backsliding. I specialize in applying natural language processing and machine learning to analyze political behavior and institutions."
  },
  {
    "objectID": "research.html#democracy-and-internet-control-theory-and-evidence-from-transparency-reports-with-pengfei-zhang",
    "href": "research.html#democracy-and-internet-control-theory-and-evidence-from-transparency-reports-with-pengfei-zhang",
    "title": "Research",
    "section": "Democracy and Internet Control: Theory and Evidence from Transparency Reports (with Pengfei Zhang)",
    "text": "Democracy and Internet Control: Theory and Evidence from Transparency Reports (with Pengfei Zhang)\n\n\n\n\n\n\nAbstract\n\n\n\n\n\nInternet control has long been considered a feature of authoritarian regimes alone. Drawing data from Google and Twitter transparency reports, we observe that democratic countries remove an equal amount of content as their authoritarian counterparts. The distinction between the two regimes lies not in the quantity but in the method of content removal. Democracy refrains from government takedown and instead delegates the removal right to the users. This paper conjectures that politicians’ reputation concern is the key to understanding this phenomenon. To that end, we develop a political agency model that explains the stylized facts and derives testable hypotheses. Using the timing of elections as a natural experiment, we provide supporting evidence that the takedown requests from democratic governments decreased significantly as the election approached. This reputation effect is not observed in authoritarian regimes or other types of requests."
  },
  {
    "objectID": "research.html#two-types-of-censorship-with-pengfei-zhang",
    "href": "research.html#two-types-of-censorship-with-pengfei-zhang",
    "title": "Research",
    "section": "Two Types of Censorship (with Pengfei Zhang)",
    "text": "Two Types of Censorship (with Pengfei Zhang)\n\n\n\n\n\n\nAbstract\n\n\n\n\n\nThis study delves into the varied internet censorship strategies of autocratic regimes, emphasizing the pivotal role of national IT capacity. Utilizing panel data from the Freedom House, V-Dem Digital Society Project, OONI, and Google Transparency Report, we employ cluster analysis to identify two primary censorship models: a pervasive control regime exemplified by China, and an influence operation regime typified by Russia. Our analysis reveals that a country’s IT capacity is a key determinant in its choice of censorship strategy, with higher IT capacity linked to fewer content removal requests and a greater likelihood of complete internet shutdowns. This research provides novel insights into the relationship between technological capabilities and censorship methods in autocracies, offering a predictive framework for understanding and anticipating their digital governance strategies.",
    "crumbs": [
      "{{< fa microchip >}} Research"
    ]
  },
  {
    "objectID": "research.html#dissertation-proposal",
    "href": "research.html#dissertation-proposal",
    "title": "Research",
    "section": "Dissertation Proposal",
    "text": "Dissertation Proposal\nMy dissertation examines internet censorship across various regimes, focusing on the intricacies of content moderation and the political implications thereof.\nView Presentation Read Proposal"
  },
  {
    "objectID": "research.html#expanding-the-horizons-extension-of-borzyskowski-vabulas-2019",
    "href": "research.html#expanding-the-horizons-extension-of-borzyskowski-vabulas-2019",
    "title": "Research",
    "section": "Expanding the Horizons: Extension of Borzyskowski & Vabulas (2019)",
    "text": "Expanding the Horizons: Extension of Borzyskowski & Vabulas (2019)\n\n\n\n\n\n\nOverview\n\n\n\n\n\nPerplexed by the concept of contagion in IGO withdrawals, this extension explores the network structures and social dynamics within international organizations."
  },
  {
    "objectID": "research.html#conflibert-usage-manual-finetuning-guide",
    "href": "research.html#conflibert-usage-manual-finetuning-guide",
    "title": "Research",
    "section": "",
    "text": "This document (continually updated!) walks political scientists through the usage of a Large Language Model (LLM) on various tasks such as classification, masking, named entity recognition & question answering. Furthermore, the document outlines the process of finetuning the LLM on the users datasets with the help of a Google Colab script.\nView Manual"
  },
  {
    "objectID": "research.html#conflibert-usage-manual-finetuning-guide-with-conflibert-lab",
    "href": "research.html#conflibert-usage-manual-finetuning-guide-with-conflibert-lab",
    "title": "Research",
    "section": "ConfliBERT Usage Manual & Finetuning guide (With ConfliBERT Lab)",
    "text": "ConfliBERT Usage Manual & Finetuning guide (With ConfliBERT Lab)\nThis document (continually updated!) walks political scientists through the usage of a Large Language Model (LLM) on various tasks such as classification, masking, named entity recognition & question answering. Furthermore, the document outlines the process of finetuning the LLM on the users datasets with the help of a Google Colab script.\nView Manual"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "EPPS Math & Coding Camp (August 2024)\nR Programming and Data Analysis\n\nCreated and maintained course website eppsmathcodingcamp.github.io\nTaught R programming fundamentals, data manipulation (dplyr), and visualization (ggplot2)\nDeveloped interactive tutorials and exercises for 5-day intensive program\nBuilt GitHub-based infrastructure for version control and content delivery\n\n\n\n\nEPPS 6313 - Introduction to Quantitative Methods\nTeaching Assistant for Dr. Pengfei Zhang\n\nLed weekly STATA lab sessions focusing on regression analysis\nGuided students through practical applications of statistical methods\nProvided individualized support for data analysis projects\n\nEPPS 6316 - Applied Regression\nTeaching Assistant for Dr. Vito D’Orazio\n\nConducted advanced R programming labs\nFocused on complex regression techniques and econometric analysis\nDeveloped practical exercises for applied data analysis"
  },
  {
    "objectID": "teaching.html#epps-6313---introduction-to-quantitative-methods",
    "href": "teaching.html#epps-6313---introduction-to-quantitative-methods",
    "title": "Teaching",
    "section": "EPPS 6313 - Introduction to Quantitative Methods",
    "text": "EPPS 6313 - Introduction to Quantitative Methods\nLectured by Dr. Pengfei Zhang\nIn my capacity as a Teaching Assistant for EPPS 6313 - Introduction to Quantitative Methods, lectured by Dr. Pengfei Zhang, I led lab sessions focused on utilizing STATA for regression analysis and other essential statistical methods. My primary goal in these labs was to bridge the gap between theory and practice, ensuring students gain a thorough understanding of quantitative methods and their application. The sessions are designed to provide hands-on experience in STATA, equipping students with the necessary skills to conduct robust policy analysis and research.",
    "crumbs": [
      "{{< fa pencil >}} Teaching"
    ]
  },
  {
    "objectID": "teaching.html#epps-6316---applied-regression",
    "href": "teaching.html#epps-6316---applied-regression",
    "title": "Teaching",
    "section": "EPPS 6316 - Applied Regression",
    "text": "EPPS 6316 - Applied Regression\nLectured by Dr. Vito D’Orazio\nAs a Teaching Assistant for EPPS 6316 - Applied Regression, under Dr. Vito D’Orazio’s tutelage, my responsibilities included conducting advanced lab sessions that predominantly use R. These labs explore more complex aspects of regression analysis, expanding upon the foundational knowledge imparted in earlier courses. My focus in these sessions was to challenge and enhance students’ understanding of applied regression, especially in the context of econometric analysis. Through practical exercises in R, students develop a deeper proficiency in handling sophisticated data analyses and are well-prepared for demanding research tasks in their academic and professional pursuits.",
    "crumbs": [
      "{{< fa pencil >}} Teaching"
    ]
  },
  {
    "objectID": "research.html#polarization-within-u.s.-foreign-media-with-arslan-khalid-kiwan-park",
    "href": "research.html#polarization-within-u.s.-foreign-media-with-arslan-khalid-kiwan-park",
    "title": "Research",
    "section": "Polarization within U.S. Foreign Media (With Arslan Khalid & Kiwan Park)",
    "text": "Polarization within U.S. Foreign Media (With Arslan Khalid & Kiwan Park)\n\n\n\n\n\n\nAbstract\n\n\n\n\n\nIn today’s highly polarized media landscape, understanding the potential biases and relationships between news publications has become increasingly important. This study aims to investigate the existence of media bias by examining the content of online news articles. We leverage a dataset containing over 200,000 articles from various media outlets, spanning diverse political orientations and subject matter. Using unsupervised machine learning techniques, we first employ clustering algorithms, such as k-means and hierarchical clustering, to group similar articles based on their content. This analysis enables us to identify patterns and common themes within the clusters, shedding light on the potential ideological leanings of the publications. Our DID analysis shows that, on average, there is a slight negative shift in sentiment scores before and after the 2016 US presidential election for articles published by liberal-leaning publications compared to those published by conservative-leaning publications. The regression analysis indicates that the ideological leaning of the publication was significantly associated with the overall sentiment score of articles, with liberal-leaning publications having higher sentiment scores on average compared to conservative-leaning publications."
  },
  {
    "objectID": "research.html#how-do-interest-groups-adapt-their-communication-strategy-to-big-shocks-an-analysis-of-the-medicare-for-all-debate-on-twitter-during-covid-19.-with-sushant-kumar-pengfei-zhang",
    "href": "research.html#how-do-interest-groups-adapt-their-communication-strategy-to-big-shocks-an-analysis-of-the-medicare-for-all-debate-on-twitter-during-covid-19.-with-sushant-kumar-pengfei-zhang",
    "title": "Research",
    "section": "How do Interest Groups adapt their Communication Strategy to Big Shocks? An analysis of the Medicare-For-All Debate on Twitter during COVID-19. (With Sushant Kumar & Pengfei Zhang)",
    "text": "How do Interest Groups adapt their Communication Strategy to Big Shocks? An analysis of the Medicare-For-All Debate on Twitter during COVID-19. (With Sushant Kumar & Pengfei Zhang)\n\n\n\n\n\n\nAbstract\n\n\n\n\n\nCOVID-19 has reinvigorated the policy debate for a universal healthcare system, attracting much attention on social media. In this paper, we study the online discourse of Medicare-For-All before and after COVID-19 by examining the Twitter feeds of two opposing health advocacy groups – Physicians for a National Health Program (PNHP) and Partnership for America’s Healthcare Future (P4AHCF). Our empirical results show a sharp contrast between the two interest groups’ communication strategies. PNHP’s tweets show more personalized stories, whereas P4AHCF’s tweets show more statistics and scientific reports. The difference in text styles is consequential. PNHP has higher engagement of Twitter users and is more adaptive to a pandemic narrative. By contrast, P4AHCF stopped tweeting entirely about Medicare-For-All after COVID-19 was declared a pandemic. We argue that the distinctive social media strategies can be explained by the groups’ different audiences and objectives. The findings add to our understanding of American’s activism on social media and the implication of the pandemic for health policy reform."
  },
  {
    "objectID": "research.html#introducing-the-conflibert-family-of-language-models-for-political-science-with-patrick-brandt-vito-dorazio-javier-osorio",
    "href": "research.html#introducing-the-conflibert-family-of-language-models-for-political-science-with-patrick-brandt-vito-dorazio-javier-osorio",
    "title": "Research",
    "section": "Introducing the ConfliBERT Family of Language Models for Political Science (with Patrick Brandt, Vito D’Orazio & Javier Osorio)",
    "text": "Introducing the ConfliBERT Family of Language Models for Political Science (with Patrick Brandt, Vito D’Orazio & Javier Osorio)\n\n\n\n\n\n\nAbstract\n\n\n\n\n\nFor decades, conflict scholars used rule-based approaches to extract information about political violence from newspapers around the world. Recent technological development in Natural Language Processing allowed us to overcome the rigidity of rule-based approaches. We review our recent ConfliBERT language model (Hu et al. 2022) and its applications in political science. ConfliBERT is a Large Language Model (LLM) specifically developed to process text related to politics and violence. It was trained on a large domain-specific corpus in English with text related to conflict, political violence, and international politics with global coverage. When fine-tuned, results show that ConfliBERT has superior performance over other LLMs like Gemma 2 (9B) and Llama 3.1 (7B) within its relevant domains. We then discuss multi-lingual extensions of ConfliBERT for Spanish and Arabic source texts and show that ConfliBERT also outperforms alternative models in their native languages. Finally, we discuss limitations of the models and propose further extensions."
  },
  {
    "objectID": "research.html#two-types-of-censorship-an-assessment-of-the-informational-autocracy-thesis-in-the-online-space",
    "href": "research.html#two-types-of-censorship-an-assessment-of-the-informational-autocracy-thesis-in-the-online-space",
    "title": "Research",
    "section": "Two Types of Censorship? An Assessment of the Informational Autocracy Thesis in the Online Space",
    "text": "Two Types of Censorship? An Assessment of the Informational Autocracy Thesis in the Online Space\n\n\n\n\n\n\nAbstract\n\n\n\n\n\nThis study tests how Guriev & Treisman’s Informational Autocracy (IA) theory applies to internet filtering practices in autocratic nations. It examines the different strategies of online censorship employed by autocratic regimes, analyzing how these practices align with or deviate from the predictions of the IA theory."
  },
  {
    "objectID": "research.html#populism-and-the-price-of-abandonment-analyzing-the-exit-dynamics-from-bilateral-investment-treaties-with-clint-peinhardt",
    "href": "research.html#populism-and-the-price-of-abandonment-analyzing-the-exit-dynamics-from-bilateral-investment-treaties-with-clint-peinhardt",
    "title": "Research",
    "section": "Populism and the Price of Abandonment: Analyzing the Exit Dynamics from Bilateral Investment Treaties (with Clint Peinhardt)",
    "text": "Populism and the Price of Abandonment: Analyzing the Exit Dynamics from Bilateral Investment Treaties (with Clint Peinhardt)\n\n\n\n\n\n\nOverview\n\n\n\n\n\nThis study explores how rising nationalist sentiment influences countries’ decisions to exit international investment agreements, combining economic data with text analysis of political rhetoric."
  },
  {
    "objectID": "teaching.html#epps-math-coding-camp",
    "href": "teaching.html#epps-math-coding-camp",
    "title": "Teaching",
    "section": "",
    "text": "In August 2024, I served as the lead instructor for the coding section of the EPPS Math & Coding Camp, a comprehensive 5-day intensive program designed for incoming PhD students. Beyond traditional classroom instruction, I spearheaded the development of the camp’s digital infrastructure, creating and maintaining an extensive online resource hub at https://eppsmathcodingcamp.github.io/.\nThe online platform features: - Interactive R tutorials covering foundational programming concepts - Comprehensive guides for data manipulation using dplyr and tidyverse - Step-by-step visualization tutorials using ggplot2 - Hands-on exercises in linear regression and statistical analysis - Version-controlled course materials using GitHub - Integrated RStudio projects for seamless learning experience\nMy responsibilities included: - Developing and delivering hands-on sessions in RStudio - Creating structured learning modules for R programming fundamentals - Designing practical exercises in data manipulation and visualization - Implementing an accessible online learning environment - Maintaining and updating course materials through GitHub - Providing individualized support through interactive coding sessions",
    "crumbs": [
      "{{< fa pencil >}} Teaching"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Shreyas Meher",
    "section": "",
    "text": "I am a Postdoctoral Researcher in the Policy, Politics, and Society group at the Erasmus School of Social and Behavioural Sciences, Erasmus University Rotterdam. My research leverages computational methods to address core questions in comparative politics, digital governance, and international relations.\nCurrently, as part of the Horizon Europe TWIN4DEM project, I develop machine learning frameworks to model and detect democratic backsliding. My substantive focus is on executive aggrandizement—the process by which elected leaders systematically weaken democratic institutions to consolidate power. My work involves creating the algorithmic core for ‘digital twins’ of political systems, which allows for the simulation and analysis of threats to democratic resilience.\nMy broader research agenda explores:\n\nCensorship & Digital Sovereignty: Analyzing how different regime types implement internet control, from subtle content moderation in democracies to overt filtering and blackouts in autocracies.\nComputational Methods & Political Conflict: Building specialized language models for structured political event classification. Previously, as a researcher on a major NSF-funded project, I developed models like ConfliBERT and ConflLlama and applied reinforcement learning to enhance event data annotation, and I remain an active contributor to this line of research.\nInternational Political Economy: The politics of investment treaties and the network effects within international organizations.\n\nMethodologically, I specialize in computational text analysis, reinforcement learning, and the application of LLMs to political science research in HPC environments."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Shreyas Meher",
    "section": "",
    "text": "Comparative Politics: Internet Control, Digital Governance, Autocratic Regimes\nInternational Political Economy: Investment Treaties, Economic Development\nComputational Social Science: Large Language Models, Political Text Analysis\nPolitical Conflict: Violence Analysis, Event Data Collection"
  },
  {
    "objectID": "index.html#current-research",
    "href": "index.html#current-research",
    "title": "Shreyas Meher",
    "section": "",
    "text": "Erasmus University Rotterdam (June 2024-Present) As a researcher on the TWIN4DEM (Strengthening Democratic Resilience Through Digital Twins) project, my work focuses on developing the machine learning frameworks that power the project’s ‘digital twins’ of European democracies. I am responsible for pre-training and fine-tuning large language models to identify signals of executive aggrandizement from political text and designing the simulation algorithms that form the core of the project’s models."
  },
  {
    "objectID": "index.html#dissertation",
    "href": "index.html#dissertation",
    "title": "Shreyas Meher",
    "section": "",
    "text": "“Digital Sovereignty: The Political Economy of Internet Governance”\nAn analysis of internet content filtering across regime types.\nAwarded by: The University of Texas at Dallas\nCommittee: Dr. Clint Peinhardt, Co-Chair\nDr. Pengfei Zhang, Co-Chair\nDr. Patrick T. Brandt\nDr. Karl Ho"
  },
  {
    "objectID": "index.html#recent-projects",
    "href": "index.html#recent-projects",
    "title": "Shreyas Meher",
    "section": "Recent Projects",
    "text": "Recent Projects\n\n\nConfliBERT Research Lab\nA computational framework for analyzing political conflict and violence using pre-trained language models.\nLearn More →\n\n\nCensorship in Democracies and Autocracies\nA comparative analysis of internet control mechanisms and their implications for electoral accountability.\nProject Details →"
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Shreyas Meher",
    "section": "Teaching",
    "text": "Teaching\n\nLead Instructor: EPPS Math & Coding Camp (R Programming), 2024\nTeaching Assistant: Introduction to Quantitative Methods, Applied Regression\nGuest Lecturer: Research Methods in Political Science"
  },
  {
    "objectID": "research/ConfliBERT.html",
    "href": "research/ConfliBERT.html",
    "title": "ConfliBERT Documentation and Usage",
    "section": "",
    "text": "Whether you are a seasoned data scientist or new to machine learning, this guide is structured to help you navigate through the varied functionalities of ConfliBERT with ease. For a hands-on introduction and demonstration, jump straight to our Demo usage. If you’re interested in applying ConfliBERT for Named Entity Recognition, see the section on NER (Named Entity Recognition) with ConfliBERT. To dive into text classification, visit the section on Classification with ConfliBERT. For masking and coding tasks, refer to the section on Masking and Coding Tasks with ConfliBERT. The section on Computational Considerations and Benchmarks for ConfliBERT covers vital aspects of performance optimization.To explore the various adaptations of ConfliBERT, see the section on [ConfliBERT Variants]. At any point, you can refer to the References / Citations for further reading and information sources that have informed the development and use of ConfliBERT."
  },
  {
    "objectID": "research/ConfliBERT.html#what-does-conflibert-do-and-why",
    "href": "research/ConfliBERT.html#what-does-conflibert-do-and-why",
    "title": "ConfliBERT Documentation and Usage",
    "section": "What does ConfliBERT do and why?:",
    "text": "What does ConfliBERT do and why?:\n\nDomain-Specific Pre-training: While many language models are trained on general corpora, ConfliBERT’s training is rooted in domain-specific corpora. This focus allows for enhanced performance in performing classification tasks related to political violence, conflict, cooperation, and diplomacy.\nUtility for Various Stakeholders: ConfliBERT is beneficial for academics, policymakers, and security analysts. It provides an efficient tool for monitoring and understanding the multifaceted dynamics of social unrest, political upheavals, and other conflict-related events on a global scale.\nAutomation and Efficiency: Traditional conflict analysis methods have their limitations in terms of scale and speed. ConfliBERT, with its automation capabilities, can parse vast datasets, significantly alleviating the challenges of manual annotations.\nBroad Application Potential: Beyond mere data categorization, ConfliBERT can be employed for a diverse range of tasks related to conflict research, including event classification, entity recognition, relationship extraction, and data augmentation."
  },
  {
    "objectID": "research/ConfliBERT.html#running-examples",
    "href": "research/ConfliBERT.html#running-examples",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Running Examples",
    "text": "Running Examples\nIn the following sections, we will walk through illustrative examples of ConfliBERT’s capabilities in action. These examples are designed to help researchers and analysts better understand how ConfliBERT automates the coding of textual data into structured conflict and event data. Grasping these capabilities is essential for a myriad of applications such as real-time monitoring of political events, in-depth conflict analysis, and the accumulation of data for international relations and peace studies.\nMake a Statement President Biden said that the U.S. alliance with Japan is stronger than ever.\nVerbal Cooperation Saudi Arabia expressed intent to restore diplomatic relations with Iran.\nMaterial Cooperation China provided humanitarian aid to Turkey.\nVerbal Conflict President Zelenskyy accused Russia of war crimes.\nMaterial Conflict Israeli forces attacked Hamas in Gaza City."
  },
  {
    "objectID": "research/ConfliBERT.html#key-features-and-components",
    "href": "research/ConfliBERT.html#key-features-and-components",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Key Features and Components:",
    "text": "Key Features and Components:\n\nPlatform and Requirements:\n\nConfliBERT’s code requires a Python installation on your computer.\nNecessary packages include torch, transformers, numpy, scikit-learn, pandas, and simpletransformers.\nCUDA 10.2 support is included for GPU acceleration.\n\nModel Versions:\n\nFour distinct versions of ConfliBERT are available:\n\nConfliBERT-scr-uncased: Pre-trained from scratch using an uncased vocabulary.\nConfliBERT-scr-cased: Pre-trained from scratch using a cased vocabulary.\nConfliBERT-cont-uncased: Continual pre-training using the original BERT’s uncased vocabulary.\nConfliBERT-cont-cased: Continual pre-training using BERT’s cased vocabulary.\n\n\nThese models are available on Huggingface and can be imported directly using its API.\nEvaluation and Usage:\n\nUsing ConfliBERT is analogous to other BERT models within the Huggingface ecosystem.\nExamples are provided for fine-tuning using the Simple Transformers library.\nA Google Colab demo is available for hands-on evaluation and experimentation.\n\nEvaluation Datasets:\n\nSeveral datasets related to news, global events, and political conflicts are mentioned for evaluation. These datasets include:\n\n20Newsgroups, BBCnews, EventStatusCorpus, GlobalContention, GlobalTerrorismDatabase, Gun Violence Database, IndiaPoliceEvents, InsightCrime, MUC-4, re3d, SATP, and CAMEO.\n\nCustom datasets can be integrated after preprocessing into the required formats.\n\nPre-Training Corpus:\n\nConfliBERT was pre-trained on an extensive corpus from the politics and conflict domain (33 GB in size).\nDue to copyright constraints, only sample scripts and a few samples from the pre-training corpus are provided. The details of the pre-training corpus are documented in Section 2 and the Appendix of the paper.\n\nPre-Training Process:\n\nConfliBERT utilizes the same pre-training scripts (run_mlm.py) from Huggingface. An example is provided for pre-training on 8 GPUs, though parameters should be adapted based on the user’s available resources.\n\nCitation:\n\nIf used for research purposes, it is recommended to cite the original paper. The citation details are provided in the repository."
  },
  {
    "objectID": "research/ConfliBERT.html#main-tasks",
    "href": "research/ConfliBERT.html#main-tasks",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Main Tasks",
    "text": "Main Tasks\n\nNER\nClassification of events\nMasking and coding tasks"
  },
  {
    "objectID": "research/ConfliBERT.html#inputs-for-ner",
    "href": "research/ConfliBERT.html#inputs-for-ner",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Inputs for NER",
    "text": "Inputs for NER\nBasic NER terminology:\n\nB (Beginning) denotes the beginning of an entity.\nI (Inside) marks the subsequent words of a multi-word entity.\nO (Outside) is used for non-entity words.\n\n\nInputs for NER using the ConfliBERT model:\n\n1. Text Pre-processing:\nBefore feeding the data to ConfliBERT (or any BERT-like model), one needs to ensure the text data is pre-processed:\n\nTokenization: Convert sentences into tokens. Depending on the language and context, tokenization might split words into subwords or characters.\nLowercasing: This step is optional and based on the pre-trained ConfliBERT model. Some BERT models are case-sensitive.\nSpecial Tokens: BERT models usually expect [CLS] and [SEP] tokens to mark the beginning and end of a sentence, respectively.\nFor example: [CLS] U.S. military chief General Colin Powell said ... [SEP]\nPadding and Truncating: Make sure each input sequence has the same length by either padding short sequences or truncating long ones.\n\n\n\n2. Setting Options for Classification:\n\nEntity Tags: Define a fixed set of entity tags, such as B-S, I-S, B-T, I-T, and O. This is crucial for the classification layer’s output size.\nLoss Function: Since NER is a multi-class classification problem, use a categorical cross-entropy loss.\nModel Architecture: Depending on the version and customization of ConfliBERT, ensure the last layer is a dense layer with an output size equal to the number of unique entity tags.\n\n\n\n3. Input Files:\nThe files provided seem to be in a CoNLL-like format, which is standard for NER tasks. A breakdown of the structure:\n\nEach word is on a new line with its respective IOB-tag.\nSentences or sequences are separated by blank lines.\n\nExample:\nU.S. B-S\nmilitary I-S\n...\n. O\n\nNATO B-S\n...\n. O\nNote: The input file’s structure needs to be consistent for effective model training and evaluation.\n\n\n4. Additional Metadata (Optional):\nDepending on the specificities of the ConfliBERT model or the task requirements:\n\nAttention Masks: Used to tell the model to pay attention to specific tokens and ignore others (like padding tokens).\nSegment IDs: If handling multiple sentences in a sequence, segment IDs can distinguish them.\n\n\n\n5. Training and Evaluation Data:\n\nTraining Data: Pre-processed sentences along with their entity tags for training the model.\nEvaluation Data: A separate set of pre-processed sentences and their tags to validate the model’s performance.\n\n\n\n\nConclusion:\nUsers should prepare their data in a CoNLL-like format, ensure text pre-processing aligns with ConfliBERT’s requirements, and define a set of entity tags for classification. The more consistent and clean the input data, the better the model’s performance.\nBased on the given example outputs, the sections for “Outputs from NER,” “Metrics for NER,” and “Evaluation of NER” are provided next."
  },
  {
    "objectID": "research/ConfliBERT.html#outputs-from-ner",
    "href": "research/ConfliBERT.html#outputs-from-ner",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Outputs from NER",
    "text": "Outputs from NER\nSentence: “For the first time in decades, there is at least the potential of an armed clash with America’s largest adversaries, Russia and China.”\nPredictions: The named entity recognition model has identified three entities in the given sentence: - decades: Temporal Entity - America’s: Government - Russia: Government - China: Government\nRaw Outputs: There also is likely the raw logits or scores associated with each token for different potential named entity classes given with the outputs. For brevity, they are not all listed here, but they can be useful for diving deeper into model decisions or for model calibration."
  },
  {
    "objectID": "research/ConfliBERT.html#metrics-for-ner",
    "href": "research/ConfliBERT.html#metrics-for-ner",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Metrics for NER",
    "text": "Metrics for NER\nHere is a general format you might encounter:\n\nPrecision: This is a measure of the accuracy provided that a specific class (or label) has been predicted. It is the number of true positives divided by the sum of true positives and false positives.\nRecall: This is a measure of the ability of the model to find all the relevant cases within a dataset. It is the number of true positives divided by the sum of true positives and false negatives.\nF1-Score: The F1 score is the harmonic mean of precision and recall. It provides a balance between the two. When it is closer to 1, it indicates better performance, and 0 indicates poorer performance.\nAccuracy: This metric calculates the ratio of correctly predicted observation to the total observations.\n\n(Note: For a complete evaluation, one would typically calculate these metrics for each entity type, as well as overall.)"
  },
  {
    "objectID": "research/ConfliBERT.html#evaluation-of-ner",
    "href": "research/ConfliBERT.html#evaluation-of-ner",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Evaluation of NER",
    "text": "Evaluation of NER\nThe model seems to correctly identify the governmental entities America’s, Russia, and China. Additionally, the model successfully picked up on the temporal entity decades.\nHowever, the evaluation of the model’s performance would ideally require a much larger test dataset, encompassing a diverse range of sentences and contexts. Furthermore, an in-depth evaluation would involve comparing the model’s predictions against a ground truth or a labeled dataset to compute metrics like precision, recall, and F1-score.\nOne key aspect to consider in such models is their confidence scores (or probabilities) associated with predictions. From the raw outputs, we can extract these values to potentially set thresholds or make informed decisions.\n\nRemember, while metrics provide a quantitative measure of performance, qualitative analysis (like manually examining where the model goes right or wrong) is invaluable, especially when deploying in critical applications."
  },
  {
    "objectID": "research/ConfliBERT.html#satp_12.20-pipeline",
    "href": "research/ConfliBERT.html#satp_12.20-pipeline",
    "title": "ConfliBERT Documentation and Usage",
    "section": "SATP_12.20 Pipeline",
    "text": "SATP_12.20 Pipeline\nHere is a detailed breakdown of the preprocessing and training steps for Named Entity Recognition (NER):\n\n1. Annotation Pipeline\n\nFile: 1_annotation_pipeline-12.21.ipynb\nDescription: This notebook likely contains the process for manually annotating or labelling the data. The annotations refer to the entities within the text that the model will later be trained to recognize.\n\n\n\n2. Preparing the Training and Testing Data\n\nFile: 2_Preprocess-Data-12.27.ipynb\nDescription: After annotation, the data needs to be prepared for model training. This might include splitting the data into training, validation, and test sets; tokenizing the data; and converting it into a format suitable for deep learning frameworks.\n\n\n\n3. BiLSTM Baseline Deep Learning Model\n\nFile: 3_BiLSTM.ipynb\nDescription: This notebook contains the implementation and training process of a BiLSTM (Bidirectional Long Short-Term Memory) model. BiLSTMs are a type of recurrent neural network that can capture context from both past and future input sequences.\n\n\n\n4. BERT Model\n\nFile: 4_transformer_reduced_bert.ipynb\nDescription: Here, the BERT (Bidirectional Encoder Representations from Transformers) model is implemented and trained. BERT is a popular transformer-based model known for its effectiveness in various NLP tasks, including NER.\n\n\n\n5. Traditional Machine Learning Model Baselines\n\nFile: 5_Baselines_reduced-2-step.ipynb\nDescription: This notebook provides baseline results using traditional machine learning models like Support Vector Machines (SVM) and Logistic Regression (LR). A two-step pipeline might refer to a two-phase process: feature extraction and then modeling.\n\n\n\n6. Hierarchical Attention Networks (HAN)\n\nSteps:\n\nCreating Word Vectors and DataLoader from train.csv:\n\nCommand: python create_input_files.py\nDescription: This script prepares the data and creates word embeddings, possibly using pre-trained models. The embeddings will be used to represent words or tokens in the data. It also sets up data loaders which are essential for training deep learning models in batches.\n\nTraining and Evaluating the HAN Model:\n\nCommand: python HAN_end2end.py\nDescription: Hierarchical Attention Networks are neural models that can pay differentiated attention to various parts of the input data, making them suitable for tasks where the importance of different parts of the input varies.\n\n\n\n\n\nResults:\nThe results section showcases the performance of various models on the task:\n\nMetrics used include:\n\nAccuracy: The ratio of correctly predicted entities to the total entities.\nPrecision: The ratio of correctly predicted positive entities to the total predicted positives.\nRecall: The ratio of correctly predicted positive entities to the total actual positives.\nF1 Score: The harmonic mean of precision and recall.\nExact Matching Ratio: This might refer to the ratio of data points where the predicted entities exactly match the true entities.\n\nThe models and their respective performance metrics are tabulated for comparison.\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy (%)\nPrecision (%)\nRecall (%)\nF1 (%)\nExact Matching Ratio (%)\n\n\n\n\nOne vs Rest\n26.07\n87.19\n27.40\n28.04\n82.20\n\n\nBinary Relevance\n26.65\n86.08\n28.29\n28.40\n82.47\n\n\nClass Chain\n27.93\n83.47\n29.78\n29.32\n82.87\n\n\nLabel Powerset\n28.86\n85.53\n30.51\n30.05\n83.14\n\n\nBiLSTM\n37.11\n53.81\n58.51\n42.12\n76.23\n\n\nHAN\n52.98\n65.64\n74.82\n55.78\n83.07\n\n\nBERT\n62.52\n74.73\n80.01\n64.59\n87.23\n\n\n\nThis comprehensive pipeline takes the data from raw, annotated form and processes it into a format that various machine learning and deep learning models can be trained on. The models’ performances are then evaluated and compared using the mentioned metrics."
  },
  {
    "objectID": "research/ConfliBERT.html#other-llm-comparisons-on-this-task",
    "href": "research/ConfliBERT.html#other-llm-comparisons-on-this-task",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Other LLM comparisons on this task",
    "text": "Other LLM comparisons on this task"
  },
  {
    "objectID": "research/ConfliBERT.html#related-literature-in-ner",
    "href": "research/ConfliBERT.html#related-literature-in-ner",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Related Literature in NER",
    "text": "Related Literature in NER"
  },
  {
    "objectID": "research/ConfliBERT.html#understanding-classification-metrics-in-conflibert",
    "href": "research/ConfliBERT.html#understanding-classification-metrics-in-conflibert",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Understanding Classification Metrics in ConfliBERT",
    "text": "Understanding Classification Metrics in ConfliBERT\nThis document provides an overview of classification metrics in the context of political event categorization using ConfliBERT, drawing upon foundational concepts in machine learning classification such as True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN) as explained in the Google Machine Learning Crash Course (see “Machine Learning Crash Course: Classification: True/False Positive/Negative” (2021)). We will use a hypothetical example to illustrate these concepts clearly:\nMetrics Explained:\n\nTrue Positives (TP): Sentences where the Predicted Category matches the actual category (highest logit score).\n\n\nFalse Positives (FP): Cases where the Predicted Category was chosen, but it is not the actual category.\n\n\nFalse Negatives (FN): Cases where the actual category was correct, but the Predicted Category was different.\n\n\nTrue Negatives (TN): All other cases where neither the predicted nor the actual category was chosen.\n\nHypothetical Example for Classification:\nConsider an event: Israeli forces attacked Hamas in Gaza City. Let’s place this event in a 2x2 confusion matrix to understand potential outcomes from ConfliBERT’s classification:\n\n\n\n\n\n\n\nTrue Positive (TP)\nFalse Negative (FN)\n\n\n\n\nConfliBERT accurately classifies the event as “Material Conflict” due to the active engagement and physical altercation reported.\nConfliBERT incorrectly classifies the event as a non-conflict category, such as “Verbal Cooperation,” missing the material nature of the conflict.\n\n\n\n\n\n\n\n\n\n\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\n\nConfliBERT incorrectly classifies a peaceful negotiation as “Material Conflict,” suggesting a physical altercation where there was none.\nConfliBERT correctly identifies that no material conflict occurred when Israeli forces did not engage with Hamas.\n\n\n\nDetailed Breakdown:\n\nTrue Positive (TP): - Reality: Israeli forces engage in a physical altercation with Hamas in Gaza City. - ConfliBERT Says: “Material Conflict.” - Outcome: ConfliBERT’s classification aligns with the real-world event, providing accurate data for analysis.\n\n\nFalse Positive (FP): - Reality: A diplomatic meeting between Israeli and Hamas representatives occurs without physical confrontation. - ConfliBERT Says: “Material Conflict.” - Outcome: ConfliBERT erroneously signals a conflict, potentially leading to misinformed analysis.\n\n\nFalse Negative (FN): - Reality: Israeli forces attack Hamas in Gaza City. - ConfliBERT Says: “Verbal Cooperation” or another non-conflict category. - Outcome: Misclassification downplays the severity, potentially affecting response strategies.\n\n\nTrue Negative (TN): - Reality: Israeli and Hamas representatives engage in peaceful talks. - ConfliBERT Says: “Verbal Cooperation” or an appropriate non-conflict category. - Outcome: ConfliBERT accurately reflects the peaceful nature of the event, aiding correct analysis and policy decisions.\n\nApplying Metrics for Evaluation:\nThe subsequent sections will explore how to use these metrics—sensitivity, specificity, precision, and accuracy—to evaluate ConfliBERT’s performance in classifying complex political events. These metrics are essential for assessing ConfliBERT’s accuracy and fine-tuning its predictions to align with real-world observations.\nHere is a deep dive into how ConfliBERT enhances classification in the context of conflict research:\n\nGranular Event Categorization: While it is essential to identify an incident as a protest or a riot, the true value lies in discerning the nature of these events. ConfliBERT’s classification capability can distinguish between a peaceful protest, a violent uprising, or even a state-sanctioned crackdown. This granularity ensures a comprehensive understanding of the event’s nature.\nAdaptive Learning with Domain-Specific Data: Given ConfliBERT’s training on conflict-related texts, it possesses a heightened sensitivity to the subtle nuances within conflict narratives. This domain-specific expertise translates to a more accurate and contextual classification, be it categorizing a skirmish as sectarian, ethnic, or political.\nTemporal Classification: Conflicts evolve, and so do their narratives. ConfliBERT can classify events based on their temporal context, distinguishing between a long-standing civil war’s initial skirmishes and its climax battles or identifying the phases of diplomatic negotiations.\nAutomating Large-Scale Data Processing: Conflict research often grapples with voluminous data from diverse sources like news articles, social media, and firsthand accounts. Manual classification of such vast data is not only labor-intensive but also prone to inconsistencies. ConfliBERT automates this, ensuring uniformity and efficiency.\nReal-time Classification for Dynamic Responses: In the rapidly changing landscape of political conflicts, timely responses are crucial. With its swift classification capabilities, ConfliBERT can process real-time data, ensuring that researchers and policymakers are equipped with classified insights as events transpire.\nSupporting Cross-Referencing and Validation: By classifying data, ConfliBERT also aids in cross-referencing. For instance, if two sources provide conflicting narratives of an event, having them classified can help researchers quickly juxtapose and validate the information.\nAssisting Predictive Analysis: Once events are classified systematically, it becomes feasible to perform predictive analysis. Recognizing patterns from past events can provide insights into potential future scenarios, aiding preemptive measures and strategies.\n\nIn summary, ConfliBERT’s classification capabilities not only structure the unorganized maze of conflict data but also elevate the depth and breadth of analysis. By providing categorized, context-aware insights, it becomes an invaluable asset for researchers, analysts, and policymakers navigating the intricate domain of conflict research.\nFor a different example, let’s create a hypothetical news report about a political demonstration that turned violent. This example will have more variance in the types of police actions and outcomes, and it will include different categorizations.\nOriginal Text (Pre-Processing): “During a large political demonstration in the capital, clashes erupted between protesters and police. Police reportedly used water cannons and rubber bullets to control the situation. Several protesters were detained, and there were reports of injuries among both police and demonstrators. A local shop was vandalized during the chaos. The protest was eventually dispersed, and order was restored by the authorities.”\nProcessed Text (Post-Processing): The text is segmented into individual sentences and processed for analysis, with each sentence receiving specific categorizations based on the content.\n\n\n\n\n\n\n\n\n\n\n\nProcessed Sentence\nUSE_OF_FORCE\nDETENTION\nINJURIES\nVANDALISM\nORDER_RESTORED\n\n\n\n\nDuring a large political demonstration in the capital, clashes erupted between protesters and police\n0\n0\n0\n0\n0\n\n\nPolice reportedly used water cannons and rubber bullets to control the situation\n1\n0\n0\n0\n0\n\n\nSeveral protesters were detained\n0\n1\n0\n0\n0\n\n\nThere were reports of injuries among both police and demonstrators\n0\n0\n1\n0\n0\n\n\nA local shop was vandalized during the chaos\n0\n0\n0\n1\n0\n\n\nThe protest was eventually dispersed, and order was restored by the authorities\n0\n0\n0\n0\n1\n\n\n\nIn this table: - “USE_OF_FORCE” indicates if the sentence suggests the use of force by police. - “DETENTION” denotes if the sentence implies that protesters were detained. - “INJURIES” signifies if there were injuries reported among the police or demonstrators. - “VANDALISM” marks if there was property damage or vandalism mentioned. - “ORDER_RESTORED” is affirmative if the sentence indicates that order was restored by the authorities.\nThis structured approach to analyzing news text provides a clear understanding of the various aspects of the event, which is essential for conflict analysis and research."
  },
  {
    "objectID": "research/ConfliBERT.html#inputs-for-classification",
    "href": "research/ConfliBERT.html#inputs-for-classification",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Inputs for Classification",
    "text": "Inputs for Classification\nTo harness the capabilities of ConfliBERT for classification, one needs to be well-acquainted with the expected input format, necessary pre-processing steps, and customization options:\nText Pre-processing:\n\nTokenization:\n\nTokenizing is the process of breaking down the text into smaller chunks, often words or sub-words.\nExample: The sentence “UNRWA said it had suspended aid” will be tokenized as [“UNRWA”, “said”, “it”, “had”, “suspended”, “aid”].\n\nFormatting:\n\nBased on the dataset structure provided, the text should be divided into specific segments namely ‘sentence’, ‘source’, and ‘target’.\nExample: For the text “UNRWA said it had suspended aid deliveries to Gaza”, “UNRWA” will be the source, “Gaza” will be the target, and the whole string is the sentence.\n\nSpecial Character Removal:\n\nIt is crucial to ensure that any non-textual or special characters that do not contribute to the semantic meaning of the sentence are removed.\nExample: If the sentence is “UNRWA said it had suspended aid!!!”, the exclamatory marks can be removed for a cleaner input as “UNRWA said it had suspended aid”.\n\nLowercasing/Uppercasing:\n\nDepending on the model variant being used (cased or uncased), ensure that the text is transformed accordingly.\nExample: For the uncased variant, “UNRWA said it had suspended aid” becomes “unrwa said it had suspended aid”.\n\nSentence Segmentation:\n\nIf the input is a long paragraph or document, break it down into individual sentences.\nExample: For a paragraph “UNRWA said it had suspended aid. It was a major decision.”, the segmented sentences will be [“UNRWA said it had suspended aid.”, “It was a major decision.”].\n\n\nCustomizing Classification with ConfliBERT:\n\nFine-tuning:\n\nAlthough ConfliBERT is pre-trained, its versatility shines when fine-tuned on a specific dataset. This way, the model becomes more attuned to the nuances of the data.\nExample: If the focus is on classifying statements made in a legal context, fine-tune ConfliBERT on legal documents or court verdicts.\n\nUsing Different Model Variants:\n\nChoose a ConfliBERT version that aligns with the nature of the text data. If the distinction between uppercase and lowercase letters is significant, opt for a cased model.\n\nAdjusting Model Parameters:\n\nDuring fine-tuning or prediction, tweak model parameters like learning rate, batch size, or epoch count to optimize performance.\n\nFeedback Loop:\n\nConsider setting up a feedback loop where misclassifications can be corrected and fed back into the model for continuous learning.\n\n\n\nDefine and discuss update methods\nThere are four versions of ConfliBERT, each serving a specific purpose:\n\n\n\n\n\n\n\nVersion\nDefinition\n\n\n\n\nConfliBERT-scr-uncased:\nThis version is pre-trained from scratch using a custom uncased vocabulary. This is the preferred version as it has been built from the ground up with a specific vocabulary set.\n\n\nConfliBERT-scr-cased:\nThis version is also pre-trained from scratch but uses a custom cased vocabulary. This means it differentiates between uppercase and lowercase letters.\n\n\nConfliBERT-cont-uncased:\nThis version is built by continually pre-training on top of the original BERT’s uncased vocabulary.\n\n\nConfliBERT-cont-cased:\nThis version is similar to the above but uses the original BERT’s cased vocabulary.\n\n\n\nScratch versus Continuous Defined: 1. Scratch: This refers to building the model from scratch, meaning the model is trained without any prior knowledge. The vocabulary is also built from the base without any prior context. 2. Continuous: Continuous pre-training means the model is built on top of an existing model (in this case, BERT). This leverages the knowledge already present in BERT and fine-tunes it for a specific task."
  },
  {
    "objectID": "research/ConfliBERT.html#outputs-from-classification",
    "href": "research/ConfliBERT.html#outputs-from-classification",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Outputs from Classification",
    "text": "Outputs from Classification\nWhen ConfliBERT provides a classification output, it organizes the results hierarchically and presents detailed information for clarity and to aid interpretation. Here is a deep dive into what each segment of the output signifies:\n\nIndex and Basic Information:\n\nIndex: Each entry or instance in the dataset being classified is assigned a unique identifier or index.\nGold Penta: This is a reference label, indicating the correct classification as provided in the dataset.\nSentence: The original textual input that was classified.\nSource: The entity or group mentioned as the originator or subject of the action in the sentence.\nTarget: The entity or group mentioned as the recipient or object of the action in the sentence.\n\nLevel 1 Classification:\n\nTense: The temporal context in which the event occurred, like ‘past’ or ‘future’.\nPrompt Text: A standardized, simplified representation of the sentence. For instance, if the sentence is “UNRWA said it had suspended aid deliveries to Gaza”, the prompt text might be “‘Source’ reduced aid to ‘Target’”, where ‘Source’ represents the source (UNRWA) and ‘Target’ represents the target (Gaza).\nRoot Code: This signifies the primary, overarching classification category. For example, “SANCTION”.\nScore: This provides a confidence measure. The higher the score, the more confident the model is about its classification.\n\nLevel 1 Evaluation:\n\nGold Root vs. Prediction: Here, the model’s prediction for the root code is compared to the correct answer or “gold” standard. For instance, if the gold standard is “SANCTION” and the model also predicts “SANCTION”, then this comparison would indicate a match.\nGold Penta vs. Prediction: Similarly, the model’s prediction for the gold penta is compared to the correct answer. If both match, the model’s prediction is considered correct for that instance.\n\nLevel 2 Classification:\n\nRoot Code: The primary classification category is reiterated here for clarity.\nPrompt Text: Similar to level 1, this provides a standardized representation of the sentence, but might be slightly more detailed or provide alternate interpretations.\nFuture and Past Scores: For each classification prompt, there might be separate scores indicating the model’s confidence for both future and past contexts. This helps understand how the model perceives the temporal aspect of the event.\nL2 Root: This provides a more detailed or nuanced classification based on the primary root code from level 1.\n\nLevel 2 Evaluation:\n\nThis section is similar to the level 1 evaluation but pertains to the more detailed classifications from level 2.\nGold Root vs. Prediction: The model’s level 2 root code prediction is compared to the correct or “gold” root code. If both are the same, the prediction is accurate.\nGold Penta vs. Prediction: The predicted gold penta value at level 2 is compared against the correct value. If they match, the model’s prediction is deemed correct for that instance.\n\n\nIn essence, the output from ConfliBERT provides both a high-level and a detailed classification of the input sentence, complete with confidence scores, standardized representations, and a direct comparison to known correct answers for evaluation purposes. This hierarchical structure ensures a comprehensive understanding of the sentence’s context, entities involved, and the nature of their interaction."
  },
  {
    "objectID": "research/ConfliBERT.html#extant-evaluation-datasets",
    "href": "research/ConfliBERT.html#extant-evaluation-datasets",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Extant Evaluation datasets:",
    "text": "Extant Evaluation datasets:\nThe datasets"
  },
  {
    "objectID": "research/ConfliBERT.html#huggingface-usage-here",
    "href": "research/ConfliBERT.html#huggingface-usage-here",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Huggingface usage here",
    "text": "Huggingface usage here\n(To be filled with information on how to use ConfliBERT with the Huggingface library, if applicable.)"
  },
  {
    "objectID": "research/ConfliBERT.html#metrics-for-classification",
    "href": "research/ConfliBERT.html#metrics-for-classification",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Metrics for Classification",
    "text": "Metrics for Classification\n(To be filled with relevant metrics used to evaluate the performance of ConfliBERT for classification, e.g., accuracy, F1 score, etc.)"
  },
  {
    "objectID": "research/ConfliBERT.html#evaluation-of-classification",
    "href": "research/ConfliBERT.html#evaluation-of-classification",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Evaluation of Classification",
    "text": "Evaluation of Classification\n(To be filled with details on how ConfliBERT’s classification performance was evaluated, including methodologies, datasets used, and results.)"
  },
  {
    "objectID": "research/ConfliBERT.html#other",
    "href": "research/ConfliBERT.html#other",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Other",
    "text": "Other"
  },
  {
    "objectID": "research/ConfliBERT.html#inputs-for-masking-and-coding",
    "href": "research/ConfliBERT.html#inputs-for-masking-and-coding",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Inputs for Masking and Coding",
    "text": "Inputs for Masking and Coding\nTo utilize ConfliBERT for masking and coding tasks, the input data must be structured and pre-processed according to specific guidelines.\n\nTokenization: The text should be tokenized into sub-words or words. The tokenization method will depend on the pre-trained model and tokenizer available for ConfliBERT.\nFormatting: For the [MASK] task, specific tokens or words in the sentence should be replaced with the [MASK] token. The model will then predict the missing token based on the context provided by the rest of the sentence.\nSegmentation: If the input consists of multiple sentences, they should be divided into distinct segments. Each segment will be input separately into the model.\nPadding: All inputs should be padded to have the same length for batching during model training or inference."
  },
  {
    "objectID": "research/ConfliBERT.html#outputs-from-masking-and-coding",
    "href": "research/ConfliBERT.html#outputs-from-masking-and-coding",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Outputs from Masking and Coding",
    "text": "Outputs from Masking and Coding\n\nPredicted Tokens: The primary output from the masking task is the token predicted by ConfliBERT to replace the [MASK] token. The model will provide a probability distribution over the vocabulary, and the token with the highest probability is usually taken as the prediction.\nEmbeddings: ConfliBERT will also produce embeddings for each token in the input sequence. These embeddings can be used for downstream tasks or further analysis.\nCoding Outputs: Depending on the coding task, ConfliBERT might classify a given input into predefined categories or produce other forms of structured output."
  },
  {
    "objectID": "research/ConfliBERT.html#metrics-for-masking-and-coding",
    "href": "research/ConfliBERT.html#metrics-for-masking-and-coding",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Metrics for Masking and Coding",
    "text": "Metrics for Masking and Coding\n\nAccuracy: Measures the percentage of [MASK] tokens that the model predicted correctly.\nPerplexity: Provides insight into how well the probability distribution predicted by the model aligns with the actual distribution of the [MASK] token.\nF1-Score: Used mainly for coding tasks, it considers both precision and recall to provide a more holistic view of model performance.\nLoss: The model’s objective function value, which it tries to minimize during training."
  },
  {
    "objectID": "research/ConfliBERT.html#evaluation-of-masking-and-coding",
    "href": "research/ConfliBERT.html#evaluation-of-masking-and-coding",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Evaluation of Masking and Coding",
    "text": "Evaluation of Masking and Coding\n\nIn-domain vs. Out-of-domain Evaluation: It is essential to evaluate the model’s performance both on data similar to its training data and on entirely different datasets.\nHuman Evaluation: Sometimes, multiple tokens can correctly fill a [MASK] token, so human evaluators can provide more nuanced feedback on the model’s predictions.\nAblation Studies: Evaluating the model’s performance by removing or modifying certain parts can help understand the importance of various components."
  },
  {
    "objectID": "research/ConfliBERT.html#other-llm-comparisons-on-this-task-1",
    "href": "research/ConfliBERT.html#other-llm-comparisons-on-this-task-1",
    "title": "ConfliBERT Documentation and Usage",
    "section": "Other LLM comparisons on this task",
    "text": "Other LLM comparisons on this task\n\nBERT: BERT is the pioneering model that introduced the [MASK] task. It is insightful to compare newer models like ConfliBERT with BERT to see the advancements.\nRoBERTa: A variant of BERT, RoBERTa changes some key hyperparameters and training strategies, which can have different performances on the masking task."
  },
  {
    "objectID": "research/ConfliBERT.html#cpu-usage-1-core",
    "href": "research/ConfliBERT.html#cpu-usage-1-core",
    "title": "ConfliBERT Documentation and Usage",
    "section": "CPU Usage (1 Core)",
    "text": "CPU Usage (1 Core)\nBERT, and by extension models like ConfliBERT, are heavily parallelized models, which means they benefit significantly from multiple cores or GPUs. Using just a single core would be significantly slower and is not recommended for any sizable inference or training task. However, for very light tasks or experimentation, a single core could suffice, albeit with extended processing times."
  },
  {
    "objectID": "research/ConfliBERT.html#cpu-multicore",
    "href": "research/ConfliBERT.html#cpu-multicore",
    "title": "ConfliBERT Documentation and Usage",
    "section": "CPU (Multicore)",
    "text": "CPU (Multicore)\nBased on BERT benchmarks [https://vincentteyssier.medium.com/bert-inference-cost-performance-analysis-cpu-vs-gpu-b58a2420b2c8]:\n\n8 vCPUs (e2-highcpu-8):\n\nProcessing Time per File: 69.4s - 70.5s for 1500 samples.\nOverall vCPU Usage: Approximately 75%.\nCost: $0.197872/hour or $144.45 monthly.\n\n\n\n16 vCPUs (e2-highcpu-16):\n\nProcessing Time per File: 40.80s - 43.34s for 1500 samples.\nOverall vCPU Usage: Approximately 60%.\nCost: $0.395744/hour or $288.89 monthly.\n\n\n\n32 vCPUs (e2-highcpu-32):\n\nProcessing Time per File: 35.73s - 40.21s for 1500 samples.\nOverall vCPU Usage: Approximately 40%.\nCost: $0.791488/hour.\n\nThe overarching pattern reveals that as the number of CPU cores increases, the processing time decreases. However, there is a point of diminishing returns: scaling from 16 to 32 vCPUs does not halve the processing time. Additionally, the vCPU usage decreases, indicating underutilization of resources with higher cores."
  },
  {
    "objectID": "research/ConfliBERT.html#with-gpus-and-other-architectures-non-intelamd",
    "href": "research/ConfliBERT.html#with-gpus-and-other-architectures-non-intelamd",
    "title": "ConfliBERT Documentation and Usage",
    "section": "With GPUs and other Architectures (non-Intel/AMD)",
    "text": "With GPUs and other Architectures (non-Intel/AMD)\nGPUs, especially those designed for deep learning tasks, like the NVIDIA Tesla series, provide significant speed-ups for BERT-like models due to their parallel processing capabilities.\n\n1 GPU (n1-standard-8 + NVIDIA Tesla V100):\n\nProcessing Time per File: 2.16s - 3.28s for 1500 samples. This speed is around 25 times faster than the 8 vCPU instance and approximately 15 times faster than the 32 vCPU instance.\nCost: $2.004/hour or $1,462.99 monthly.\n\nFor specialized architectures beyond Intel/AMD, the benchmarks would vary based on the specific architecture. For example, FPGAs or TPUs might offer different performance and cost metrics. However, TPUs, in particular, have been known to provide excellent performance for TensorFlow-based models, including BERT derivatives.\nIn conclusion, while BERT benchmarks give an idea of the computational requirements, users should perform their own benchmarks to ascertain the exact requirements for ConfliBERT. Always consider the scale of the task, available budget, and desired processing time when choosing the computational infrastructure."
  },
  {
    "objectID": "research.html#working-papers",
    "href": "research.html#working-papers",
    "title": "Research",
    "section": "Working Papers",
    "text": "Working Papers\n\nPopulism and Investment Treaties\nwith Clint Peinhardt\nInvestigating how nationalist rhetoric influences countries’ decisions to exit international investment agreements.\n\n\nIGO Withdrawal Networks\nExtending Borzyskowski & Vabulas (2019) to examine network effects in international organization withdrawals.\n\n\nMedia Polarization in International News Coverage\nwith Arslan Khalid & Kiwan Park\nUsing machine learning to analyze ideological patterns in international news coverage across 200,000+ articles, examining shifts in sentiment and framing before and after the 2016 US election.\n\n\nEvent Horizon: Revolutionizing Data Annotation with Reinforcement Learning\nIntroducing a novel reinforcement learning framework built on deepseek’s GRPO, merging political science with advanced AI to create structured, transparent annotations for complex events.\nView Model on Hugging Face"
  },
  {
    "objectID": "research.html#ongoing-projects",
    "href": "research.html#ongoing-projects",
    "title": "Research",
    "section": "Ongoing Projects",
    "text": "Ongoing Projects\n\nPopulism and Investment Treaties\nwith Clint Peinhardt\nInvestigating how nationalist rhetoric influences countries’ decisions to exit international investment agreements.\n\n\nIGO Withdrawal Networks\nExtending Borzyskowski & Vabulas (2019) to examine network effects in international organization withdrawals."
  },
  {
    "objectID": "research.html#dissertation",
    "href": "research.html#dissertation",
    "title": "Research",
    "section": "Dissertation",
    "text": "Dissertation\n“Digital Sovereignty: The Political Economy of Internet Governance”\nSlides"
  },
  {
    "objectID": "research.html#resources",
    "href": "research.html#resources",
    "title": "Research",
    "section": "Resources",
    "text": "Resources\n\nConfliBERT Documentation\nA comprehensive guide for political scientists on using and fine-tuning ConfliBERT for various NLP tasks. View Manual"
  },
  {
    "objectID": "teaching.html#teaching-experience",
    "href": "teaching.html#teaching-experience",
    "title": "Teaching",
    "section": "",
    "text": "EPPS Math & Coding Camp (August 2024)\nR Programming and Data Analysis\n\nCreated and maintained course website eppsmathcodingcamp.github.io\nTaught R programming fundamentals, data manipulation (dplyr), and visualization (ggplot2)\nDeveloped interactive tutorials and exercises for 5-day intensive program\nBuilt GitHub-based infrastructure for version control and content delivery\n\n\n\n\nEPPS 6313 - Introduction to Quantitative Methods\nTeaching Assistant for Dr. Pengfei Zhang\n\nLed weekly STATA lab sessions focusing on regression analysis\nGuided students through practical applications of statistical methods\nProvided individualized support for data analysis projects\n\nEPPS 6316 - Applied Regression\nTeaching Assistant for Dr. Vito D’Orazio\n\nConducted advanced R programming labs\nFocused on complex regression techniques and econometric analysis\nDeveloped practical exercises for applied data analysis"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\nConfliBERT: Foundation Models for Political Violence Analysis\nwith Patrick T. Brandt, Sultan Alsarra, Vito D’Orazio, Dagmar Heintze, Latifur Khan, Javier Osorio, and Marcus Sianan\nPolitical Analysis (R&R)\nDeveloping specialized language models for political conflict analysis that outperform general-purpose LLMs like Gemma 2, Llama 3.1, and Qwen 2.5 in accuracy, precision, and recall while being hundreds of times faster. Link\n\n\nConflLlama: Domain-Specific Adaptation of Large Language Models for Conflict Event Classification\nwith Patrick T. Brandt\nResearch & Politics, Forthcoming\nIntroducing ConflLlama, a specialized variant of Llama 3.1 fine-tuned for political conflict classification, demonstrating superior performance in event coding and conflict analysis tasks compared to traditional approaches. View Model on Hugging Face\n\n\nPublic Health Advocacy in Times of Pandemic: An Analysis of the Medicare-For-All Debate on Twitter During COVID-19\nwith Sushant Kumar & Pengfei Zhang\nBehavioral Sciences\nAnalyzing how health advocacy groups adapted their Medicare-For-All messaging on Twitter during the COVID-19 pandemic, revealing distinct approaches to public engagement and narrative adaptation. Link"
  },
  {
    "objectID": "index.html#recent-updates",
    "href": "index.html#recent-updates",
    "title": "Shreyas Meher",
    "section": "",
    "text": "February 2025: Paper on Interest Group Communication accepted for R&R at Behavioral Sciences\nJanuary 2025: Released ConfliBERT v2.0 with improved event classification\nDecember 2024: Presented at APSA Political Communication Conference\nNovember 2024: New working paper on IGO withdrawal networks\nOctober 2024: Published ConflLlama-Q documentation"
  },
  {
    "objectID": "research.html#under-review",
    "href": "research.html#under-review",
    "title": "Research",
    "section": "Under Review",
    "text": "Under Review\n\nDemocracy and Internet Control: Theory and Evidence from Transparency Reports\nwith Pengfei Zhang\nExamining how democratic and authoritarian regimes differ in their approach to internet content moderation, using data from transparency reports. We find that while both regime types remove similar amounts of content, their methods and motivations differ significantly.\n\n\nTwo Types of Censorship? An Assessment of the Informational Autocracy Thesis\nTesting how Guriev & Treisman’s Informational Autocracy theory applies to internet filtering practices in autocratic nations, with a focus on regime strategies and implementation methods."
  },
  {
    "objectID": "index.html#title-subtitle-page-layout-full-toc-false",
    "href": "index.html#title-subtitle-page-layout-full-toc-false",
    "title": "Shreyas Meher",
    "section": "",
    "text": "{.rounded-circle .img-fluid .mb-3 width=“250px”}\nPostdoctoral Researcher Erasmus School of Social and Behavioural Sciences\nErasmus University Rotterdam\n{{&lt; fa brands twitter &gt;}} Twitter {{&lt; fa brands github &gt;}} GitHub {{&lt; fa brands linkedin &gt;}} LinkedIn\n\n\n\n\n\nI am a Postdoctoral Researcher in the Policy, Politics, and Society group at the Erasmus School of Social and Behavioural Sciences, Erasmus University Rotterdam. My research leverages computational methods to address core questions in comparative politics and international relations.\nCurrently, as part of the Horizon Europe TWIN4DEM project, I develop machine learning frameworks to model and detect democratic backsliding. My substantive focus is on executive aggrandizement—the process by which elected leaders systematically weaken democratic institutions to consolidate power. My work involves creating the algorithmic core for ‘digital twins’ of political systems, which allows for the simulation and analysis of threats to democratic resilience.\nMy broader research agenda explores:\n\nComputational Politics & Governance: Using large language models (LLMs) and reinforcement learning to detect and analyze political phenomena, from online censorship to the erosion of democratic norms.\nInternational Political Economy: The politics of investment treaties and the network effects within international organizations.\nPolitical Conflict & Event Data: Building and fine-tuning specialized language models (ConfliBERT, ConflLlama) for structured political event classification.\n\nMethodologically, I specialize in computational text analysis, reinforcement learning, and the application of LLMs to political science research.\n\n\n\n\n\nErasmus University Rotterdam (2024-present) As a researcher on the TWIN4DEM (Strengthening Democratic Resilience Through Digital Twins) project, my work focuses on Work Packages related to machine learning and algorithm design. I am responsible for developing advanced computational tools to process and analyze political data in real-time. This includes pre-training and fine-tuning large language models to identify signals of executive aggrandizement and designing simulation algorithms that form the foundation of the project’s ‘digital twin’ models of European democracies.\n\n\n\n\n“Digital Sovereignty: The Political Economy of Internet Governance”\nAn analysis of internet content filtering across regime types.\nAwarded by: The University of Texas at Dallas\nCommittee: Dr. Clint Peinhardt, Co-Chair\nDr. Pengfei Zhang, Co-Chair\nDr. Patrick T. Brandt\nDr. Karl Ho"
  },
  {
    "objectID": "research.html#work-in-progress",
    "href": "research.html#work-in-progress",
    "title": "Research",
    "section": "Work in Progress",
    "text": "Work in Progress\n\nThe TWIN4DEM Project: Modeling Democratic Backsliding\nAs a Postdoctoral Researcher on the Horizon Europe-funded TWIN4DEM project, my work involves developing the machine learning core for ‘digital twins’ of political systems. We aim to model and simulate threats to democratic resilience, with a specific focus on detecting and understanding executive aggrandizement in European democracies."
  }
]