<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Patrick T. Brandt, Vito D’Orazio, Yibo Hu, Latifur Khan, Shreyas Meher, Javier Osorio, Marcus Sianan">
<meta name="dcterms.date" content="2023-11-01">
<meta name="description" content="What ConfliBERT is and how to use it.">

<title>ConfliBERT Documentation and Usage – Shreyas Meher</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<meta name="twitter:title" content="ConfliBERT Documentation and Usage – Shreyas Meher">
<meta name="twitter:description" content="What ConfliBERT is and how to use it.">
<meta name="twitter:creator" content="@onlinecape">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar docked fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">
      Shreyas Meher
      </li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Shreyas Meher</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"> Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../research.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"> Research</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../teaching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"> Teaching</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../CV_2024.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text"> Vitae</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="mailto:shreyas.meher@utdallas.edu" class="sidebar-item-text sidebar-link">
 <span class="menu-text"> Contact</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">ConfliBERT Documentation and Usage</h1>
</div>

<div>
  <div class="description">
    What ConfliBERT is and how to use it.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Patrick T. Brandt, Vito D’Orazio, Yibo Hu, Latifur Khan, Shreyas Meher, Javier Osorio, Marcus Sianan </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 1, 2023</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Whether you are a seasoned data scientist or new to machine learning, this guide is structured to help you navigate through the varied functionalities of ConfliBERT with ease. For a hands-on introduction and demonstration, jump straight to our <a href="#demo-usage">Demo usage</a>. If you’re interested in applying ConfliBERT for <strong>Named Entity Recognition</strong>, see the section on <a href="#ner-named-entity-recognition-with-conflibert">NER (Named Entity Recognition) with ConfliBERT</a>. To dive into <strong>text classification</strong>, visit the section on <a href="#classification-with-conflibert">Classification with ConfliBERT</a>. For <strong>masking and coding tasks</strong>, refer to the section on <a href="#masking-and-coding-tasks-with-conflibert">Masking and Coding Tasks with ConfliBERT</a>. The section on <a href="#computational-considerations-and-benchmarks-for-conflibert">Computational Considerations and Benchmarks for ConfliBERT</a> covers vital aspects of performance optimization.To explore the various adaptations of ConfliBERT, see the section on [ConfliBERT Variants]. At any point, you can refer to the <a href="#references-citations">References / Citations</a> for further reading and information sources that have informed the development and use of ConfliBERT.</p>
<section id="demo-usage" class="level1">
<h1>Demo usage</h1>
<p>For a hands-on example of ConfliBERT in action, check out the <a href="https://colab.research.google.com/drive/1d4557lxoDWKTx0FWcmSPlLx9UEn2BdcA?usp=sharing">Google Colab Demo</a>.</p>
<p>This Google Colab document is a comprehensive guide for setting up and running experiments with the ConfliBERT language model, which is specifically tailored for analyzing political conflict and violence events. Here’s a breakdown of its functions:</p>
<ol type="1">
<li><p><strong>Environment Setup</strong>:</p>
<ul>
<li>The document begins by installing necessary Python packages like <code>torch</code>, <code>transformers</code>, <code>numpy</code>, <code>scikit-learn</code>, and <code>pandas</code>, which are essential for machine learning tasks.</li>
<li>It then installs <code>simpletransformers</code>, a library that simplifies training and using transformer models.</li>
</ul></li>
<li><p><strong>Cloning the ConfliBERT Source Code</strong>:</p>
<ul>
<li>The script clones the ConfliBERT repository from GitHub, ensuring that the latest version of the code and models are used.</li>
</ul></li>
<li><p><strong>Importing Required Modules and Setting Arguments</strong>:</p>
<ul>
<li>Necessary Python modules like <code>pandas</code> for data manipulation, and <code>argparse</code> for handling command line arguments, are imported.</li>
<li>A namespace for arguments (<code>args</code>) is created to manage various settings and configurations.</li>
</ul></li>
<li><p><strong>Selecting a Dataset</strong>:</p>
<ul>
<li>ConfliBERT allows you to choose from a diverse range of processed datasets, each suited for specific analytical tasks. Below is a table showcasing the available datasets and their corresponding tasks. Depending on your research needs or project requirements, you can select the most appropriate dataset:</li>
</ul></li>
</ol>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Task</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>20news</td>
<td>binary</td>
</tr>
<tr class="even">
<td>BBC_News</td>
<td>binary</td>
</tr>
<tr class="odd">
<td>IndiaPoliceEvents_doc</td>
<td>multilabel</td>
</tr>
<tr class="even">
<td>IndiaPoliceEvents_sents</td>
<td>multilabel</td>
</tr>
<tr class="odd">
<td>cameo_class</td>
<td>multiclass</td>
</tr>
<tr class="even">
<td>cameo_ner</td>
<td>ner</td>
</tr>
<tr class="odd">
<td>insightCrime</td>
<td>multilabel</td>
</tr>
<tr class="even">
<td>re3d</td>
<td>ner</td>
</tr>
<tr class="odd">
<td>satp_relevant</td>
<td>multilabel</td>
</tr>
</tbody>
</table>
<ul>
<li>Each dataset is tailored for different types of classification tasks, such as binary classification, multilabel classification, multiclass classification, and named entity recognition (NER). This versatility allows for a wide range of analyses, from simple binary decisions to complex entity extraction and multilabel classifications.</li>
</ul>
<p>For more detailed information about each dataset and to understand how to align them with your specific needs, please visit the <a href="https://github.com/eventdata/ConfliBERT/tree/main/configs">ConfliBERT GitHub repository</a>.</p>
<ol start="5" type="1">
<li><strong>Training Configuration</strong>:
<ul>
<li>It sets up training configurations including the task type (<code>multilabel</code>), the number of seeds for training, initial seed value, number of epochs, batch size, and maximum sequence length.</li>
<li>Two models are included in this example (<code>ConfliBERT-scr-uncased</code> and <code>bert-base-uncased</code>), with their respective paths and configurations.</li>
</ul></li>
<li><strong>Custom Configuration and Data Loading</strong>:
<ul>
<li>The custom training configuration is saved to the <code>args</code> object.</li>
<li>The script then loads training, evaluation, and testing datasets, along with the number of labels, and label list for Named Entity Recognition (NER) tasks, if applicable.</li>
</ul></li>
<li><strong>Model Training</strong>:
<ul>
<li>The document executes a loop to run experiments for all the models listed in the configuration.</li>
<li>For each model, it sets up an output directory and calls the <code>train_multi_seed</code> function, passing the training, evaluation, and test datasets along with the model configurations.</li>
</ul></li>
<li><strong>Results Compilation</strong>:
<ul>
<li>Finally, it reads and displays the full report of the training results from a CSV file. This file includes detailed performance metrics for each model and configuration.</li>
</ul></li>
</ol>
<p>Overall, this Colab document is an all-in-one toolkit for users looking to leverage the ConfliBERT model for their research or projects related to political events analysis.</p>
</section>
<section id="introduction-to-conflibert" class="level1">
<h1>Introduction to ConfliBERT:</h1>
<p>ConfliBERT <span class="citation" data-cites="hu2022conflibert">Hu et al. (<a href="#ref-hu2022conflibert" role="doc-biblioref">2022</a>)</span> is a domain-specific pre-trained language model tailored for the analysis of political conflict and violence that was developed through a collaboration between conflict scholars and computer scientists. It was introduced in a paper titled “ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence,” which was presented at the North American Chapter of the Association for Computational Linguistics (NAACL) in 2022. ConfliBERT’s design and architecture cater to a wide range of interactions, from material and verbal conflicts to cooperative endeavors, encompassing events like protests, insurgencies, civil wars, and diplomatic disputes.</p>
<p>The study and monitoring of political violence and conflict have long been vital domains within the political science and policy communities. The traditional methods used by many event data systems to understanding these dynamics, such as manual coding and pattern-matching techniques, though valuable, have limitations in terms of scale and adaptability. Also, many of these legacy systems are dictionary-based, which makes them difficult to maintain and update <span class="citation" data-cites="halterman2023plover">Halterman et al. (<a href="#ref-halterman2023plover" role="doc-biblioref">2023</a>)</span>. ConfliBERT helps to overcome these issues with its machine learning approach to data analysis.</p>
<p>ConfliBERT offers a promising avenue for advancing research in political science by enabling the efficient and comprehensive analysis of conflict processes. By bridging the expertise of political science and advanced natural language processing, it has the potential to redefine traditional methods, facilitating more nuanced and expansive research outcomes. It also represents a significant advancement in domain-specific language models for political conflict and violence, offering researchers and practitioners a tool that has been pre-trained on relevant data, thus ensuring better performance on domain-related tasks.</p>
<section id="what-does-conflibert-do-and-why" class="level2">
<h2 class="anchored" data-anchor-id="what-does-conflibert-do-and-why">What does ConfliBERT do and why?:</h2>
<ol type="1">
<li><p><strong>Domain-Specific Pre-training:</strong> While many language models are trained on general corpora, ConfliBERT’s training is rooted in domain-specific corpora. This focus allows for enhanced performance in performing classification tasks related to political violence, conflict, cooperation, and diplomacy.</p></li>
<li><p><strong>Utility for Various Stakeholders:</strong> ConfliBERT is beneficial for academics, policymakers, and security analysts. It provides an efficient tool for monitoring and understanding the multifaceted dynamics of social unrest, political upheavals, and other conflict-related events on a global scale.</p></li>
<li><p><strong>Automation and Efficiency:</strong> Traditional conflict analysis methods have their limitations in terms of scale and speed. ConfliBERT, with its automation capabilities, can parse vast datasets, significantly alleviating the challenges of manual annotations.</p></li>
<li><p><strong>Broad Application Potential:</strong> Beyond mere data categorization, ConfliBERT can be employed for a diverse range of tasks related to conflict research, including event classification, entity recognition, relationship extraction, and data augmentation.</p></li>
</ol>
</section>
<section id="running-examples" class="level2">
<h2 class="anchored" data-anchor-id="running-examples">Running Examples</h2>
<p>In the following sections, we will walk through illustrative examples of ConfliBERT’s capabilities in action. These examples are designed to help researchers and analysts better understand how ConfliBERT automates the coding of textual data into structured conflict and event data. Grasping these capabilities is essential for a myriad of applications such as real-time monitoring of political events, in-depth conflict analysis, and the accumulation of data for international relations and peace studies.</p>
<p><em>Make a Statement</em> President Biden said that the U.S. alliance with Japan is stronger than ever.</p>
<p><em>Verbal Cooperation</em> Saudi Arabia expressed intent to restore diplomatic relations with Iran.</p>
<p><em>Material Cooperation</em> China provided humanitarian aid to Turkey.</p>
<p><em>Verbal Conflict</em> President Zelenskyy accused Russia of war crimes.</p>
<p><em>Material Conflict</em> Israeli forces attacked Hamas in Gaza City.</p>
</section>
<section id="key-features-and-components" class="level2">
<h2 class="anchored" data-anchor-id="key-features-and-components">Key Features and Components:</h2>
<ol type="1">
<li><p><strong>Platform and Requirements:</strong></p>
<ul>
<li>ConfliBERT’s code requires a Python installation on your computer.</li>
<li>Necessary packages include torch, transformers, numpy, scikit-learn, pandas, and simpletransformers.</li>
<li>CUDA 10.2 support is included for GPU acceleration.</li>
</ul></li>
<li><p><strong>Model Versions:</strong></p>
<ul>
<li>Four distinct versions of ConfliBERT are available:
<ol type="1">
<li>ConfliBERT-scr-uncased: Pre-trained from scratch using an uncased vocabulary.</li>
<li>ConfliBERT-scr-cased: Pre-trained from scratch using a cased vocabulary.</li>
<li>ConfliBERT-cont-uncased: Continual pre-training using the original BERT’s uncased vocabulary.</li>
<li>ConfliBERT-cont-cased: Continual pre-training using BERT’s cased vocabulary.</li>
</ol></li>
</ul>
<p>These models are available on Huggingface and can be imported directly using its API.</p></li>
<li><p><strong>Evaluation and Usage:</strong></p>
<ul>
<li>Using ConfliBERT is analogous to other BERT models within the Huggingface ecosystem.</li>
<li>Examples are provided for fine-tuning using the Simple Transformers library.</li>
<li>A Google Colab demo is available for hands-on evaluation and experimentation.</li>
</ul></li>
<li><p><strong>Evaluation Datasets:</strong></p>
<ul>
<li>Several datasets related to news, global events, and political conflicts are mentioned for evaluation. These datasets include:
<ul>
<li>20Newsgroups, BBCnews, EventStatusCorpus, GlobalContention, GlobalTerrorismDatabase, Gun Violence Database, IndiaPoliceEvents, InsightCrime, MUC-4, re3d, SATP, and CAMEO.</li>
</ul></li>
<li>Custom datasets can be integrated after preprocessing into the required formats.</li>
</ul></li>
<li><p><strong>Pre-Training Corpus:</strong></p>
<ul>
<li>ConfliBERT was pre-trained on an extensive corpus from the politics and conflict domain (33 GB in size).</li>
<li>Due to copyright constraints, only sample scripts and a few samples from the pre-training corpus are provided. The details of the pre-training corpus are documented in Section 2 and the Appendix of the paper.</li>
</ul></li>
<li><p><strong>Pre-Training Process:</strong></p>
<ul>
<li>ConfliBERT utilizes the same pre-training scripts (<code>run_mlm.py</code>) from Huggingface. An example is provided for pre-training on 8 GPUs, though parameters should be adapted based on the user’s available resources.</li>
</ul></li>
<li><p><strong>Citation:</strong></p>
<ul>
<li>If used for research purposes, it is recommended to cite the original paper. The citation details are provided in the repository.</li>
</ul></li>
</ol>
</section>
<section id="main-tasks" class="level2">
<h2 class="anchored" data-anchor-id="main-tasks">Main Tasks</h2>
<ul>
<li><p>NER</p></li>
<li><p>Classification of events</p></li>
<li><p>Masking and coding tasks</p></li>
</ul>
</section>
</section>
<section id="ner-named-entity-recognition-with-conflibert" class="level1">
<h1>NER (Named Entity Recognition) with ConfliBERT</h1>
<p>Named Entity Recognition (NER) is a fundamental task in the field of natural language processing (NLP) that involves identifying and classifying key information (entities) in text into predefined categories. These categories can include names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.</p>
<p>ConfliBERT, like BERT, is inherently capable of recognizing and categorizing entities. It uses the context within which words appear to determine their meaning and classify them accordingly. The standard model can identify a variety of entities such as:</p>
<ul>
<li><code>PERSON</code>: People, including fictional persons.</li>
<li><code>GPE</code>: Geopolitical entities, like countries, cities, and states.</li>
<li><code>ORG</code>: Organizations, including governmental, companies, and agencies.</li>
</ul>
<p>ConfliBERT enhances BERT’s capabilities by being specifically fine-tuned on conflict and event data. This specialized focus enables it to be more effective in contexts related to political science, international relations, and conflict studies.</p>
<section id="fine-tuning-for-custom-entities" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-for-custom-entities">Fine-Tuning for Custom Entities</h3>
<p>However, the scope of entities that can be recognized is not limited to the defaults. Users can fine-tune ConfliBERT on domain-specific corpora that contain unique entities of relevance to their research. For instance, one might train it to recognize military equipment, political parties, or specific event-related terminology.</p>
<p>To integrate more entities and classifications, researchers can proceed with the following steps:</p>
<ol type="1">
<li><strong>Data Collection</strong>: Compile a dataset where the additional entities are labeled in the context of their use.</li>
<li><strong>Annotation</strong>: Use a consistent annotation schema to mark up the entities in the dataset.</li>
<li><strong>Model Training</strong>: Fine-tune ConfliBERT on this annotated dataset. This involves training the model to adjust its weights and biases to better predict your custom labels.</li>
<li><strong>Evaluation</strong>: Assess the model’s performance on a separate validation dataset and iterate on your approach as needed.</li>
</ol>
<p>Fine-tuning allows ConfliBERT to expand its understanding and recognition capabilities to align with specific research needs or domain-specific requirements.</p>
<p>
<span style="color: blue;">President Biden</span> <span style="font-weight: bold;">PERSON</span> said that the <span style="color: green;">U.S.</span> <span style="font-weight: bold;">GPE</span> alliance with <span style="color: green;">Japan</span> <span style="font-weight: bold;">GPE</span> is stronger than ever.
</p>
<p>
<span style="color: green;">Saudi Arabia</span> <span style="font-weight: bold;">GPE</span> expressed intent to restore diplomatic relations with <span style="color: green;">Iran</span> <span style="font-weight: bold;">GPE</span>.
</p>
<p>
<span style="color: green;">China</span> <span style="font-weight: bold;">GPE</span> provided humanitarian aid to <span style="color: green;">Turkey</span> <span style="font-weight: bold;">GPE</span>.
</p>
<p>
<span style="color: blue;">President Zelenskyy</span> <span style="font-weight: bold;">PERSON</span> accused <span style="color: green;">Russia</span> <span style="font-weight: bold;">GPE</span> of war crimes.
</p>
<p>
<span style="color: orange;">Israeli forces</span> <span style="font-weight: bold;">ORG</span> attacked <span style="color: orange;">Hamas</span> <span style="font-weight: bold;">ORG</span> in <span style="color: green;">Gaza City</span> <span style="font-weight: bold;">GPE</span>.
</p>
<p>Event data, in conflict research, refers to the systematic and chronological cataloging of political interactions, actions, and conflicts. It is instrumental in detailing when and where an incident happened, who was involved, and the nature of the event. The granularity of event data can range from large-scale political changes to minor altercations. Thus, the swift and accurate extraction of these data points from voluminous and diverse sources is paramount.</p>
<p>Here is how NER, when integrated with ConfliBERT, aids in the enrichment of event data:</p>
<ol type="1">
<li><p><strong>Contextual Identification</strong>: With the specialized pre-training of ConfliBERT on conflict and political violence texts, the NER process becomes adept at distinguishing entities that are of significance in this domain. For instance, while general NLP models might identify ‘Aleppo’ merely as a location, ConfliBERT can contextualize it within the Syrian conflict, recognizing it as a key site of unrest.</p></li>
<li><p><strong>Actor Recognition and Classification</strong>: Political events often involve a myriad of actors - from state agents and rebel groups to international entities. ConfliBERT’s NER can discern and categorize these actors, providing clarity on the roles they play in specific incidents. This facilitates a richer and more layered analysis of events.</p></li>
<li><p><strong>Event Dynamics and Temporal Analysis</strong>: By associating entities with timestamps and specific actions, ConfliBERT can provide a chronological sequence of events. This temporal dimension is vital in understanding the progression of conflicts and predicting potential future escalations.</p></li>
<li><p><strong>Enhancing Manual Coding</strong>: Manual coding of event data, a traditional method, can be significantly enhanced by automating the extraction process. ConfliBERT can quickly scan through vast amounts of data, flagging key entities and events, thereby reducing the manual labor and errors inherent in such a task.</p></li>
<li><p><strong>Real-time Analysis</strong>: Given the swift nature of NER in ConfliBERT, real-time sources like news articles or social media feeds can be processed promptly. This ensures that event datasets remain up-to-date, capturing the dynamism of political conflicts.</p></li>
<li><p><strong>Event Characterization</strong>: Beyond just identifying entities, NER in ConfliBERT can also help in characterizing the event itself. By recognizing and associating various entities and actions, the model can provide insights into the nature of the event, be it a peaceful protest, armed insurgency, or a diplomatic negotiation.</p></li>
</ol>
<p>In essence, the synergy between ConfliBERT’s NER capabilities and event data extraction offers a transformative approach to conflict research. It streamlines the data gathering process, enhances the depth and breadth of analysis, and empowers researchers with a tool that resonates with the nuances of political violence and upheaval.</p>
</section>
<section id="inputs-for-ner" class="level2">
<h2 class="anchored" data-anchor-id="inputs-for-ner">Inputs for NER</h2>
<p>Basic NER terminology:</p>
<ul>
<li><strong>B</strong> (Beginning) denotes the beginning of an entity.</li>
<li><strong>I</strong> (Inside) marks the subsequent words of a multi-word entity.</li>
<li><strong>O</strong> (Outside) is used for non-entity words.</li>
</ul>
<section id="inputs-for-ner-using-the-conflibert-model" class="level3">
<h3 class="anchored" data-anchor-id="inputs-for-ner-using-the-conflibert-model">Inputs for NER using the ConfliBERT model:</h3>
<section id="text-pre-processing" class="level4">
<h4 class="anchored" data-anchor-id="text-pre-processing">1. <strong>Text Pre-processing</strong>:</h4>
<p>Before feeding the data to ConfliBERT (or any BERT-like model), one needs to ensure the text data is pre-processed:</p>
<ul>
<li><p><strong>Tokenization</strong>: Convert sentences into tokens. Depending on the language and context, tokenization might split words into subwords or characters.</p></li>
<li><p><strong>Lowercasing</strong>: This step is optional and based on the pre-trained ConfliBERT model. Some BERT models are case-sensitive.</p></li>
<li><p><strong>Special Tokens</strong>: BERT models usually expect [CLS] and [SEP] tokens to mark the beginning and end of a sentence, respectively.</p>
<p>For example: <code>[CLS] U.S. military chief General Colin Powell said ... [SEP]</code></p></li>
<li><p><strong>Padding and Truncating</strong>: Make sure each input sequence has the same length by either padding short sequences or truncating long ones.</p></li>
</ul>
</section>
<section id="setting-options-for-classification" class="level4">
<h4 class="anchored" data-anchor-id="setting-options-for-classification">2. <strong>Setting Options for Classification</strong>:</h4>
<ul>
<li><p><strong>Entity Tags</strong>: Define a fixed set of entity tags, such as <code>B-S</code>, <code>I-S</code>, <code>B-T</code>, <code>I-T</code>, and <code>O</code>. This is crucial for the classification layer’s output size.</p></li>
<li><p><strong>Loss Function</strong>: Since NER is a multi-class classification problem, use a categorical cross-entropy loss.</p></li>
<li><p><strong>Model Architecture</strong>: Depending on the version and customization of ConfliBERT, ensure the last layer is a dense layer with an output size equal to the number of unique entity tags.</p></li>
</ul>
</section>
<section id="input-files" class="level4">
<h4 class="anchored" data-anchor-id="input-files">3. <strong>Input Files</strong>:</h4>
<p>The files provided seem to be in a CoNLL-like format, which is standard for NER tasks. A breakdown of the structure:</p>
<ul>
<li>Each word is on a new line with its respective IOB-tag.</li>
<li>Sentences or sequences are separated by blank lines.</li>
</ul>
<p>Example:</p>
<pre><code>U.S. B-S
military I-S
...
. O

NATO B-S
...
. O</code></pre>
<p><strong>Note</strong>: The input file’s structure needs to be consistent for effective model training and evaluation.</p>
</section>
<section id="additional-metadata-optional" class="level4">
<h4 class="anchored" data-anchor-id="additional-metadata-optional">4. <strong>Additional Metadata (Optional)</strong>:</h4>
<p>Depending on the specificities of the ConfliBERT model or the task requirements:</p>
<ul>
<li><strong>Attention Masks</strong>: Used to tell the model to pay attention to specific tokens and ignore others (like padding tokens).</li>
<li><strong>Segment IDs</strong>: If handling multiple sentences in a sequence, segment IDs can distinguish them.</li>
</ul>
</section>
<section id="training-and-evaluation-data" class="level4">
<h4 class="anchored" data-anchor-id="training-and-evaluation-data">5. <strong>Training and Evaluation Data</strong>:</h4>
<ul>
<li><strong>Training Data</strong>: Pre-processed sentences along with their entity tags for training the model.</li>
<li><strong>Evaluation Data</strong>: A separate set of pre-processed sentences and their tags to validate the model’s performance.</li>
</ul>
</section>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion:</h3>
<p>Users should prepare their data in a CoNLL-like format, ensure text pre-processing aligns with ConfliBERT’s requirements, and define a set of entity tags for classification. The more consistent and clean the input data, the better the model’s performance.</p>
<p>Based on the given example outputs, the sections for “Outputs from NER,” “Metrics for NER,” and “Evaluation of NER” are provided next.</p>
<hr>
</section>
</section>
<section id="outputs-from-ner" class="level2">
<h2 class="anchored" data-anchor-id="outputs-from-ner">Outputs from NER</h2>
<p><strong>Sentence:</strong> “For the first time in decades, there is at least the potential of an armed clash with America’s largest adversaries, Russia and China.”</p>
<p><strong>Predictions:</strong> The named entity recognition model has identified three entities in the given sentence: - <code>decades</code>: Temporal Entity - <code>America’s</code>: Government - <code>Russia</code>: Government - <code>China</code>: Government</p>
<p><strong>Raw Outputs:</strong> There also is likely the raw logits or scores associated with each token for different potential named entity classes given with the outputs. For brevity, they are not all listed here, but they can be useful for diving deeper into model decisions or for model calibration.</p>
<hr>
</section>
<section id="metrics-for-ner" class="level2">
<h2 class="anchored" data-anchor-id="metrics-for-ner">Metrics for NER</h2>
<p>Here is a general format you might encounter:</p>
<ul>
<li><p><strong>Precision:</strong> This is a measure of the accuracy provided that a specific class (or label) has been predicted. It is the number of true positives divided by the sum of true positives and false positives.</p></li>
<li><p><strong>Recall:</strong> This is a measure of the ability of the model to find all the relevant cases within a dataset. It is the number of true positives divided by the sum of true positives and false negatives.</p></li>
<li><p><strong>F1-Score:</strong> The F1 score is the harmonic mean of precision and recall. It provides a balance between the two. When it is closer to 1, it indicates better performance, and 0 indicates poorer performance.</p></li>
<li><p><strong>Accuracy:</strong> This metric calculates the ratio of correctly predicted observation to the total observations.</p></li>
</ul>
<p>(Note: For a complete evaluation, one would typically calculate these metrics for each entity type, as well as overall.)</p>
<hr>
</section>
<section id="evaluation-of-ner" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-of-ner">Evaluation of NER</h2>
<p>The model seems to correctly identify the governmental entities <code>America’s</code>, <code>Russia</code>, and <code>China</code>. Additionally, the model successfully picked up on the temporal entity <code>decades</code>.</p>
<p>However, the evaluation of the model’s performance would ideally require a much larger test dataset, encompassing a diverse range of sentences and contexts. Furthermore, an in-depth evaluation would involve comparing the model’s predictions against a ground truth or a labeled dataset to compute metrics like precision, recall, and F1-score.</p>
<p>One key aspect to consider in such models is their confidence scores (or probabilities) associated with predictions. From the raw outputs, we can extract these values to potentially set thresholds or make informed decisions.</p>
<hr>
<p>Remember, while metrics provide a quantitative measure of performance, qualitative analysis (like manually examining where the model goes right or wrong) is invaluable, especially when deploying in critical applications.</p>
</section>
<section id="satp_12.20-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="satp_12.20-pipeline"><a href="https://github.com/javierosorio/SATP/tree/master">SATP_12.20 Pipeline</a></h2>
<p>Here is a detailed breakdown of the preprocessing and training steps for Named Entity Recognition (NER):</p>
<section id="annotation-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="annotation-pipeline">1. Annotation Pipeline</h3>
<ul>
<li><strong>File</strong>: <code>1_annotation_pipeline-12.21.ipynb</code></li>
<li><strong>Description</strong>: This notebook likely contains the process for manually annotating or labelling the data. The annotations refer to the entities within the text that the model will later be trained to recognize.</li>
</ul>
</section>
<section id="preparing-the-training-and-testing-data" class="level3">
<h3 class="anchored" data-anchor-id="preparing-the-training-and-testing-data">2. Preparing the Training and Testing Data</h3>
<ul>
<li><strong>File</strong>: <code>2_Preprocess-Data-12.27.ipynb</code></li>
<li><strong>Description</strong>: After annotation, the data needs to be prepared for model training. This might include splitting the data into training, validation, and test sets; tokenizing the data; and converting it into a format suitable for deep learning frameworks.</li>
</ul>
</section>
<section id="bilstm-baseline-deep-learning-model" class="level3">
<h3 class="anchored" data-anchor-id="bilstm-baseline-deep-learning-model">3. BiLSTM Baseline Deep Learning Model</h3>
<ul>
<li><strong>File</strong>: <code>3_BiLSTM.ipynb</code></li>
<li><strong>Description</strong>: This notebook contains the implementation and training process of a BiLSTM (Bidirectional Long Short-Term Memory) model. BiLSTMs are a type of recurrent neural network that can capture context from both past and future input sequences.</li>
</ul>
</section>
<section id="bert-model" class="level3">
<h3 class="anchored" data-anchor-id="bert-model">4. BERT Model</h3>
<ul>
<li><strong>File</strong>: <code>4_transformer_reduced_bert.ipynb</code></li>
<li><strong>Description</strong>: Here, the BERT (Bidirectional Encoder Representations from Transformers) model is implemented and trained. BERT is a popular transformer-based model known for its effectiveness in various NLP tasks, including NER.</li>
</ul>
</section>
<section id="traditional-machine-learning-model-baselines" class="level3">
<h3 class="anchored" data-anchor-id="traditional-machine-learning-model-baselines">5. Traditional Machine Learning Model Baselines</h3>
<ul>
<li><strong>File</strong>: <code>5_Baselines_reduced-2-step.ipynb</code></li>
<li><strong>Description</strong>: This notebook provides baseline results using traditional machine learning models like Support Vector Machines (SVM) and Logistic Regression (LR). A two-step pipeline might refer to a two-phase process: feature extraction and then modeling.</li>
</ul>
</section>
<section id="hierarchical-attention-networks-han" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-attention-networks-han">6. Hierarchical Attention Networks (HAN)</h3>
<ul>
<li><strong>Steps</strong>:
<ol type="1">
<li><strong>Creating Word Vectors and DataLoader from train.csv</strong>:
<ul>
<li><strong>Command</strong>: <code>python create_input_files.py</code></li>
<li><strong>Description</strong>: This script prepares the data and creates word embeddings, possibly using pre-trained models. The embeddings will be used to represent words or tokens in the data. It also sets up data loaders which are essential for training deep learning models in batches.</li>
</ul></li>
<li><strong>Training and Evaluating the HAN Model</strong>:
<ul>
<li><strong>Command</strong>: <code>python HAN_end2end.py</code></li>
<li><strong>Description</strong>: Hierarchical Attention Networks are neural models that can pay differentiated attention to various parts of the input data, making them suitable for tasks where the importance of different parts of the input varies.</li>
</ul></li>
</ol></li>
</ul>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results:</h3>
<p>The results section showcases the performance of various models on the task:</p>
<ul>
<li>Metrics used include:
<ul>
<li><strong>Accuracy</strong>: The ratio of correctly predicted entities to the total entities.</li>
<li><strong>Precision</strong>: The ratio of correctly predicted positive entities to the total predicted positives.</li>
<li><strong>Recall</strong>: The ratio of correctly predicted positive entities to the total actual positives.</li>
<li><strong>F1 Score</strong>: The harmonic mean of precision and recall.</li>
<li><strong>Exact Matching Ratio</strong>: This might refer to the ratio of data points where the predicted entities exactly match the true entities.</li>
</ul></li>
<li>The models and their respective performance metrics are tabulated for comparison.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Accuracy (%)</th>
<th style="text-align: center;">Precision (%)</th>
<th style="text-align: center;">Recall (%)</th>
<th style="text-align: center;">F1 (%)</th>
<th style="text-align: center;">Exact Matching Ratio (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">One vs Rest</td>
<td style="text-align: center;">26.07</td>
<td style="text-align: center;">87.19</td>
<td style="text-align: center;">27.40</td>
<td style="text-align: center;">28.04</td>
<td style="text-align: center;">82.20</td>
</tr>
<tr class="even">
<td style="text-align: center;">Binary Relevance</td>
<td style="text-align: center;">26.65</td>
<td style="text-align: center;">86.08</td>
<td style="text-align: center;">28.29</td>
<td style="text-align: center;">28.40</td>
<td style="text-align: center;">82.47</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Class Chain</td>
<td style="text-align: center;">27.93</td>
<td style="text-align: center;">83.47</td>
<td style="text-align: center;">29.78</td>
<td style="text-align: center;">29.32</td>
<td style="text-align: center;">82.87</td>
</tr>
<tr class="even">
<td style="text-align: center;">Label Powerset</td>
<td style="text-align: center;">28.86</td>
<td style="text-align: center;">85.53</td>
<td style="text-align: center;">30.51</td>
<td style="text-align: center;">30.05</td>
<td style="text-align: center;">83.14</td>
</tr>
<tr class="odd">
<td style="text-align: center;">BiLSTM</td>
<td style="text-align: center;">37.11</td>
<td style="text-align: center;">53.81</td>
<td style="text-align: center;">58.51</td>
<td style="text-align: center;">42.12</td>
<td style="text-align: center;">76.23</td>
</tr>
<tr class="even">
<td style="text-align: center;">HAN</td>
<td style="text-align: center;">52.98</td>
<td style="text-align: center;">65.64</td>
<td style="text-align: center;">74.82</td>
<td style="text-align: center;">55.78</td>
<td style="text-align: center;">83.07</td>
</tr>
<tr class="odd">
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">62.52</td>
<td style="text-align: center;">74.73</td>
<td style="text-align: center;">80.01</td>
<td style="text-align: center;">64.59</td>
<td style="text-align: center;">87.23</td>
</tr>
</tbody>
</table>
<p>This comprehensive pipeline takes the data from raw, annotated form and processes it into a format that various machine learning and deep learning models can be trained on. The models’ performances are then evaluated and compared using the mentioned metrics.</p>
</section>
</section>
<section id="other-llm-comparisons-on-this-task" class="level2">
<h2 class="anchored" data-anchor-id="other-llm-comparisons-on-this-task">Other LLM comparisons on this task</h2>
</section>
<section id="related-literature-in-ner" class="level2">
<h2 class="anchored" data-anchor-id="related-literature-in-ner">Related Literature in NER</h2>
</section>
</section>
<section id="classification-with-conflibert" class="level1">
<h1>Classification with ConfliBERT</h1>
<p>Text classification is the process of assigning tags or categories to text according to its content. ConfliBERT can be trained to classify text into categories such as ‘Verbal Cooperation’, ‘Material Cooperation’, ‘Verbal Conflict’, and ‘Material Conflict’, based on the context and content of the input text.</p>
<section id="expanding-classification-categories" class="level3">
<h3 class="anchored" data-anchor-id="expanding-classification-categories">Expanding Classification Categories</h3>
<p>Similar to entity recognition, classification categories can be tailored. Researchers can introduce additional categories like “Economic Sanctions”, “Legal Actions”, “Diplomatic Events”, etc., by fine-tuning ConfliBERT with appropriately tagged training data. This custom classification can help in creating more nuanced datasets that capture the complexity of international relations and conflict scenarios.</p>
<p>The fine-tuning process for expanding classification capabilities involves:</p>
<ol type="1">
<li><strong>Category Definition</strong>: Define clear and distinct categories that you want ConfliBERT to classify the text into.</li>
<li><strong>Data Annotation</strong>: Label a sufficiently large corpus of text with these categories.</li>
<li><strong>Model Training</strong>: Use this labeled data to fine-tune ConfliBERT so that it can predict the probability of any given text belonging to these categories.</li>
<li><strong>Model Evaluation</strong>: Test the model’s classification accuracy on unseen data and refine your categories and data as needed.</li>
</ol>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Sentence</th>
<th style="text-align: center;">Make a Statement</th>
<th style="text-align: center;">Verbal Cooperation</th>
<th style="text-align: center;">Material Cooperation</th>
<th style="text-align: center;">Verbal Conflict</th>
<th style="text-align: center;">Material Conflict</th>
<th style="text-align: center;">Predicted Category</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>President Biden said that the U.S. alliance with Japan is stronger than ever.</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">Make a Statement</td>
</tr>
<tr class="even">
<td>Saudi Arabia expressed intent to restore diplomatic relations with Iran.</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">Verbal Cooperation</td>
</tr>
<tr class="odd">
<td>China provided humanitarian aid to Turkey.</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">Material Cooperation</td>
</tr>
<tr class="even">
<td>President Zelenskyy accused Russia of war crimes.</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">Verbal Conflict</td>
</tr>
<tr class="odd">
<td>Israeli forces attacked Hamas in Gaza City.</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">Material Conflict</td>
</tr>
</tbody>
</table>
<p>Please note, the logits (probabilities) here are illustrative and assume that the model is making predictions with high confidence and that each sentence strongly represents its respective category. The “Predicted Category” is determined by the highest logit score for each sentence. In real-world scenarios, the logits would be the actual output from a model like ConfliBERT after processing the sentences.</p>
<p>Here’s the revised section, incorporating HTML inline styling for color changes as per your examples:</p>
<hr>
</section>
<section id="understanding-classification-metrics-in-conflibert" class="level2">
<h2 class="anchored" data-anchor-id="understanding-classification-metrics-in-conflibert">Understanding Classification Metrics in ConfliBERT</h2>
<p>This document provides an overview of classification metrics in the context of political event categorization using ConfliBERT, drawing upon foundational concepts in machine learning classification such as True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN) as explained in the Google Machine Learning Crash Course (see <span class="citation" data-cites="googleMLCrashCourse"><span>“Machine Learning Crash Course: Classification: True/False Positive/Negative”</span> (<a href="#ref-googleMLCrashCourse" role="doc-biblioref">2021</a>)</span>). We will use a hypothetical example to illustrate these concepts clearly:</p>
<p><strong>Metrics Explained:</strong></p>
<p>
<span style="color: green;"><strong>True Positives (TP):</strong> Sentences where the Predicted Category matches the actual category (highest logit score).</span>
</p>
<p>
<span style="color: red;"><strong>False Positives (FP):</strong> Cases where the Predicted Category was chosen, but it is not the actual category.</span>
</p>
<p>
<span style="color: red;"><strong>False Negatives (FN):</strong> Cases where the actual category was correct, but the Predicted Category was different.</span>
</p>
<p>
<span style="color: green;"><strong>True Negatives (TN):</strong> All other cases where neither the predicted nor the actual category was chosen.</span>
</p>
<p><strong>Hypothetical Example for Classification:</strong></p>
<p>Consider an event: <em>Israeli forces attacked Hamas in Gaza City.</em> Let’s place this event in a 2x2 confusion matrix to understand potential outcomes from ConfliBERT’s classification:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 47%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>True Positive (TP)</th>
<th>False Negative (FN)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ConfliBERT accurately classifies the event as “Material Conflict” due to the active engagement and physical altercation reported.</td>
<td>ConfliBERT incorrectly classifies the event as a non-conflict category, such as “Verbal Cooperation,” missing the material nature of the conflict.</td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<colgroup>
<col style="width: 54%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>False Positive (FP)</th>
<th>True Negative (TN)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ConfliBERT incorrectly classifies a peaceful negotiation as “Material Conflict,” suggesting a physical altercation where there was none.</td>
<td>ConfliBERT correctly identifies that no material conflict occurred when Israeli forces did not engage with Hamas.</td>
</tr>
</tbody>
</table>
<p><strong>Detailed Breakdown:</strong></p>
<p>
<span style="color: green;"><strong>True Positive (TP):</strong> - <em>Reality:</em> Israeli forces engage in a physical altercation with Hamas in Gaza City. - <em>ConfliBERT Says:</em> “Material Conflict.” - <em>Outcome:</em> ConfliBERT’s classification aligns with the real-world event, providing accurate data for analysis.</span>
</p>
<p>
<span style="color: red;"><strong>False Positive (FP):</strong> - <em>Reality:</em> A diplomatic meeting between Israeli and Hamas representatives occurs without physical confrontation. - <em>ConfliBERT Says:</em> “Material Conflict.” - <em>Outcome:</em> ConfliBERT erroneously signals a conflict, potentially leading to misinformed analysis.</span>
</p>
<p>
<span style="color: red;"><strong>False Negative (FN):</strong> - <em>Reality:</em> Israeli forces attack Hamas in Gaza City. - <em>ConfliBERT Says:</em> “Verbal Cooperation” or another non-conflict category. - <em>Outcome:</em> Misclassification downplays the severity, potentially affecting response strategies.</span>
</p>
<p>
<span style="color: green;"><strong>True Negative (TN):</strong> - <em>Reality:</em> Israeli and Hamas representatives engage in peaceful talks. - <em>ConfliBERT Says:</em> “Verbal Cooperation” or an appropriate non-conflict category. - <em>Outcome:</em> ConfliBERT accurately reflects the peaceful nature of the event, aiding correct analysis and policy decisions.</span>
</p>
<p><strong>Applying Metrics for Evaluation:</strong></p>
<p>The subsequent sections will explore how to use these metrics—sensitivity, specificity, precision, and accuracy—to evaluate ConfliBERT’s performance in classifying complex political events. These metrics are essential for assessing ConfliBERT’s accuracy and fine-tuning its predictions to align with real-world observations.</p>
<p>Here is a deep dive into how ConfliBERT enhances classification in the context of conflict research:</p>
<ol type="1">
<li><p><strong>Granular Event Categorization</strong>: While it is essential to identify an incident as a protest or a riot, the true value lies in discerning the nature of these events. ConfliBERT’s classification capability can distinguish between a peaceful protest, a violent uprising, or even a state-sanctioned crackdown. This granularity ensures a comprehensive understanding of the event’s nature.</p></li>
<li><p><strong>Adaptive Learning with Domain-Specific Data</strong>: Given ConfliBERT’s training on conflict-related texts, it possesses a heightened sensitivity to the subtle nuances within conflict narratives. This domain-specific expertise translates to a more accurate and contextual classification, be it categorizing a skirmish as sectarian, ethnic, or political.</p></li>
<li><p><strong>Temporal Classification</strong>: Conflicts evolve, and so do their narratives. ConfliBERT can classify events based on their temporal context, distinguishing between a long-standing civil war’s initial skirmishes and its climax battles or identifying the phases of diplomatic negotiations.</p></li>
<li><p><strong>Automating Large-Scale Data Processing</strong>: Conflict research often grapples with voluminous data from diverse sources like news articles, social media, and firsthand accounts. Manual classification of such vast data is not only labor-intensive but also prone to inconsistencies. ConfliBERT automates this, ensuring uniformity and efficiency.</p></li>
<li><p><strong>Real-time Classification for Dynamic Responses</strong>: In the rapidly changing landscape of political conflicts, timely responses are crucial. With its swift classification capabilities, ConfliBERT can process real-time data, ensuring that researchers and policymakers are equipped with classified insights as events transpire.</p></li>
<li><p><strong>Supporting Cross-Referencing and Validation</strong>: By classifying data, ConfliBERT also aids in cross-referencing. For instance, if two sources provide conflicting narratives of an event, having them classified can help researchers quickly juxtapose and validate the information.</p></li>
<li><p><strong>Assisting Predictive Analysis</strong>: Once events are classified systematically, it becomes feasible to perform predictive analysis. Recognizing patterns from past events can provide insights into potential future scenarios, aiding preemptive measures and strategies.</p></li>
</ol>
<p>In summary, ConfliBERT’s classification capabilities not only structure the unorganized maze of conflict data but also elevate the depth and breadth of analysis. By providing categorized, context-aware insights, it becomes an invaluable asset for researchers, analysts, and policymakers navigating the intricate domain of conflict research.</p>
<p>For a different example, let’s create a hypothetical news report about a political demonstration that turned violent. This example will have more variance in the types of police actions and outcomes, and it will include different categorizations.</p>
<p><strong>Original Text (Pre-Processing)</strong>: “During a large political demonstration in the capital, clashes erupted between protesters and police. Police reportedly used water cannons and rubber bullets to control the situation. Several protesters were detained, and there were reports of injuries among both police and demonstrators. A local shop was vandalized during the chaos. The protest was eventually dispersed, and order was restored by the authorities.”</p>
<p><strong>Processed Text (Post-Processing)</strong>: The text is segmented into individual sentences and processed for analysis, with each sentence receiving specific categorizations based on the content.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 44%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th>Processed Sentence</th>
<th>USE_OF_FORCE</th>
<th>DETENTION</th>
<th>INJURIES</th>
<th>VANDALISM</th>
<th>ORDER_RESTORED</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>During a large political demonstration in the capital, clashes erupted between protesters and police</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>Police reportedly used water cannons and rubber bullets to control the situation</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>Several protesters were detained</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>There were reports of injuries among both police and demonstrators</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>A local shop was vandalized during the chaos</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>The protest was eventually dispersed, and order was restored by the authorities</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>In this table: - “USE_OF_FORCE” indicates if the sentence suggests the use of force by police. - “DETENTION” denotes if the sentence implies that protesters were detained. - “INJURIES” signifies if there were injuries reported among the police or demonstrators. - “VANDALISM” marks if there was property damage or vandalism mentioned. - “ORDER_RESTORED” is affirmative if the sentence indicates that order was restored by the authorities.</p>
<p>This structured approach to analyzing news text provides a clear understanding of the various aspects of the event, which is essential for conflict analysis and research.</p>
</section>
<section id="inputs-for-classification" class="level2">
<h2 class="anchored" data-anchor-id="inputs-for-classification">Inputs for Classification</h2>
<p>To harness the capabilities of ConfliBERT for classification, one needs to be well-acquainted with the expected input format, necessary pre-processing steps, and customization options:</p>
<p><strong>Text Pre-processing:</strong></p>
<ol type="1">
<li><strong>Tokenization:</strong>
<ul>
<li>Tokenizing is the process of breaking down the text into smaller chunks, often words or sub-words.</li>
<li>Example: The sentence “UNRWA said it had suspended aid” will be tokenized as [“UNRWA”, “said”, “it”, “had”, “suspended”, “aid”].</li>
</ul></li>
<li><strong>Formatting:</strong>
<ul>
<li>Based on the dataset structure provided, the text should be divided into specific segments namely ‘sentence’, ‘source’, and ‘target’.</li>
<li>Example: For the text “UNRWA said it had suspended aid deliveries to Gaza”, “UNRWA” will be the source, “Gaza” will be the target, and the whole string is the sentence.</li>
</ul></li>
<li><strong>Special Character Removal:</strong>
<ul>
<li>It is crucial to ensure that any non-textual or special characters that do not contribute to the semantic meaning of the sentence are removed.</li>
<li>Example: If the sentence is “UNRWA said it had suspended aid!!!”, the exclamatory marks can be removed for a cleaner input as “UNRWA said it had suspended aid”.</li>
</ul></li>
<li><strong>Lowercasing/Uppercasing:</strong>
<ul>
<li>Depending on the model variant being used (cased or uncased), ensure that the text is transformed accordingly.</li>
<li>Example: For the uncased variant, “UNRWA said it had suspended aid” becomes “unrwa said it had suspended aid”.</li>
</ul></li>
<li><strong>Sentence Segmentation:</strong>
<ul>
<li>If the input is a long paragraph or document, break it down into individual sentences.</li>
<li>Example: For a paragraph “UNRWA said it had suspended aid. It was a major decision.”, the segmented sentences will be [“UNRWA said it had suspended aid.”, “It was a major decision.”].</li>
</ul></li>
</ol>
<p><strong>Customizing Classification with ConfliBERT:</strong></p>
<ol type="1">
<li><strong>Fine-tuning:</strong>
<ul>
<li>Although ConfliBERT is pre-trained, its versatility shines when fine-tuned on a specific dataset. This way, the model becomes more attuned to the nuances of the data.</li>
<li>Example: If the focus is on classifying statements made in a legal context, fine-tune ConfliBERT on legal documents or court verdicts.</li>
</ul></li>
<li><strong>Using Different Model Variants:</strong>
<ul>
<li>Choose a ConfliBERT version that aligns with the nature of the text data. If the distinction between uppercase and lowercase letters is significant, opt for a cased model.</li>
</ul></li>
<li><strong>Adjusting Model Parameters:</strong>
<ul>
<li>During fine-tuning or prediction, tweak model parameters like learning rate, batch size, or epoch count to optimize performance.</li>
</ul></li>
<li><strong>Feedback Loop:</strong>
<ul>
<li>Consider setting up a feedback loop where misclassifications can be corrected and fed back into the model for continuous learning.</li>
</ul></li>
</ol>
<section id="define-and-discuss-update-methods" class="level3">
<h3 class="anchored" data-anchor-id="define-and-discuss-update-methods">Define and discuss update methods</h3>
<p>There are four versions of ConfliBERT, each serving a specific purpose:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 80%">
</colgroup>
<thead>
<tr class="header">
<th>Version</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ConfliBERT-scr-uncased:</td>
<td>This version is pre-trained from scratch using a custom uncased vocabulary. This is the preferred version as it has been built from the ground up with a specific vocabulary set.</td>
</tr>
<tr class="even">
<td>ConfliBERT-scr-cased:</td>
<td>This version is also pre-trained from scratch but uses a custom cased vocabulary. This means it differentiates between uppercase and lowercase letters.</td>
</tr>
<tr class="odd">
<td>ConfliBERT-cont-uncased:</td>
<td>This version is built by continually pre-training on top of the original BERT’s uncased vocabulary.</td>
</tr>
<tr class="even">
<td>ConfliBERT-cont-cased:</td>
<td>This version is similar to the above but uses the original BERT’s cased vocabulary.</td>
</tr>
</tbody>
</table>
<p><strong>Scratch versus Continuous Defined:</strong> 1. <strong>Scratch:</strong> This refers to building the model from scratch, meaning the model is trained without any prior knowledge. The vocabulary is also built from the base without any prior context. 2. <strong>Continuous:</strong> Continuous pre-training means the model is built on top of an existing model (in this case, BERT). This leverages the knowledge already present in BERT and fine-tunes it for a specific task.</p>
</section>
</section>
<section id="outputs-from-classification" class="level2">
<h2 class="anchored" data-anchor-id="outputs-from-classification">Outputs from Classification</h2>
<p>When ConfliBERT provides a classification output, it organizes the results hierarchically and presents detailed information for clarity and to aid interpretation. Here is a deep dive into what each segment of the output signifies:</p>
<ol type="1">
<li><strong>Index and Basic Information:</strong>
<ul>
<li><strong>Index:</strong> Each entry or instance in the dataset being classified is assigned a unique identifier or index.</li>
<li><strong>Gold Penta:</strong> This is a reference label, indicating the correct classification as provided in the dataset.</li>
<li><strong>Sentence:</strong> The original textual input that was classified.</li>
<li><strong>Source:</strong> The entity or group mentioned as the originator or subject of the action in the sentence.</li>
<li><strong>Target:</strong> The entity or group mentioned as the recipient or object of the action in the sentence.</li>
</ul></li>
<li><strong>Level 1 Classification:</strong>
<ul>
<li><strong>Tense:</strong> The temporal context in which the event occurred, like ‘past’ or ‘future’.</li>
<li><strong>Prompt Text:</strong> A standardized, simplified representation of the sentence. For instance, if the sentence is “UNRWA said it had suspended aid deliveries to Gaza”, the prompt text might be “‘Source’ reduced aid to ‘Target’”, where ‘Source’ represents the source (UNRWA) and ‘Target’ represents the target (Gaza).</li>
<li><strong>Root Code:</strong> This signifies the primary, overarching classification category. For example, “SANCTION”.</li>
<li><strong>Score:</strong> This provides a confidence measure. The higher the score, the more confident the model is about its classification.</li>
</ul></li>
<li><strong>Level 1 Evaluation:</strong>
<ul>
<li><strong>Gold Root vs.&nbsp;Prediction:</strong> Here, the model’s prediction for the root code is compared to the correct answer or “gold” standard. For instance, if the gold standard is “SANCTION” and the model also predicts “SANCTION”, then this comparison would indicate a match.</li>
<li><strong>Gold Penta vs.&nbsp;Prediction:</strong> Similarly, the model’s prediction for the gold penta is compared to the correct answer. If both match, the model’s prediction is considered correct for that instance.</li>
</ul></li>
<li><strong>Level 2 Classification:</strong>
<ul>
<li><strong>Root Code:</strong> The primary classification category is reiterated here for clarity.</li>
<li><strong>Prompt Text:</strong> Similar to level 1, this provides a standardized representation of the sentence, but might be slightly more detailed or provide alternate interpretations.</li>
<li><strong>Future and Past Scores:</strong> For each classification prompt, there might be separate scores indicating the model’s confidence for both future and past contexts. This helps understand how the model perceives the temporal aspect of the event.</li>
<li><strong>L2 Root:</strong> This provides a more detailed or nuanced classification based on the primary root code from level 1.</li>
</ul></li>
<li><strong>Level 2 Evaluation:</strong>
<ul>
<li>This section is similar to the level 1 evaluation but pertains to the more detailed classifications from level 2.</li>
<li><strong>Gold Root vs.&nbsp;Prediction:</strong> The model’s level 2 root code prediction is compared to the correct or “gold” root code. If both are the same, the prediction is accurate.</li>
<li><strong>Gold Penta vs.&nbsp;Prediction:</strong> The predicted gold penta value at level 2 is compared against the correct value. If they match, the model’s prediction is deemed correct for that instance.</li>
</ul></li>
</ol>
<p>In essence, the output from ConfliBERT provides both a high-level and a detailed classification of the input sentence, complete with confidence scores, standardized representations, and a direct comparison to known correct answers for evaluation purposes. This hierarchical structure ensures a comprehensive understanding of the sentence’s context, entities involved, and the nature of their interaction.</p>
</section>
<section id="extant-evaluation-datasets" class="level2">
<h2 class="anchored" data-anchor-id="extant-evaluation-datasets">Extant Evaluation datasets:</h2>
<p><a href="https://github.com/eventdata/ConfliBERT#evaluation-datasets">The datasets</a></p>
</section>
<section id="huggingface-usage-here" class="level2">
<h2 class="anchored" data-anchor-id="huggingface-usage-here">Huggingface usage here</h2>
<p>(To be filled with information on how to use ConfliBERT with the Huggingface library, if applicable.)</p>
</section>
<section id="metrics-for-classification" class="level2">
<h2 class="anchored" data-anchor-id="metrics-for-classification">Metrics for Classification</h2>
<p>(To be filled with relevant metrics used to evaluate the performance of ConfliBERT for classification, e.g., accuracy, F1 score, etc.)</p>
</section>
<section id="evaluation-of-classification" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-of-classification">Evaluation of Classification</h2>
<p>(To be filled with details on how ConfliBERT’s classification performance was evaluated, including methodologies, datasets used, and results.)</p>
</section>
<section id="other" class="level2">
<h2 class="anchored" data-anchor-id="other">Other</h2>
</section>
</section>
<section id="masking-and-coding-tasks-with-conflibert" class="level1">
<h1>Masking and Coding Tasks with ConfliBERT</h1>
<p><strong>[MASK]</strong> said that the U.S. alliance with Japan is stronger than ever.</p>
<p>Saudi Arabia expressed intent to restore diplomatic relations with <strong>[MASK]</strong>.</p>
<p><strong>[MASK]</strong> provided humanitarian aid to Turkey.</p>
<p>President Zelenskyy accused <strong>[MASK]</strong> of war crimes.</p>
<p><strong>[MASK]</strong> forces attacked Hamas in Gaza City.</p>
<p>Language models like ConfliBERT employ a technique known as “masked language modeling” during the training process. Here is an in-depth breakdown of how masking functions within ConfliBERT:</p>
<p><strong>1. The Principle of Masking:</strong><br>
Masking is the process where a certain percentage of input tokens are replaced with a special <code>[MASK]</code> token. This technique challenges the model to predict the masked word from its context. This approach ensures that the model pays attention to both the left and the right context of a word, fostering bidirectional understanding.</p>
<p><strong>2. Benefits for Conflict Research:</strong><br>
In conflict research, data can often be inconsistent, fragmented, or incomplete due to the sensitive nature of the information or the challenges in data collection in conflict zones. Masking in ConfliBERT can be pivotal as: - It trains the model to reconstruct missing or obscured information. - It allows the model to predict contextually relevant terms, crucial for accurate event classification and relationship extraction.</p>
<p><strong>3. Applications in Downstream Tasks:</strong><br>
The masked language modeling task equips ConfliBERT to handle various downstream tasks such as named entity recognition, relationship extraction, and event classification. The model’s ability to predict masked entities can aid in: - Identifying relevant actors or groups in conflict narratives. - Extracting the type of conflict event (e.g., bombing, armed assault). - Predicting relationships between entities in a given text.</p>
<p><strong>4. Enhancing Domain-Specific Understanding:</strong><br>
The specific vocabulary and context associated with the political conflict domain make it challenging. When ConfliBERT undergoes training with masking on conflict-specific datasets, it becomes better at recognizing domain-specific terms, like “separatists”, and understanding their relevance. This is crucial, as standard models might misinterpret or fragment these terms.</p>
<p><strong>5. Integration with Other Pre-training Techniques:</strong><br>
While masking is essential, it is one of many pre-training techniques. In conjunction with techniques like next-sentence-prediction, ConfliBERT can develop a deeper understanding of text sequences and relationships, enhancing its performance on complex conflict data.</p>
<p>In conclusion, masking is not just a technique but a cornerstone in the training regimen of ConfliBERT. It ensures that the model is well-equipped to tackle the intricate nuances and challenges associated with political conflict research. Through effective masking strategies, ConfliBERT becomes adept at providing comprehensive insights into conflict narratives, making it a valuable tool for conflict researchers and analysts.</p>
<section id="inputs-for-masking-and-coding" class="level2">
<h2 class="anchored" data-anchor-id="inputs-for-masking-and-coding">Inputs for Masking and Coding</h2>
<p>To utilize ConfliBERT for masking and coding tasks, the input data must be structured and pre-processed according to specific guidelines.</p>
<ol type="1">
<li><p><strong>Tokenization</strong>: The text should be tokenized into sub-words or words. The tokenization method will depend on the pre-trained model and tokenizer available for ConfliBERT.</p></li>
<li><p><strong>Formatting</strong>: For the <code>[MASK]</code> task, specific tokens or words in the sentence should be replaced with the <code>[MASK]</code> token. The model will then predict the missing token based on the context provided by the rest of the sentence.</p></li>
<li><p><strong>Segmentation</strong>: If the input consists of multiple sentences, they should be divided into distinct segments. Each segment will be input separately into the model.</p></li>
<li><p><strong>Padding</strong>: All inputs should be padded to have the same length for batching during model training or inference.</p></li>
</ol>
</section>
<section id="outputs-from-masking-and-coding" class="level2">
<h2 class="anchored" data-anchor-id="outputs-from-masking-and-coding">Outputs from Masking and Coding</h2>
<ol type="1">
<li><p><strong>Predicted Tokens</strong>: The primary output from the masking task is the token predicted by ConfliBERT to replace the [MASK] token. The model will provide a probability distribution over the vocabulary, and the token with the highest probability is usually taken as the prediction.</p></li>
<li><p><strong>Embeddings</strong>: ConfliBERT will also produce embeddings for each token in the input sequence. These embeddings can be used for downstream tasks or further analysis.</p></li>
<li><p><strong>Coding Outputs</strong>: Depending on the coding task, ConfliBERT might classify a given input into predefined categories or produce other forms of structured output.</p></li>
</ol>
</section>
<section id="metrics-for-masking-and-coding" class="level2">
<h2 class="anchored" data-anchor-id="metrics-for-masking-and-coding">Metrics for Masking and Coding</h2>
<ol type="1">
<li><p><strong>Accuracy</strong>: Measures the percentage of [MASK] tokens that the model predicted correctly.</p></li>
<li><p><strong>Perplexity</strong>: Provides insight into how well the probability distribution predicted by the model aligns with the actual distribution of the [MASK] token.</p></li>
<li><p><strong>F1-Score</strong>: Used mainly for coding tasks, it considers both precision and recall to provide a more holistic view of model performance.</p></li>
<li><p><strong>Loss</strong>: The model’s objective function value, which it tries to minimize during training.</p></li>
</ol>
</section>
<section id="evaluation-of-masking-and-coding" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-of-masking-and-coding">Evaluation of Masking and Coding</h2>
<ol type="1">
<li><p><strong>In-domain vs.&nbsp;Out-of-domain Evaluation</strong>: It is essential to evaluate the model’s performance both on data similar to its training data and on entirely different datasets.</p></li>
<li><p><strong>Human Evaluation</strong>: Sometimes, multiple tokens can correctly fill a [MASK] token, so human evaluators can provide more nuanced feedback on the model’s predictions.</p></li>
<li><p><strong>Ablation Studies</strong>: Evaluating the model’s performance by removing or modifying certain parts can help understand the importance of various components.</p></li>
</ol>
</section>
<section id="other-llm-comparisons-on-this-task-1" class="level2">
<h2 class="anchored" data-anchor-id="other-llm-comparisons-on-this-task-1">Other LLM comparisons on this task</h2>
<ol type="1">
<li><p><strong>BERT</strong>: BERT is the pioneering model that introduced the [MASK] task. It is insightful to compare newer models like ConfliBERT with BERT to see the advancements.</p></li>
<li><p><strong>RoBERTa</strong>: A variant of BERT, RoBERTa changes some key hyperparameters and training strategies, which can have different performances on the masking task.</p></li>
</ol>
</section>
</section>
<section id="computational-considerations-and-benchmarks-for-conflibert" class="level1">
<h1>Computational Considerations and Benchmarks for ConfliBERT</h1>
<p>When implementing BERT-based models such as ConfliBERT, one needs to consider the computational implications. While BERT benchmarks provide useful insights, ConfliBERT, as a derivative of BERT, might exhibit different behaviors. Here is a general breakdown based on the information provided:</p>
<section id="cpu-usage-1-core" class="level2">
<h2 class="anchored" data-anchor-id="cpu-usage-1-core">CPU Usage (1 Core)</h2>
<p>BERT, and by extension models like ConfliBERT, are heavily parallelized models, which means they benefit significantly from multiple cores or GPUs. Using just a single core would be significantly slower and is not recommended for any sizable inference or training task. However, for very light tasks or experimentation, a single core could suffice, albeit with extended processing times.</p>
</section>
<section id="cpu-multicore" class="level2">
<h2 class="anchored" data-anchor-id="cpu-multicore">CPU (Multicore)</h2>
<p>Based on BERT benchmarks [<a href="https://vincentteyssier.medium.com/bert-inference-cost-performance-analysis-cpu-vs-gpu-b58a2420b2c8" class="uri">https://vincentteyssier.medium.com/bert-inference-cost-performance-analysis-cpu-vs-gpu-b58a2420b2c8</a>]:</p>
<section id="vcpus-e2-highcpu-8" class="level3">
<h3 class="anchored" data-anchor-id="vcpus-e2-highcpu-8">8 vCPUs (e2-highcpu-8):</h3>
<ul>
<li><strong>Processing Time per File</strong>: 69.4s - 70.5s for 1500 samples.</li>
<li><strong>Overall vCPU Usage</strong>: Approximately 75%.</li>
<li><strong>Cost</strong>: $0.197872/hour or $144.45 monthly.</li>
</ul>
</section>
<section id="vcpus-e2-highcpu-16" class="level3">
<h3 class="anchored" data-anchor-id="vcpus-e2-highcpu-16">16 vCPUs (e2-highcpu-16):</h3>
<ul>
<li><strong>Processing Time per File</strong>: 40.80s - 43.34s for 1500 samples.</li>
<li><strong>Overall vCPU Usage</strong>: Approximately 60%.</li>
<li><strong>Cost</strong>: $0.395744/hour or $288.89 monthly.</li>
</ul>
</section>
<section id="vcpus-e2-highcpu-32" class="level3">
<h3 class="anchored" data-anchor-id="vcpus-e2-highcpu-32">32 vCPUs (e2-highcpu-32):</h3>
<ul>
<li><strong>Processing Time per File</strong>: 35.73s - 40.21s for 1500 samples.</li>
<li><strong>Overall vCPU Usage</strong>: Approximately 40%.</li>
<li><strong>Cost</strong>: $0.791488/hour.</li>
</ul>
<p>The overarching pattern reveals that as the number of CPU cores increases, the processing time decreases. However, there is a point of diminishing returns: scaling from 16 to 32 vCPUs does not halve the processing time. Additionally, the vCPU usage decreases, indicating underutilization of resources with higher cores.</p>
</section>
</section>
<section id="with-gpus-and-other-architectures-non-intelamd" class="level2">
<h2 class="anchored" data-anchor-id="with-gpus-and-other-architectures-non-intelamd">With GPUs and other Architectures (non-Intel/AMD)</h2>
<p>GPUs, especially those designed for deep learning tasks, like the NVIDIA Tesla series, provide significant speed-ups for BERT-like models due to their parallel processing capabilities.</p>
<section id="gpu-n1-standard-8-nvidia-tesla-v100" class="level3">
<h3 class="anchored" data-anchor-id="gpu-n1-standard-8-nvidia-tesla-v100">1 GPU (n1-standard-8 + NVIDIA Tesla V100):</h3>
<ul>
<li><strong>Processing Time per File</strong>: 2.16s - 3.28s for 1500 samples. This speed is around 25 times faster than the 8 vCPU instance and approximately 15 times faster than the 32 vCPU instance.</li>
<li><strong>Cost</strong>: $2.004/hour or $1,462.99 monthly.</li>
</ul>
<p>For specialized architectures beyond Intel/AMD, the benchmarks would vary based on the specific architecture. For example, FPGAs or TPUs might offer different performance and cost metrics. However, TPUs, in particular, have been known to provide excellent performance for TensorFlow-based models, including BERT derivatives.</p>
<p>In conclusion, while BERT benchmarks give an idea of the computational requirements, users should perform their own benchmarks to ascertain the exact requirements for ConfliBERT. Always consider the scale of the task, available budget, and desired processing time when choosing the computational infrastructure.</p>
</section>
</section>
</section>
<section id="references-citations" class="level1">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References / Citations</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-halterman2023plover" class="csl-entry" role="listitem">
Halterman, Andrew, Benjamin Bagozzi, Andreas Beger, Phil Schrodt, and Grace Scraborough. 2023. <span>“PLOVER and POLECAT: A New Political Event Ontology and Dataset.”</span>
</div>
<div id="ref-hu2022conflibert" class="csl-entry" role="listitem">
Hu, Yibo, MohammadSaleh Hosseini, Erick Skorupa Parolin, Javier Osorio, Latifur Khan, Patrick Brandt, and Vito D’Orazio. 2022. <span>“ConfliBERT: A Pre-Trained Language Model for Political Conflict and Violence.”</span> In <em>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 5469–82.
</div>
<div id="ref-googleMLCrashCourse" class="csl-entry" role="listitem">
<span>“Machine Learning Crash Course: Classification: True/False Positive/Negative.”</span> 2021. Google. <a href="https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative">https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2023 Shreyas Meher</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>