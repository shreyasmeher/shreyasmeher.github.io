{
  "cells": [
    {
      "cell_type": "raw",
      "id": "0e984b88",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'BLS Scraper Project'\n",
        "author: 'Shreyas Meher'\n",
        "date: today\n",
        "date-format: \"D MMMM YYYY\"\n",
        "format:\n",
        "    html\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e391168",
      "metadata": {},
      "source": [
        "# Project Summary\n",
        "\n",
        "The script is a .ipynb file named as Final.ipynb. Load the jupyter notebook with Jupyter Lab or Jupyter notebook.\n",
        "\n",
        "The first few cells in the script ask the user to input the working url of the BLS content/table that they want to scrape using the script. Here, they are expected to input a url from https://www.bls.gov/bls/newsrels.htm#OPLC, after which the url is stored by the script.\n",
        "\n",
        "The url is then scraped using BeautifulSoup, looking for the relevant tags of the various tables in the page. This script is unique in the sense that it allows for selective scraping of multiple tables in the webpage, which is not a base function of the packages used (Pandas or BeautifulSoup). Tables are first unwrapped from within the various tags and then merged using a loop function.\n",
        "\n",
        "The user is then asked which of the tables in the page (in the case that there are multiple of them - eg. https://www.bls.gov/regions/southwest/news-release/2022/occupationalemploymentandwages_houston_20220624.htm) they want to scrape.\n",
        "\n",
        "To clean the data up even more, the footnote markers are then removed from the dataframe using another function. As the data scraped is going to be quantitative in nature, $ signs and markers are removed from the dataframe created as well. This is to ensure that the data is readable by statistical software and requires very little cleaning after it is run through the script. You might have to change a few tags in the script for this to work, and so I have #'d out the commands to do the same.\n",
        "\n",
        "Finally, the user is then prompted once again to define a .csv filename for the data to be stored to in their local computing environment. The .csv file is then stored onto the working directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ee959305",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table attributes\n",
            "['regular']\n"
          ]
        }
      ],
      "source": [
        "#workspace init\n",
        "import urllib\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#defining variables with user-input\n",
        "\n",
        "def new_func():\n",
        "    url = input('Please enter the BLS publication that you want to scrape table from:')\n",
        "    return url\n",
        "\n",
        "url = new_func()\n",
        "data = urllib.request.urlopen(url).read()\n",
        "\n",
        "sp = BeautifulSoup(data,'html.parser')\n",
        "\n",
        "#Let us first get the table attributes on the BLS website\n",
        "print('Table attributes')\n",
        "for table in sp.find_all('table'):\n",
        "    print(table.get('class'))\n",
        "    break\n",
        "\n",
        "lsttb = sp.find('table',class_='regular')\n",
        "\n",
        "#check for multiple tables and unwrap them\n",
        "\n",
        "for table in sp.findChildren(attrs={'id': 'regular'}): \n",
        "    for c in table.children:\n",
        "        if c.name in ['tbody', 'thead']:\n",
        "            c.unwrap()\n",
        "\n",
        "#dataset creation \n",
        "\n",
        "data_pandas = pd.read_html(str(sp), flavor=\"bs4\",thousands=',',decimal='.')\n",
        "\n",
        "#clean dataset\n",
        "df = pd.concat(data_pandas, axis=0)#to convert lists of pd.read_html to dataframe\n",
        "\n",
        "#export to csv\n",
        "df.to_csv(input('Specify .csv filename:'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "64924d34f5c95f261818ee0e41d58b43741dcf0b72677c10b726c1fe86046c0c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
